<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>logging.DEBUG</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://imperio-wxm.github.io/"/>
  <updated>2020-04-04T02:48:00.470Z</updated>
  <id>http://imperio-wxm.github.io/</id>
  
  <author>
    <name>wxmimperio</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark SequenceFile 多路径输出</title>
    <link href="http://imperio-wxm.github.io/2020/01/15/Spark-SequenceFile-Multipath-Output/"/>
    <id>http://imperio-wxm.github.io/2020/01/15/Spark-SequenceFile-Multipath-Output/</id>
    <published>2020-01-15T13:18:30.000Z</published>
    <updated>2020-04-04T02:48:00.470Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>使用Spark读写SequenceFile时，有时需要根据一定的规则动态生成文件的输出路径，且需要将某一类的数据都输出到同一个Path下，这时就需要使用MultipleSequenceFileOutputFormat </p></div><a id="more"></a><table><thead><tr><th>SoftWare</th><th>Version</th></tr></thead><tbody><tr><td>Scala</td><td>2.11.11</td></tr><tr><td>Spark</td><td>2.4.0</td></tr></tbody></table><h1 id="MultipleSequenceFileOutputFormat-源码预览"><a href="#MultipleSequenceFileOutputFormat-源码预览" class="headerlink" title="MultipleSequenceFileOutputFormat 源码预览"></a>MultipleSequenceFileOutputFormat 源码预览</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 继承自MultipleOutputFormat</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MultipleSequenceFileOutputFormat</span> &lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">MultipleOutputFormat</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> SequenceFileOutputFormat&lt;K,V&gt; theSequenceFileOutputFormat = <span class="keyword">null</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 重写了 getBaseRecordWriter 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> RecordWriter&lt;K, V&gt; <span class="title">getBaseRecordWriter</span><span class="params">(FileSystem fs, JobConf job, String name, Progressable arg3)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (theSequenceFileOutputFormat == <span class="keyword">null</span>) &#123;</span><br><span class="line">            theSequenceFileOutputFormat = <span class="keyword">new</span> SequenceFileOutputFormat&lt;K,V&gt;();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回 SequenceFileOutputFormat 的getRecordWriter，提供writer句柄</span></span><br><span class="line">        <span class="keyword">return</span> theSequenceFileOutputFormat.getRecordWriter(fs, job, name, arg3);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="SequenceFileOutputFormat-源码预览"><a href="#SequenceFileOutputFormat-源码预览" class="headerlink" title="SequenceFileOutputFormat 源码预览"></a>SequenceFileOutputFormat 源码预览</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RecordWriter&lt;K, V&gt; <span class="title">getRecordWriter</span><span class="params">(FileSystem ignored, JobConf job, String name, Progressable progress)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取输出临时目录</span></span><br><span class="line">    Path file = FileOutputFormat.getTaskOutputPath(job, name);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取FileSystem操作句柄</span></span><br><span class="line">    FileSystem fs = file.getFileSystem(job);</span><br><span class="line">    CompressionCodec codec = <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">// 设置压缩编码</span></span><br><span class="line">    CompressionType compressionType = CompressionType.NONE;</span><br><span class="line">    <span class="keyword">if</span> (getCompressOutput(job)) &#123;</span><br><span class="line">        <span class="comment">// find the kind of compression to do</span></span><br><span class="line">        compressionType = getOutputCompressionType(job);</span><br><span class="line">        <span class="comment">// find the right codec</span></span><br><span class="line">        Class&lt;? extends CompressionCodec&gt; codecClass = getOutputCompressorClass(job,</span><br><span class="line">        DefaultCodec.class);</span><br><span class="line">        codec = ReflectionUtils.newInstance(codecClass, job);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 正式创建SequenceFile writer，其实这里和我们自己写sequenceFile writer过程是一样的</span></span><br><span class="line">    <span class="keyword">final</span> SequenceFile.Writer out = SequenceFile.createWriter(fs, job, file, job.getOutputKeyClass(), job.getOutputValueClass(), compressionType, codec, progress);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回被RecordWriter 包装后的 sequenceFile writer</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> RecordWriter&lt;K, V&gt;() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span><span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">            out.append(key, value);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(Reporter reporter)</span> <span class="keyword">throws</span> IOException </span>&#123; </span><br><span class="line">            out.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">....其他方法略过，例如设置Compression的方法等</span><br></pre></td></tr></table></figure><h1 id="MultipleOutputFormat-源码预览"><a href="#MultipleOutputFormat-源码预览" class="headerlink" title="MultipleOutputFormat 源码预览"></a>MultipleOutputFormat 源码预览</h1><p><code>主要是对这个类中方法的重写</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 内容略，就是上面的getRecordWriter，获取writer</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordWriter&lt;K, V&gt; <span class="title">getRecordWriter</span><span class="params">(FileSystem fs, JobConf job, String name, Progressable arg3)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    ..........</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Generate the leaf name for the output file name. The default behavior does</span></span><br><span class="line"><span class="comment">* not change the leaf file name (such as part-00000)</span></span><br><span class="line"><span class="comment">* </span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">*          the leaf file name for the output file</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span> the given leaf file name</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 定义输出文件名</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> String <span class="title">generateLeafFileName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Generate the file output file name based on the given key and the leaf file</span></span><br><span class="line"><span class="comment">* name. The default behavior is that the file name does not depend on the</span></span><br><span class="line"><span class="comment">* key.</span></span><br><span class="line"><span class="comment">* </span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">*          the key of the output data</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">*          the leaf file name</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span> generated file name</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 可根据key，value值自定义输出文件路径</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> String <span class="title">generateFileNameForKeyValue</span><span class="params">(K key, V value, String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Generate the actual key from the given key/value. The default behavior is that</span></span><br><span class="line"><span class="comment">* the actual key is equal to the given key</span></span><br><span class="line"><span class="comment">* </span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">*          the key of the output data</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">*          the value of the output data</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span> the actual key derived from the given key/value</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 自定义输出key</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> K <span class="title">generateActualKey</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> key;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Generate the actual value from the given key and value. The default behavior is that</span></span><br><span class="line"><span class="comment">* the actual value is equal to the given value</span></span><br><span class="line"><span class="comment">* </span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">*          the key of the output data</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">*          the value of the output data</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span> the actual value derived from the given key/value</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 自定义输出value</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> V <span class="title">generateActualValue</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="自定义OutputFormat"><a href="#自定义OutputFormat" class="headerlink" title="自定义OutputFormat"></a>自定义OutputFormat</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义一个输出类，继承自MultipleSequenceFileOutputFormat</span></span><br><span class="line"><span class="comment">// 然后重写里面的各种方法</span></span><br><span class="line"><span class="comment">// 其中重写generateFileNameForKeyValue以达到自定义路径的目的</span></span><br><span class="line"><span class="comment">// 可以根据key，value生成目录，以达到将相同key或其他形式的内容输出到同一个路径下</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitMultipleSequenceFileOutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>] <span class="keyword">extends</span> <span class="title">MultipleSequenceFileOutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>](<span class="params"></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 可以自定义输出文件名，such as part-00000</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateLeafFileName</span></span>(name: <span class="type">String</span>): <span class="type">String</span> = <span class="keyword">super</span>.generateLeafFileName(name)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 可以自定义输出value</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualValue</span></span>(key: <span class="type">K</span>, value: <span class="type">V</span>): <span class="type">V</span> = <span class="keyword">super</span>.generateActualValue(key, value)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 可以自定义输出key</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualKey</span></span>(key: <span class="type">K</span>, value: <span class="type">V</span>): <span class="type">K</span> = <span class="keyword">super</span>.generateActualKey(key, value)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateFileNameForKeyValue</span></span>(key: <span class="type">K</span>, value: <span class="type">V</span>, name: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="comment">// 具体拆分逻辑</span></span><br><span class="line">        <span class="comment">// 例如文件中key的分布为：a、b、c，我希望a、b、c各自有独立的目录，且独立目录中只有包含各自key 的数据</span></span><br><span class="line">        <span class="comment">// 这样输出的目录就是：</span></span><br><span class="line">        <span class="comment">//     basePath/a/part-00000  且key为a的全在这个文件中</span></span><br><span class="line">        <span class="comment">//     basePath/b/part-00000  且key为b的全在这个文件中</span></span><br><span class="line">        <span class="comment">//     basePath/c/part-00000  且key为c的全在这个文件中</span></span><br><span class="line">        key.toString + <span class="string">"/"</span> + name</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="自定义OutputFormat-使用"><a href="#自定义OutputFormat-使用" class="headerlink" title="自定义OutputFormat 使用"></a>自定义OutputFormat 使用</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步是读取数据源，可以是其他，我这里是读取一个txt文件然后根据一定规则转换成sequenceFile</span></span><br><span class="line"><span class="comment">// 第二步是数据中间的处理逻辑</span></span><br><span class="line"><span class="comment">// 第三步是输出，使用saveAsHadoopFile(输出base路径，key的类型，value的类型，输出文件类型)，最后设置自定义的SplitMultipleSequenceFileOutputFormat</span></span><br><span class="line"><span class="comment">// 注意：其中sequenceFile的key，value都必须是可序列化的，例如hadoop text</span></span><br><span class="line"></span><br><span class="line">sparkContext.textFile(inputPath)</span><br><span class="line">    .map(line =&gt; parserData(line))</span><br><span class="line">    .filter(newLine =&gt; filterData(newLine))</span><br><span class="line">    ......数据处理逻辑</span><br><span class="line">    .saveAsHadoopFile(outPath, classOf[<span class="type">Text</span>], classOf[<span class="type">Text</span>], classOf[<span class="type">SplitMultipleSequenceFileOutputFormat</span>[_, _]])</span><br></pre></td></tr></table></figure><h1 id="spark-file-split案例"><a href="#spark-file-split案例" class="headerlink" title="spark-file-split案例"></a>spark-file-split案例</h1><ul><li><a href="https://github.com/imperio-wxm/bigdata-code-snippets/tree/master/spark-file-split" target="_blank" rel="noopener">spark-file-split 项目</a></li></ul><p>【此Project的作用是】：</p><ol><li>数据以竖线分隔的文本，且与avro schema字段对应</li><li>通过文本内容和avro schema，判断数据时间event_time是否符合入参标准</li><li>从文本内容中解析出表名、数据时间，生成分区日期、自定义输出路径</li><li>使得同一张表、同一日期分区的数据，落入同一个path</li><li>之后可以将输出目录的数据直接load到hive sequence表中</li></ol><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;使用Spark读写SequenceFile时，有时需要根据一定的规则动态生成文件的输出路径，且需要将某一类的数据都输出到同一个Path下，这时就需要使用MultipleSequenceFileOutputFormat &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://imperio-wxm.github.io/categories/Spark/"/>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/Spark/BigData/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Spark" scheme="http://imperio-wxm.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Flume TaildirSource 源码扩展(支持子目录)</title>
    <link href="http://imperio-wxm.github.io/2019/12/20/Flume-TaildirSource-Expand/"/>
    <id>http://imperio-wxm.github.io/2019/12/20/Flume-TaildirSource-Expand/</id>
    <published>2019-12-20T05:09:56.000Z</published>
    <updated>2020-02-13T16:14:50.749Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>Flume TaildirSource可实时监控当前目录下一批文件，并记录每个文件最新消费位置，flume进程重启后不会有重复消费的问题；但是无法监听当前目录的子目录，要实现多目录的监听就需要配置多个group，无法做到动态识别子目录</p></div><a id="more"></a><p>对TaildirSource的使用不做过多介绍，请参考官方文档：<a href="http://flume.apache.org/releases/content/1.7.0/FlumeUserGuide.html" target="_blank" rel="noopener">Flume-1.7.0 官方文档</a></p><blockquote><p>参考源代码为flume 1.7.0版本</p></blockquote><p><a href="https://github.com/apache/flume/tree/flume-1.7/flume-ng-sources/flume-taildir-source" target="_blank" rel="noopener">TaildirSource 官方源码</a></p><p><code>为了与官网源码区别，我将源码所有.java文件都加了前缀Multiple</code></p><p><strong><a href="https://github.com/imperio-wxm/multiple-taildir-source" target="_blank" rel="noopener">MultipleTaildirSource 扩展源码</a></strong></p><h2 id="源码结构"><a href="#源码结构" class="headerlink" title="源码结构"></a>源码结构</h2><blockquote><p>MultipleTaildirMatcher.jar</p></blockquote><ul><li>用于根据条件获取目录文件等操作</li></ul><blockquote><p>MultipleReliableTaildirEventReader.jar</p></blockquote><ul><li>用于文件位置记录维护、flume event封装、事件处理等操作</li></ul><blockquote><p>MultipleTaildirSource.jar</p></blockquote><ul><li>Source的入口，用于参数初始化、轮询线程池定义、文件位置更新等操作</li></ul><blockquote><p>MultipleTailFile.jar</p></blockquote><ul><li>封装了具体文件的操作，利用RandomAccessFile可以获取文件位置指针等信息</li></ul><blockquote><p>MultipleTaildirSourceConfigurationConstants.jar</p></blockquote><ul><li>参数配置的常量信息等</li></ul><h2 id="目录及其子目录遍历"><a href="#目录及其子目录遍历" class="headerlink" title="目录及其子目录遍历"></a>目录及其子目录遍历</h2><p>首先需要找到获取文件目录的源码地方：<code>MultipleTaildirMatcher.jar</code></p><p>相关方法：<code>getMatchingFiles</code>、<code>getMatchingFilesNoCache</code>、<code>getFiles</code></p><ul><li>getMatchingFiles</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> currentParentDirMTime = parentDir.lastModified();</span><br><span class="line"><span class="comment">// 当前时间 - 最后一次检查时间 大于 强制刷新间隔，则进行目录的强制刷新</span></span><br><span class="line"><span class="keyword">boolean</span> needForceRefresh = (now - lastCheckedTime) &gt; forceRefreshIntervalMs;</span><br><span class="line"></span><br><span class="line"><span class="comment">// lastSeenParentDirMTime &lt; currentParentDirMTime 为根目录修改时间大于上次查看时间</span></span><br><span class="line"><span class="comment">// !(currentParentDirMTime &lt; lastCheckedTime) 为最后一次查看时间小于等于目录修改时间</span></span><br><span class="line"><span class="keyword">if</span> (!cachePatternMatching ||</span><br><span class="line">        lastSeenParentDirMTime &lt; currentParentDirMTime ||</span><br><span class="line">        !(currentParentDirMTime &lt; lastCheckedTime) ||</span><br><span class="line">        needForceRefresh) &#123;</span><br><span class="line">    logger.info(<span class="string">"Fetching files....."</span>);</span><br><span class="line">    <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">    List&lt;File&gt; files = getFiles(parentDir);</span><br><span class="line">    getAllFilesCost = System.currentTimeMillis() - start;</span><br><span class="line">    <span class="keyword">if</span> (getAllFilesCost &gt; forceRefreshIntervalMs) &#123;</span><br><span class="line">        logger.error(String.format(<span class="string">"Traversing the directory takes too long!!! cost = %s ms"</span>, getAllFilesCost));</span><br><span class="line">    &#125;</span><br><span class="line">    logger.info(String.format(<span class="string">"Getting files takes time = %s ms"</span>, getAllFilesCost));</span><br><span class="line">    lastMatchedFiles = sortByLastModifiedTime(files);</span><br><span class="line">    lastSeenParentDirMTime = currentParentDirMTime;</span><br><span class="line">    lastCheckedTime = now;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为有上一次已经识别的文件缓存<code>lastMatchedFiles</code>，以上代码的作用是检测是否需要从底层从新获取一次文件列表以更新缓存</p><p>需要注意的是<code>currentParentDirMTime</code>这个变量，表示目录最后一次更新时间<code>parentDir.lastModified()</code></p><p>这里有一个问题是，当我们操作子目录时（例如向子目录cp一个新文件），此时根目录的最后修改时间是不发生改变的，例如：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- root (lastModified = 12:00)</span><br><span class="line">   -- dir01 (lastModified = 12:05)</span><br><span class="line">      -- file.txt</span><br></pre></td></tr></table></figure></p><p>root为根目录，在其中有dir01一个子目录，dir01下有一个file.txt文件；如果此时cp一个file_copy.txt到dir01下面，只会更新dir01目录的lastModified，而root的lastModified不发生改变，<code>此时虽然有新的文件加入了，但依然不满足if 条件，无法刷新缓存</code></p><p>因为源码是只支持一个根目录的，所以源码这样写是没有问题，但是目前我们需要对其子目录也做监听，就要考虑如何进入if中</p><ul><li><p>方案一：每次轮训都将当前根目录下所有dir的最后时间获取，取出最大的时间更新currentParentDirMTime；此方案需要每次都轮序全目录，太消耗资源（不可用）</p></li><li><p>方案二：定时强制刷新缓存，即不管是否满足lastModified条件，到达一定时间就强制从底层获取文件列表更新缓存（可用）</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用方案二，加入强制刷新时间参数</span></span><br><span class="line"><span class="comment">// （当前轮序时间 - 最近一次轮训时间） &gt; 我们所设置的间隔，就强制刷新</span></span><br><span class="line"><span class="keyword">boolean</span> needForceRefresh = (now - lastCheckedTime) &gt; forceRefreshIntervalMs;</span><br></pre></td></tr></table></figure><hr><ul><li>getMatchingFilesNoCache</li></ul><p>这个方法为源码方法，主要是看下源码如何获取文件路径，以及我们应该如何修改</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;File&gt; <span class="title">getMatchingFilesNoCache</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    List&lt;File&gt; result = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">try</span> (DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(parentDir.toPath(), fileFilter)) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Path entry : stream) &#123;</span><br><span class="line">            result.add(entry.toFile());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        logger.error(<span class="string">"I/O exception occurred while listing parent directory. "</span> +</span><br><span class="line">                <span class="string">"Files already matched will be returned. "</span> + parentDir.toPath(), e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法使用了<code>Files.newDirectoryStream</code>将<code>parentDir</code>下的文件全部获取，这里的<code>fileFilter</code>是通过正则过滤符合条件的文件</p><ul><li>getFiles</li></ul><p>这个方法是我自己实现的，获取目录及其子目录下的所有文件路径</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;File&gt; <span class="title">getFiles</span><span class="params">(File files)</span> </span>&#123;</span><br><span class="line">    List&lt;File&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    result.add(files);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; result.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (result.get(i).isDirectory()) &#123;</span><br><span class="line">            File[] list = result.get(i).listFiles();</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> != list &amp;&amp; list.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (File f : list) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (<span class="keyword">null</span> != pattern) &#123;</span><br><span class="line">                        <span class="comment">// 只加入符合正则的文件</span></span><br><span class="line">                        <span class="keyword">if</span> (f.isFile() &amp;&amp; pattern.matcher(f.getAbsolutePath()).matches()) &#123;</span><br><span class="line">                            result.add(f);</span><br><span class="line">                        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (f.isDirectory()) &#123;</span><br><span class="line">                            result.add(f);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        result.add(f);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    result = result.stream().filter(File::isFile).collect(Collectors.toList());</span><br><span class="line">    result.forEach(f -&gt; logger.debug(<span class="string">"Path = "</span> + f));</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里用了一个非递归的遍历，因为在目录结构复杂层数比较多的情况下，使用递归入栈出栈和消耗的cpu、内存都要高，不是最佳选择；</p><p><code>遍历目录是一个性能瓶颈点，当遍历目录的耗时已经大于你所设置的强制刷新缓存间隔时，就会出现性能问题</code></p><p>关于遍历目录也有2个备选方案：</p><ul><li>Nio2遍历</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这种遍历是效率最高的，但是会有一个问题，当目录是软链ln -s时，根目录的文件无法被识别出来，导致遗漏文件，这个问题暂时还没深究</span></span><br><span class="line"></span><br><span class="line">FindJavaVisitor findJavaVisitor = <span class="keyword">new</span> FindJavaVisitor();</span><br><span class="line">Files.walkFileTree(startingDir, findJavaVisitor);</span><br><span class="line">System.out.println(findJavaVisitor.getFileList().size());</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FindJavaVisitor</span> <span class="keyword">extends</span> <span class="title">SimpleFileVisitor</span>&lt;<span class="title">Path</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> List&lt;File&gt; fileList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> List&lt;File&gt; dirs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> FileVisitResult <span class="title">visitFile</span><span class="params">(Path file, BasicFileAttributes attrs)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        fileList.add(file.toFile());</span><br><span class="line">        <span class="keyword">return</span> FileVisitResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> FileVisitResult <span class="title">preVisitDirectory</span><span class="params">(Path dir, BasicFileAttributes attrs)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dirs.add(dir.toFile());</span><br><span class="line">        <span class="keyword">return</span> FileVisitResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;File&gt; <span class="title">getFileList</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> fileList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;File&gt; <span class="title">getDirs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> dirs;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Google Guava遍历</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 此方法的遍历效率最低，不推荐使用</span></span><br><span class="line"></span><br><span class="line">List&lt;File&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (File file : com.google.common.io.Files.fileTraverser().breadthFirst(<span class="keyword">new</span> File(basePath))) &#123;</span><br><span class="line">    <span class="keyword">if</span>(file.isFile()) &#123;</span><br><span class="line">        list.add(file);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="新增参数"><a href="#新增参数" class="headerlink" title="新增参数"></a>新增参数</h2><p>在<code>MultipleTaildirSourceConfigurationConstants.jar</code>中新增了两个参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 强制刷新缓存时间间隔，默认：2 * 60 * 1000 ms</span></span><br><span class="line">forceRefreshIntervalMs</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对文件过滤的正则表达式，满足正则的文件路径才会监听，默认 null 不进行过滤全部监听 </span></span><br><span class="line">filePathRegularExpression</span><br></pre></td></tr></table></figure><ul><li>具体使用和测试方式参考<a href="https://github.com/imperio-wxm/multiple-taildir-source/blob/master/README.md" target="_blank" rel="noopener">README.md</a></li></ul><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;Flume TaildirSource可实时监控当前目录下一批文件，并记录每个文件最新消费位置，flume进程重启后不会有重复消费的问题；但是无法监听当前目录的子目录，要实现多目录的监听就需要配置多个group，无法做到动态识别子目录&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Flume" scheme="http://imperio-wxm.github.io/categories/Flume/"/>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/Flume/BigData/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Flume" scheme="http://imperio-wxm.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch 缩减索引Shards</title>
    <link href="http://imperio-wxm.github.io/2019/12/07/Elasticsearch-Shrink-Index/"/>
    <id>http://imperio-wxm.github.io/2019/12/07/Elasticsearch-Shrink-Index/</id>
    <published>2019-12-07T06:17:23.000Z</published>
    <updated>2020-01-11T17:04:20.702Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>当对一个Index的Shards分配过多，原本数据量没有这么大造成资源浪费时。通过Shrink操作可以减少Index的Shards，以下将介绍如何shrink index </p></div><a id="more"></a><table><thead><tr><th>software</th><th>version</th></tr></thead><tbody><tr><td>elasticsearch</td><td>6.4.2</td></tr></tbody></table><p>通常<code>Shrink</code>索引的目的是由于建立索引时对数据量预估不足，指定的shards过多真实数据量很小，导致资源浪费</p><h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><ol><li><p>缩小索引是指将原索引分片数缩小到一定数量，但缩小的数量必须为原数量的因子（即原分片数量是新分片倍数），例如8个分片可以缩小到4、2、1个分片，如果原分片数量为素数则只能缩小到一个分片。在缩小开始时，每个分片的复制都必须在同一节点（node）存在</p></li><li><p>首先，以相同配置创建目标索引，但是主分片数量减少。然后硬链接（ hard-linking ） 各部分自原索引到目标索引。（如果系统不支持硬链接，那么索引的所有部分都将复制迁移到新索引，将会花费大量时间）最终，将会恢复目标索引，因为目标索引刚被重新打开就会被关闭</p></li><li><p>为了缩小索引，索引必须被标记为只读，所有分片都会复制到一个相同的节点并且节点健康为绿色</p></li></ol><h1 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h1><h2 id="1-将索引设置为只读模式"><a href="#1-将索引设置为只读模式" class="headerlink" title="1.将索引设置为只读模式"></a>1.将索引设置为只读模式</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT /wxm_character_login_glog_two-190801/_settings</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"settings"</span>: &#123;</span><br><span class="line">    <span class="attr">"index.routing.allocation.require._name"</span>: <span class="string">"your-host-name"</span>, </span><br><span class="line">    <span class="attr">"index.blocks.write"</span>: <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-并且将保证your-host-name中有所有不同分片的其中一份"><a href="#2-并且将保证your-host-name中有所有不同分片的其中一份" class="headerlink" title="2.并且将保证your-host-name中有所有不同分片的其中一份"></a>2.并且将保证<code>your-host-name</code>中有所有不同分片的其中一份</h2><blockquote><p>如何理解<code>所有不同分片的其中一份</code>，例如es 有3个节点 n1、n2、n3，原始index有5个主分片，每个主分片各自有一个副本，在节点中的分布可能是这样的</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n1（pr1，pr2，re3，re4）</span><br><span class="line">n2（re1，re2，pr5）</span><br><span class="line">n3（re5，pr4，pr3）</span><br></pre></td></tr></table></figure><blockquote><p>设置<code>&quot;index.routing.allocation.require._name&quot;: &quot;n3&quot;</code>，则会将（pr1、re1中一个）（pr2、re2中一个）（pr3、re3中一个）移动到n3节点上，变成如下：</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n1（pr2，re3，re4）</span><br><span class="line">n2（re1，pr5）</span><br><span class="line">n3（re5，pr4，pr1，re2，pr3）</span><br></pre></td></tr></table></figure><blockquote><p>这个过程需要点时间，因为要迁移shards，可以通过_cat/recovery?v&amp;h=i,s,t,ty,st,rep,snap,f,fp,b,bp 命令，过滤出st = index的表示正在recover的index查看进度</p></blockquote><h2 id="3-当所有recover完成后，执行如下命令进行真正的索引shrink"><a href="#3-当所有recover完成后，执行如下命令进行真正的索引shrink" class="headerlink" title="3.当所有recover完成后，执行如下命令进行真正的索引shrink"></a>3.当所有recover完成后，执行如下命令进行真正的索引shrink</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST wxm_character_login_glog_two-190801/_shrink/wxm_character_login_glog_two-190803</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"settings"</span>: &#123;</span><br><span class="line">    <span class="attr">"index.number_of_replicas"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">"index.number_of_shards"</span>: <span class="number">1</span>, </span><br><span class="line">    <span class="attr">"index.codec"</span>: <span class="string">"best_compression"</span> </span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"aliases"</span>: &#123;</span><br><span class="line">    <span class="attr">"wxm_character_login_glog_two"</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>也可以通过<code>_cat/recovery?v&amp;h=i,s,t,ty,st,rep,snap,f,fp,b,bp</code> 查看进程</p><p>此过程会新建一个<code>wxm_character_login_glog_two-190803</code>的索引，然后把source index 的数据同步过去</p><p>完成后<code>wxm_character_login_glog_two-190801</code> 便可删除</p></blockquote><p><strong>但是发现一个问题，shrink之后doc number不会变，数据量也正确，但是index 的size 增大了，即使用了<code>best_compression</code>（不适用此参数默认是LZO压缩）也比之前大一些，这个问题提了一个Issue 到 Elasticsearch Discuss：</strong></p><p><a href="https://discuss.elastic.co/t/index-size-increased-after-shrink/212994" target="_blank" rel="noopener">Index size increased after shrink</a></p><h1 id="Shrink-的一些限制："><a href="#Shrink-的一些限制：" class="headerlink" title="Shrink 的一些限制："></a>Shrink 的一些限制：</h1><ul><li><p>目标索引不允许存在，即要Shrink到的那个索引不存在</p></li><li><p>源索引必须具有比目标索引更多的主分片</p></li><li><p>目标索引中的主分片数必须是源索引中主分片数的一个因子，源索引必须具有比目标索引更多的主分片，如果原索引的主分片是质数，无法因式分解，则shrink主分片只能是1</p></li><li><p>源索引的所有分片中的文档总数不得超过2147483519个，这些分片将收缩到目标索引上的单个分片中，这是可以放入单个分片的最大文档数</p></li><li><p>处理收缩过程的节点必须具有足够的可用磁盘空间，新建的shrink索引</p></li><li><p>Shrink API和 Create index API一样，可以有setting和aliases参数，所以可以为目标索引添加一些配置</p></li></ul><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;当对一个Index的Shards分配过多，原本数据量没有这么大造成资源浪费时。通过Shrink操作可以减少Index的Shards，以下将介绍如何shrink index &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://imperio-wxm.github.io/categories/Elasticsearch/"/>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/Elasticsearch/BigData/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Elasticsearch" scheme="http://imperio-wxm.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Ipsets Operation</title>
    <link href="http://imperio-wxm.github.io/2019/11/22/Ipsets-Operation/"/>
    <id>http://imperio-wxm.github.io/2019/11/22/Ipsets-Operation/</id>
    <published>2019-11-22T08:06:35.000Z</published>
    <updated>2020-01-04T08:32:54.564Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>Ip sets是一个批量操作iptables的工具，通过集合规则来管理iptables，如果有大量iptables规则的刷新将是很难维护的，通过ipsets可以有效避免这些问题</p></div><a id="more"></a><p><a href="http://ipset.netfilter.org/" target="_blank" rel="noopener">Ip sets官网</a></p><p><a href="https://github.com/imperio-wxm/java-demos-snippets/tree/master/ipset-manager" target="_blank" rel="noopener">自己实现Ipset manager工具类</a></p><h1 id="Ipsets-Install"><a href="#Ipsets-Install" class="headerlink" title="Ipsets Install"></a>Ipsets Install</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// centos</span><br><span class="line">sudo yum install ipset</span><br><span class="line"></span><br><span class="line">// ubuntu</span><br><span class="line">sudo apt-get install ipset</span><br></pre></td></tr></table></figure><h1 id="Create-Ipset-Sets"><a href="#Create-Ipset-Sets" class="headerlink" title="Create Ipset Sets"></a>Create Ipset Sets</h1><blockquote><p>ipset create SETNAME TYPENAME(method:datatype[,datatype[,datatype]])</p></blockquote><ul><li>SETNAME是创建的ipset的名称</li><li>TYPENAME是ipset的类型<ul><li>method指定ipset中的entry存放的方式（bitmap, hash, list）</li><li>datatype约定了每个entry的格式（ip, net, mac, port, iface）</li></ul></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">sudo ipset create ipwhitelist hash:ip</span><br><span class="line">sudo ipset create ipwhitelist01 hash:ip,port</span><br><span class="line"></span><br><span class="line">sudo ipset --list</span><br><span class="line"></span><br><span class="line">Name: ipwhitelist</span><br><span class="line">Type: hash:ip</span><br><span class="line">Revision: 4</span><br><span class="line">Header: family inet hashsize 1024 maxelem 65536</span><br><span class="line">Size in memory: 128</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br><span class="line"></span><br><span class="line">Name: ipwhitelist01</span><br><span class="line">Type: hash:ip,port</span><br><span class="line">Revision: 5</span><br><span class="line">Header: family inet hashsize 1024 maxelem 65536</span><br><span class="line">Size in memory: 128</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">Supported set types:</span><br><span class="line">    list:set3skbinfo support</span><br><span class="line">    list:set2comment support</span><br><span class="line">    list:set1counters support</span><br><span class="line">    list:set0Initial revision</span><br><span class="line">    hash:mac0Initial revision</span><br><span class="line">    hash:net,iface6skbinfo support</span><br><span class="line">    hash:net,iface5forceadd support</span><br><span class="line">    hash:net,iface4comment support</span><br><span class="line">    hash:net,iface3counters support</span><br><span class="line">    hash:net,iface2/0 network support</span><br><span class="line">    hash:net,iface1nomatch flag support</span><br><span class="line">    hash:net,iface0Initial revision</span><br><span class="line">    hash:net,port7skbinfo support</span><br><span class="line">    hash:net,port6forceadd support</span><br><span class="line">    hash:net,port5comment support</span><br><span class="line">    hash:net,port4counters support</span><br><span class="line">    hash:net,port3nomatch flag support</span><br><span class="line">    hash:net,port2Add/del range support</span><br><span class="line">    hash:net,port1SCTP and UDPLITE support</span><br><span class="line">    hash:net,port,net2skbinfo support</span><br><span class="line">    hash:net,port,net1forceadd support</span><br><span class="line">    hash:net,port,net0initial revision</span><br><span class="line">    hash:net,net2skbinfo support</span><br><span class="line">    hash:net,net1forceadd support</span><br><span class="line">    hash:net,net0initial revision</span><br><span class="line">    hash:net6skbinfo support</span><br><span class="line">    hash:net5forceadd support</span><br><span class="line">    hash:net4comment support</span><br><span class="line">    hash:net3counters support</span><br><span class="line">    hash:net2nomatch flag support</span><br><span class="line">    hash:net1Add/del range support</span><br><span class="line">    hash:net0Initial revision</span><br><span class="line">    hash:ip,port,net7skbinfo support</span><br><span class="line">    hash:ip,port,net6forceadd support</span><br><span class="line">    hash:ip,port,net5comment support</span><br><span class="line">    hash:ip,port,net4counters support</span><br><span class="line">    hash:ip,port,net3nomatch flag support</span><br><span class="line">    hash:ip,port,net2Add/del range support</span><br><span class="line">    hash:ip,port,net1SCTP and UDPLITE support</span><br><span class="line">    hash:ip,port,ip5skbinfo support</span><br><span class="line">    hash:ip,port,ip4forceadd support</span><br><span class="line">    hash:ip,port,ip3comment support</span><br><span class="line">    hash:ip,port,ip2counters support</span><br><span class="line">    hash:ip,port,ip1SCTP and UDPLITE support</span><br><span class="line">    hash:ip,mark2sbkinfo support</span><br><span class="line">    hash:ip,mark1forceadd support</span><br><span class="line">    hash:ip,mark0initial revision</span><br><span class="line">    hash:ip,port5skbinfo support</span><br><span class="line">    hash:ip,port4forceadd support</span><br><span class="line">    hash:ip,port3comment support</span><br><span class="line">    hash:ip,port2counters support</span><br><span class="line">    hash:ip,port1SCTP and UDPLITE support</span><br><span class="line">    hash:ip4skbinfo support</span><br><span class="line">    hash:ip3forceadd support</span><br><span class="line">    hash:ip2comment support</span><br><span class="line">    hash:ip1counters support</span><br><span class="line">    hash:ip0Initial revision</span><br><span class="line">    bitmap:port3skbinfo support</span><br><span class="line">    bitmap:port2comment support</span><br><span class="line">    bitmap:port1counters support</span><br><span class="line">    bitmap:port0Initial revision</span><br><span class="line">    bitmap:ip,mac3skbinfo support</span><br><span class="line">    bitmap:ip,mac2comment support</span><br><span class="line">    bitmap:ip,mac1counters support</span><br><span class="line">    bitmap:ip,mac0Initial revision</span><br><span class="line">    bitmap:ip3skbinfo support</span><br><span class="line">    bitmap:ip2comment support</span><br><span class="line">    bitmap:ip1counters support</span><br><span class="line">    bitmap:ip0Initial revision</span><br></pre></td></tr></table></figure><h2 id="Create-Options"><a href="#Create-Options" class="headerlink" title="Create Options"></a>Create Options</h2><h3 id="1-TTL生命周期-所有集合适用"><a href="#1-TTL生命周期-所有集合适用" class="headerlink" title="1. TTL生命周期  (所有集合适用)"></a>1. TTL生命周期  (所有集合适用)</h3><blockquote><p>设置为0，表示永久生效；可以通过 -exist来进行修改</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sudo ipset create ipwhitelist_timeout hash:ip timeout 5000</span><br><span class="line"></span><br><span class="line">Name: ipwhitelist_timeout</span><br><span class="line">Type: hash:ip</span><br><span class="line">Revision: 4</span><br><span class="line">Header: family inet hashsize 1024 maxelem 65536 timeout 5000</span><br><span class="line">Size in memory: 128</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br><span class="line"></span><br><span class="line">sudo ipset create ipwhitelist_timeout01 hash:ip timeout 0</span><br><span class="line"></span><br><span class="line">Name: ipwhitelist_timeout01</span><br><span class="line">Type: hash:ip</span><br><span class="line">Revision: 4</span><br><span class="line">Header: family inet hashsize 1024 maxelem 65536 timeout 0</span><br><span class="line">Size in memory: 128</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br></pre></td></tr></table></figure><h3 id="2-Comment-备注（所有集合适用）"><a href="#2-Comment-备注（所有集合适用）" class="headerlink" title="2. Comment 备注（所有集合适用）"></a>2. Comment 备注（所有集合适用）</h3><blockquote><p>内核和ipset本身完全忽略这个字符串，纯粹是为了注释</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo ipset create ipwhitelist_comment hash:ip comment</span><br><span class="line">sudo ipset add ipwhitelist_comment 10.1.8.145 comment "测试"</span><br><span class="line"></span><br><span class="line">Name: ipwhitelist_comment</span><br><span class="line">Type: hash:ip</span><br><span class="line">Revision: 4</span><br><span class="line">Header: family inet hashsize 1024 maxelem 65536 comment</span><br><span class="line">Size in memory: 224</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br><span class="line">10.1.8.145 comment "测试"</span><br></pre></td></tr></table></figure><h3 id="3-Hashsize初始哈希大小-、Maxelem集合存储最大数量"><a href="#3-Hashsize初始哈希大小-、Maxelem集合存储最大数量" class="headerlink" title="3. Hashsize初始哈希大小 、Maxelem集合存储最大数量"></a>3. Hashsize初始哈希大小 、Maxelem集合存储最大数量</h3><blockquote><p>hashsize定义了集合的初始哈希大小，默认值为1024。哈希大小必须是2的幂，内核会自动舍入两个哈希大小的非幂到第一个正确的值</p><p>maxelem定义了可以存储在集合中的元素的最大数量，默认值为65536</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo ipset create ipwhitelist_size hash:ip hashsize 2408  maxelem 500</span><br><span class="line"></span><br><span class="line">Name: ipwhitelist_size</span><br><span class="line">Type: hash:ip</span><br><span class="line">Revision: 4</span><br><span class="line">Header: family inet hashsize 4096 maxelem 500</span><br><span class="line">Size in memory: 128</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br></pre></td></tr></table></figure><h2 id="Set-Types"><a href="#Set-Types" class="headerlink" title="Set Types"></a>Set Types</h2><h3 id="1-Method存储方式"><a href="#1-Method存储方式" class="headerlink" title="1. Method存储方式"></a>1. Method存储方式</h3><blockquote><p>bitmap和list: 使用固定大小的存储</p><p>hash: 使用hash表来存储元素。但为了避免Hash表键冲突,在ipset会在hash表key用完后，若又有新增条目，则ipset将自动对hash表扩大</p></blockquote><p><code>这里需要注意的是，一旦使用 hash:ip和hash:ip,port 方式 进行ipset配置，而ip又非常多的话，可能会出现下面的情况.(最后是使用hash：net替换hash：ip解决的)</code></p><h3 id="2-Datatype数据类型"><a href="#2-Datatype数据类型" class="headerlink" title="2. Datatype数据类型"></a>2. Datatype数据类型</h3><blockquote><p>支持的类型有：ip, net, mac, port, iface</p></blockquote><ul><li>hash:ip</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 使用哈希存储ip主机地址(默认)或网络地址。零值IP地址不能存储在散列中</span><br><span class="line">2. 支持单个ip: 10.1.1.1，也支持ip段：10.1.1.1-10.1.1.100</span><br></pre></td></tr></table></figure><ul><li>hash:net</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1. 使用集合存储不同大小的IP网络地址。前缀大小为零的网络地址不能存储在这种类型的集合中</span><br></pre></td></tr></table></figure><ul><li>hash:ip,port</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1. 使用hash存储IP地址和端口号对。端口号与协议(默认TCP)一起，不能使用零协议号</span><br></pre></td></tr></table></figure><ul><li>hash:ip,port,net</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1. 使用hash存储IP地址、端口号和IP网络地址三元组。端口号与协议(默认TCP)一起，不能使用零协议号。前缀大小为零的网络地址也不能存储</span><br></pre></td></tr></table></figure><h1 id="Add-Rules-To-Sets"><a href="#Add-Rules-To-Sets" class="headerlink" title="Add Rules To Sets"></a>Add Rules To Sets</h1><blockquote><p>ipset add SETNAME ENTRY</p></blockquote><ul><li>ENTRY 要符合创建ipset的类型</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">sudo ipset add ipwhitelist 10.1.8.140</span><br><span class="line">sudo ipset add ipwhitelist01 10.1.8.141,5240</span><br><span class="line"></span><br><span class="line">sudo ipset -list</span><br><span class="line">Name: ipwhitelist</span><br><span class="line">Type: hash:ip</span><br><span class="line">Revision: 4</span><br><span class="line">Header: family inet hashsize 1024 maxelem 65536</span><br><span class="line">Size in memory: 224</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br><span class="line">10.1.8.140</span><br><span class="line"></span><br><span class="line">Name: ipwhitelist01</span><br><span class="line">Type: hash:ip,port</span><br><span class="line">Revision: 5</span><br><span class="line">Header: family inet hashsize 1024 maxelem 65536</span><br><span class="line">Size in memory: 192</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br><span class="line">10.1.8.141,tcp:5240</span><br></pre></td></tr></table></figure><blockquote><p> 添加时指定协议</p></blockquote><ul><li>ip,protocal:port</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo ipset add ipwhitelist01 10.1.8.141,udp:5340</span><br><span class="line"></span><br><span class="line">Name: ipwhitelist01</span><br><span class="line">Type: hash:ip,port</span><br><span class="line">Revision: 5</span><br><span class="line">Header: family inet hashsize 1024 maxelem 65536</span><br><span class="line">Size in memory: 256</span><br><span class="line">References: 0</span><br><span class="line">Members:</span><br><span class="line">10.1.8.141,tcp:5240</span><br><span class="line">10.1.8.141,udp:5340</span><br></pre></td></tr></table></figure><p><code>末尾加-exist表示已经存在就忽略，不然会报已经存在的错误</code></p><h1 id="Check-Ipset-Entry"><a href="#Check-Ipset-Entry" class="headerlink" title="Check Ipset Entry"></a>Check Ipset Entry</h1><blockquote><p>ipset test SETNAME ENTRY</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo ipset test ipwhitelist 10.1.8.140</span><br><span class="line">10.1.8.140 is in set ipwhitelist.</span><br><span class="line"></span><br><span class="line">sudo ipset test ipwhitelist01 10.1.8.141,5240</span><br><span class="line">10.1.8.141,tcp:5240 is in set ipwhitelist01.</span><br><span class="line"></span><br><span class="line">sudo ipset test ipwhitelist01 10.1.8.141,5241</span><br><span class="line">10.1.8.141,tcp:5241 is NOT in set ipwhitelist01.</span><br></pre></td></tr></table></figure><h1 id="Del、Flush、Destroy-Sets"><a href="#Del、Flush、Destroy-Sets" class="headerlink" title="Del、Flush、Destroy Sets"></a>Del、Flush、Destroy Sets</h1><blockquote><p>ipset del SETNAME ENTRY<br>ipset flush SETNAME<br>ipset destroy SETNAME</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 删除某一条记录</span><br><span class="line">sudo ipset del ipwhitelist 10.1.8.140</span><br><span class="line">// 清空某个集合</span><br><span class="line">sudo ipset flush ipwhitelist</span><br><span class="line">// 删除整个集合</span><br><span class="line">sudo ipset destroy ipwhitelist</span><br></pre></td></tr></table></figure><p><code>末尾加-exist表示不存在就忽略，不然会报不存在的错误</code></p><h1 id="Import-and-Export-ipsets"><a href="#Import-and-Export-ipsets" class="headerlink" title="Import and Export ipsets"></a>Import and Export ipsets</h1><blockquote><p>sudo ipset save SETNAME -f FILE_PATH</p><p>sudo ipset restore -f FILE_PATH</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 备份到文件</span><br><span class="line">sudo ipset save ipwhitelist01 -f /home/wxmimperio/bigdata/ipwhitelist01.txt</span><br><span class="line"></span><br><span class="line">vim  /home/wxmimperio/bigdata/ipwhitelist01.txt</span><br><span class="line">create ipwhitelist01 hash:ip,port family inet hashsize 1024 maxelem 65536</span><br><span class="line">add ipwhitelist01 10.1.8.141,tcp:5240</span><br><span class="line">add ipwhitelist01 10.1.8.141,udp:5340</span><br><span class="line"></span><br><span class="line">// 删除列表</span><br><span class="line">sudo ipset destroy ipwhitelist01</span><br><span class="line">// 从文件恢复</span><br><span class="line">sudo ipset restore -f /home/wxmimperio/bigdata/ipwhitelist01.txt</span><br></pre></td></tr></table></figure><h1 id="Use-of-ipset-and-iptables"><a href="#Use-of-ipset-and-iptables" class="headerlink" title="Use of ipset and iptables"></a>Use of ipset and iptables</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在iptables中使用ipset，只要加上-m set --match-set即可</span><br></pre></td></tr></table></figure><h2 id="1-目的ip使用ipset（ipset集合为bbb）"><a href="#1-目的ip使用ipset（ipset集合为bbb）" class="headerlink" title="1.目的ip使用ipset（ipset集合为bbb）"></a>1.目的ip使用ipset（ipset集合为bbb）</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -s 192.168.100.36 -m set --match-set bbb dst -j DROP</span><br></pre></td></tr></table></figure><h2 id="2-源ip使用ipset（ipset集合为aaa）"><a href="#2-源ip使用ipset（ipset集合为aaa）" class="headerlink" title="2.源ip使用ipset（ipset集合为aaa）"></a>2.源ip使用ipset（ipset集合为aaa）</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -m set --match-set aaa src -d 192.168.100.36 -j DROP</span><br></pre></td></tr></table></figure><h2 id="3-源和目的都使用ipset（源ip集合为aaa，目的ip集合为bbb）"><a href="#3-源和目的都使用ipset（源ip集合为aaa，目的ip集合为bbb）" class="headerlink" title="3.源和目的都使用ipset（源ip集合为aaa，目的ip集合为bbb）"></a>3.源和目的都使用ipset（源ip集合为aaa，目的ip集合为bbb）</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -m set --match-set aaa src -m set --match-set bbb dst -j DROP</span><br></pre></td></tr></table></figure><h2 id="4-设定白名单（源ip集合为ipwhitelist）"><a href="#4-设定白名单（源ip集合为ipwhitelist）" class="headerlink" title="4.设定白名单（源ip集合为ipwhitelist）"></a>4.设定白名单（源ip集合为ipwhitelist）</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -A INPUT -p tcp -m set --match-set ipwhitelist src -j ACCEPT</span><br></pre></td></tr></table></figure><h2 id="5-为ipset-list开启特定端口"><a href="#5-为ipset-list开启特定端口" class="headerlink" title="5.为ipset list开启特定端口"></a>5.为ipset list开启特定端口</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -A INPUT -p tcp -m multiport --destination-port 21,22 -m set --match-set ipwhitelist src -j ACCEPT</span><br></pre></td></tr></table></figure><h2 id="6-在hash-ip-port情况下，指定源、目标的权限"><a href="#6-在hash-ip-port情况下，指定源、目标的权限" class="headerlink" title="6.在hash ip:port情况下，指定源、目标的权限"></a>6.在hash ip:port情况下，指定源、目标的权限</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -p tcp -m set --match-set ipwhitelist src,dst -j REJECT</span><br><span class="line">表示：src为源ip，dst为目标端口</span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;Ip sets是一个批量操作iptables的工具，通过集合规则来管理iptables，如果有大量iptables规则的刷新将是很难维护的，通过ipsets可以有效避免这些问题&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Kudu-Deploy-Non-ClouderaManager</title>
    <link href="http://imperio-wxm.github.io/2019/08/23/Kudu-Deploy-Non-ClouderaManager/"/>
    <id>http://imperio-wxm.github.io/2019/08/23/Kudu-Deploy-Non-ClouderaManager/</id>
    <published>2019-08-23T09:26:43.000Z</published>
    <updated>2019-08-24T16:05:46.035Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>CDH 5.10.x开始支持Kudu 1.2.0+的管理，但是由于版本匹配问题，很多时候想用比较新的Kudu但是CDH版本不允许，此时就需要脱离CM手动搭建集群 </p></div><a id="more"></a><h2 id="环境与前期准备"><a href="#环境与前期准备" class="headerlink" title="环境与前期准备"></a>环境与前期准备</h2><ul><li>Kudu与CDH版本的依赖关系</li></ul><p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_kudu" target="_blank" rel="noopener">CDH版本支持</a></p><ul><li>RPMS查询</li></ul><p>(具体版本可更换url上的版本号进行查看)</p><p><a href="https://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5.16.2/RPMS/x86_64/" target="_blank" rel="noopener">CDH5.16.2-Kudu 1.7.0</a></p><ul><li>root用户</li></ul><p>在安装部署Kudu的时候必须是root用户，过程中还会涉及新建kudu用户</p><h2 id="安装依赖与配置"><a href="#安装依赖与配置" class="headerlink" title="安装依赖与配置"></a>安装依赖与配置</h2><ul><li>依赖安装</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install autoconf automake cyrus-sasl-devel cyrus-sasl-gssapi \</span><br><span class="line">  cyrus-sasl-plain flex gcc gcc-c++ gdb git java-<span class="number">1.8</span>.0-openjdk-devel \</span><br><span class="line">  krb5-server krb5-workstation libtool make openssl-devel patch \</span><br><span class="line">  pkgconfig redhat-lsb-core rsync unzip vim-common which</span><br></pre></td></tr></table></figure><ul><li>修改系统最大文件句柄数</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/security/limits.conf </span><br><span class="line"></span><br><span class="line">* soft nofile <span class="number">65535</span></span><br><span class="line">* hard nofile <span class="number">65535</span></span><br></pre></td></tr></table></figure><ul><li>域名互通</li></ul><blockquote><p>如果节点之间有域名，则必须添加hosts，使得各节点之间可以用域名互通</p></blockquote><h2 id="下载并安装RPM安装包"><a href="#下载并安装RPM安装包" class="headerlink" title="下载并安装RPM安装包"></a>下载并安装RPM安装包</h2><p><a href="https://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5.16.2/RPMS/x86_64/" target="_blank" rel="noopener">CDH5.16.2-Kudu 1.7.0下载</a></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 依次wget</span></span><br><span class="line"></span><br><span class="line">kudu-<span class="number">1.7</span>.0+cdh5.16.2+<span class="number">0</span>-<span class="number">1</span>.cdh5.16.2.p0.24.el6.x86_64.rpm</span><br><span class="line">kudu-client-devel-<span class="number">1.7</span>.0+cdh5.16.2+<span class="number">0</span>-<span class="number">1</span>.cdh5.16.2.p0.24.el6.x86_64.rpm</span><br><span class="line">kudu-client0-<span class="number">1.7</span>.0+cdh5.16.2+<span class="number">0</span>-<span class="number">1</span>.cdh5.16.2.p0.24.el6.x86_64.rpm</span><br><span class="line">kudu-debuginfo-<span class="number">1.7</span>.0+cdh5.16.2+<span class="number">0</span>-<span class="number">1</span>.cdh5.16.2.p0.24.el6.x86_64.rpm</span><br><span class="line">kudu-master-<span class="number">1.7</span>.0+cdh5.16.2+<span class="number">0</span>-<span class="number">1</span>.cdh5.16.2.p0.24.el6.x86_64.rpm</span><br><span class="line">kudu-tserver-<span class="number">1.7</span>.0+cdh5.16.2+<span class="number">0</span>-<span class="number">1</span>.cdh5.16.2.p0.24.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下载目录</span></span><br><span class="line">rpm -ivh --nodeps *</span><br><span class="line"></span><br><span class="line">Preparing...             ########################################### [100%]</span><br><span class="line">1:kudu                   ########################################### [ 17%]</span><br><span class="line">2:kudu-client0           ########################################### [ 33%]</span><br><span class="line">3:kudu-client-devel      ########################################### [ 50%]</span><br><span class="line">4:kudu-master            ########################################### [ 67%]</span><br><span class="line">5:kudu-tserver           ########################################### [ 83%]</span><br><span class="line">6:kudu-debuginfo         ########################################### [100%]</span><br></pre></td></tr></table></figure><h2 id="创建数据目录，用户授权"><a href="#创建数据目录，用户授权" class="headerlink" title="创建数据目录，用户授权"></a>创建数据目录，用户授权</h2><blockquote><p>由于Kudu对文件系统要求很高，坏盘会导致节点Crash，元数据损坏会导致节点无法重启，所以尽量将 <code>data</code>、<code>wal、metadata</code>、<code>logs</code> 分多盘符存储</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /app/kudu/master/data</span><br><span class="line">mkdir -p /app/kudu/master/logs</span><br><span class="line">mkdir -p /app/kudu/master/metadata</span><br><span class="line">mkdir -p /app/kudu/master/wal</span><br><span class="line"></span><br><span class="line"><span class="comment">// 授权必须为kudu用户</span></span><br><span class="line">chown -R kudu:kudu kudu/</span><br></pre></td></tr></table></figure><h2 id="Master-and-Tserver-config"><a href="#Master-and-Tserver-config" class="headerlink" title="Master and Tserver config"></a>Master and Tserver config</h2><ul><li>Master</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/kudu/conf/master.gflagfile</span><br><span class="line"></span><br><span class="line">--fromenv=rpc_bind_addresses</span><br><span class="line">--fromenv=log_dir</span><br><span class="line"></span><br><span class="line">--fs_wal_dir=/app/kudu/master/wal</span><br><span class="line">--fs_data_dirs=/app/kudu/master/data</span><br><span class="line">--fs_metadata_dir=/app/kudu/master/metadata</span><br><span class="line">--log_dir=/app/kudu/master/logs</span><br><span class="line">--master_addresses=xxxx:<span class="number">7051</span>,xxxx:<span class="number">7051</span>,xxxx:<span class="number">7051</span></span><br><span class="line">--block_cache_capacity_mb=<span class="number">4096</span></span><br><span class="line">--max_log_size=<span class="number">40</span></span><br></pre></td></tr></table></figure><ul><li>Tserver</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/kudu/conf/tserver.gflagfile</span><br><span class="line"></span><br><span class="line">--fromenv=rpc_bind_addresses</span><br><span class="line">--fromenv=log_dir</span><br><span class="line"></span><br><span class="line">--fs_wal_dir=/app/kudu/tserver/wal</span><br><span class="line">--fs_data_dirs=/app/kudu/tserver/data</span><br><span class="line">--fs_metadata_dir=/app/kudu/tserver/metadata</span><br><span class="line">--log_dir=/app/kudu/tserver/logs</span><br><span class="line">--tserver_master_addrs=xxxx:<span class="number">7051</span>,xxxx:<span class="number">7051</span>,xxxx:<span class="number">7051</span></span><br><span class="line">--block_cache_capacity_mb=<span class="number">4096</span></span><br><span class="line">--max_log_size=<span class="number">40</span></span><br></pre></td></tr></table></figure><h2 id="NTP同步"><a href="#NTP同步" class="headerlink" title="NTP同步"></a>NTP同步</h2><blockquote><p>Kudu对NTP同步要求很高，不同步会导致节点Crash</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install ntp</span><br><span class="line"></span><br><span class="line">sudo /etc/init.d/ntpd restart</span><br></pre></td></tr></table></figure><h2 id="启动、停止、重启"><a href="#启动、停止、重启" class="headerlink" title="启动、停止、重启"></a>启动、停止、重启</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// start</span></span><br><span class="line">/etc/init.d/kudu-master start</span><br><span class="line">/etc/init.d/kudu-tserver start</span><br><span class="line"></span><br><span class="line"><span class="comment">// restart</span></span><br><span class="line">/etc/init.d/kudu-master restart</span><br><span class="line">/etc/init.d/kudu-tserver restart</span><br><span class="line"></span><br><span class="line"><span class="comment">// stop</span></span><br><span class="line">/etc/init.d/kudu-master stop</span><br><span class="line">/etc/init.d/kudu-tserver stop</span><br></pre></td></tr></table></figure><h2 id="Web-UI"><a href="#Web-UI" class="headerlink" title="Web UI"></a>Web UI</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// master管理界面</span></span><br><span class="line">http:<span class="comment">//ip:8051/masters</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// tablet server管理界面</span></span><br><span class="line">http:<span class="comment">//ip:8050/</span></span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;CDH 5.10.x开始支持Kudu 1.2.0+的管理，但是由于版本匹配问题，很多时候想用比较新的Kudu但是CDH版本不允许，此时就需要脱离CM手动搭建集群 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Kudu" scheme="http://imperio-wxm.github.io/categories/Kudu/"/>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/Kudu/BigData/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Kudu" scheme="http://imperio-wxm.github.io/tags/Kudu/"/>
    
  </entry>
  
  <entry>
    <title>Hudi Getting Started</title>
    <link href="http://imperio-wxm.github.io/2019/08/09/Hudi-Introduction/"/>
    <id>http://imperio-wxm.github.io/2019/08/09/Hudi-Introduction/</id>
    <published>2019-08-09T06:50:43.000Z</published>
    <updated>2019-08-10T04:58:51.717Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>Hudi是类似Carbondata的<code>软数据层</code>，可以支持streaming的写入、读取，同时也支持将metadata同步到hive，提供Spark、Hive、Presto的SQL查询 </p></div><a id="more"></a><table><thead><tr><th>software</th><th>version</th></tr></thead><tbody><tr><td>Spark</td><td>2.4.0.cloudera1</td></tr><tr><td>Hadoop</td><td>2.6.0-cdh5.11.1</td></tr><tr><td>Hive</td><td>1.1.0-cdh5.11.1</td></tr><tr><td>Hudi</td><td>0.4.7</td></tr></tbody></table><h1 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h1><p>可以查看官方文档的Quickstart进行编译：</p><p><a href="https://hudi.apache.org/quickstart.html" target="_blank" rel="noopener">Quickstart</a></p><p><a href="https://github.com/apache/incubator-hudi" target="_blank" rel="noopener">Github</a> clone 最新的tag</p><ul><li>支持Java 8+</li><li>支持Hive 1+（Hive 2+ 据说有不少问题，可以通过Github Issue查询）</li><li>支持Spark2.x+</li><li>支持Apache、CDH Hadoop</li></ul><blockquote><p>编译可选项</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd incubator-hudi-hoodie-<span class="number">0.4</span>.7</span><br><span class="line"></span><br><span class="line">mvn clean install -DskipITs -DskipTests -Dhadoop.version=<span class="number">2.6</span>.0-cdh5.11.1 -Dhive.version=<span class="number">1.1</span>.0-cdh5.11.1</span><br></pre></td></tr></table></figure><p style="color:red">参数 -DskipITs用来跳过integration test，test中含有docker test，没有docker环境无法编译通过</p><p><a href="https://github.com/apache/incubator-hudi/issues/632" target="_blank" rel="noopener">-DskipITs相关issue</a></p><ul><li>编译成功</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary <span class="keyword">for</span> Hoodie <span class="number">0.4</span>.7:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] Hoodie ............................................. SUCCESS [  <span class="number">2.894</span> s]</span><br><span class="line">[INFO] hoodie-common ...................................... SUCCESS [ <span class="number">20.807</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-mr ................................... SUCCESS [  <span class="number">2.372</span> s]</span><br><span class="line">[INFO] hoodie-hive ........................................ SUCCESS [  <span class="number">1.500</span> s]</span><br><span class="line">[INFO] hoodie-timeline-service ............................ SUCCESS [ <span class="number">15.561</span> s]</span><br><span class="line">[INFO] hoodie-client ...................................... SUCCESS [  <span class="number">5.584</span> s]</span><br><span class="line">[INFO] hoodie-spark ....................................... SUCCESS [ <span class="number">36.749</span> s]</span><br><span class="line">[INFO] hoodie-utilities ................................... SUCCESS [ <span class="number">35.936</span> s]</span><br><span class="line">[INFO] hoodie-cli ......................................... SUCCESS [ <span class="number">14.495</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-mr-bundle ............................ SUCCESS [  <span class="number">2.126</span> s]</span><br><span class="line">[INFO] hoodie-hive-bundle ................................. SUCCESS [ <span class="number">23.160</span> s]</span><br><span class="line">[INFO] hoodie-spark-bundle ................................ SUCCESS [<span class="number">02</span>:<span class="number">43</span> min]</span><br><span class="line">[INFO] hoodie-presto-bundle ............................... SUCCESS [ <span class="number">23.024</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-docker ............................... SUCCESS [  <span class="number">0.616</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-base-docker .......................... SUCCESS [  <span class="number">0.491</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-namenode-docker ...................... SUCCESS [  <span class="number">0.079</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-datanode-docker ...................... SUCCESS [  <span class="number">0.076</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-history-docker ....................... SUCCESS [  <span class="number">0.072</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-hive-docker .......................... SUCCESS [  <span class="number">0.763</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-sparkbase-docker ..................... SUCCESS [  <span class="number">0.095</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-sparkmaster-docker ................... SUCCESS [  <span class="number">0.089</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-sparkworker-docker ................... SUCCESS [  <span class="number">0.087</span> s]</span><br><span class="line">[INFO] hoodie-hadoop-sparkadhoc-docker .................... SUCCESS [  <span class="number">0.085</span> s]</span><br><span class="line">[INFO] hoodie-integ-test .................................. SUCCESS [  <span class="number">1.102</span> s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time:  <span class="number">05</span>:<span class="number">51</span> min</span><br><span class="line">[INFO] Finished at: <span class="number">2019</span>-<span class="number">08</span>-<span class="number">09</span>T15:<span class="number">46</span>:<span class="number">24</span>+<span class="number">08</span>:<span class="number">00</span></span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><h1 id="Write-With-Spark"><a href="#Write-With-Spark" class="headerlink" title="Write With Spark"></a>Write With Spark</h1><h2 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h2><ol><li>从hive的一张表查询数据</li><li>转换成DF后写出到Hudi的新表</li><li>将这个新表sync到hive的metadata</li></ol><h2 id="Write-Code"><a href="#Write-Code" class="headerlink" title="Write Code"></a>Write Code</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">String warehouseLocation = <span class="keyword">new</span> File(<span class="string">"spark-warehouse"</span>).getAbsolutePath();</span><br><span class="line">SparkSession spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.appName(<span class="string">"Spark Hudi Write Test"</span>)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; hiveQuery = spark.sql(<span class="string">" select * from dw.xxx where part_date='2019-08-02'"</span>);</span><br><span class="line"></span><br><span class="line">hiveQuery.write()</span><br><span class="line">.format(<span class="string">"com.uber.hoodie"</span>)</span><br><span class="line">.option(DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY(), <span class="keyword">true</span>)</span><br><span class="line">.option(DataSourceWriteOptions.HIVE_URL_OPT_KEY(), <span class="string">"jdbc:hive2://xxx:10000"</span>)</span><br><span class="line">.option(DataSourceWriteOptions.HIVE_DATABASE_OPT_KEY(), <span class="string">"dw"</span>)</span><br><span class="line">.option(DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY(), <span class="keyword">true</span>)</span><br><span class="line">.option(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY(), <span class="string">"hoodie_wxm_test"</span>)</span><br><span class="line">.option(DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY(), <span class="string">"part_date"</span>)</span><br><span class="line">.option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), <span class="string">"key"</span>)</span><br><span class="line">.option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), <span class="string">"part_date"</span>)</span><br><span class="line">.option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), <span class="string">"event_time"</span>)</span><br><span class="line">.option(HoodieWriteConfig.TABLE_NAME, <span class="string">"hoodie_wxm_test"</span>)</span><br><span class="line">.mode(SaveMode.Append)</span><br><span class="line">.save(<span class="string">"hudi data path"</span>);</span><br></pre></td></tr></table></figure><table><thead><tr><th>Options</th><th>描述</th></tr></thead><tbody><tr><td>HIVE_ASSUME_DATE_PARTITION_OPT_KEY</td><td>如果hive以日期分区，则日期的format格式，默认：yyyy/mm/dd</td></tr><tr><td>HIVE_URL_OPT_KEY</td><td>hive metastore url</td></tr><tr><td>HIVE_DATABASE_OPT_KEY</td><td>sync 到hive的库名</td></tr><tr><td>HIVE_SYNC_ENABLED_OPT_KEY</td><td>是否将hudi表的元数据sync到hive</td></tr><tr><td>HIVE_TABLE_OPT_KEY</td><td>sync 到hive 的表名</td></tr><tr><td>HIVE_PARTITION_FIELDS_OPT_KEY</td><td>sync 到hive表的分区键从哪个列中提取</td></tr><tr><td>RECORDKEY_FIELD_OPT_KEY</td><td>hudi 中recordKey从哪个列中提取</td></tr><tr><td>PARTITIONPATH_FIELD_OPT_KEY</td><td>hudi 中分区键从哪个列中提取</td></tr><tr><td>PRECOMBINE_FIELD_OPT_KEY</td><td>hudi 中预合并从哪个列中提取</td></tr><tr><td>HoodieWriteConfig.TABLE_NAME</td><td>hudi 的表名</td></tr></tbody></table><p>更多Options请参考<a href="https://hudi.apache.org/configurations.html" target="_blank" rel="noopener">Configurations</a></p><h2 id="Submit-Job"><a href="#Submit-Job" class="headerlink" title="Submit Job"></a>Submit Job</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要将 hoodie-hadoop-mr-0.4.7.jar、hoodie-spark-0.4.7.jar、hoodie-hive-0.4.7.jar、hoodie-common-0.4.7.jar 引入到spark shell 提交依赖中</span></span><br><span class="line"></span><br><span class="line">spark2-submit --jars basePath/hoodie-hadoop-mr-<span class="number">0.4</span>.7.jar,basePath/hoodie-spark-<span class="number">0.4</span>.7.jar,basePath/hoodie-hive-<span class="number">0.4</span>.7.jar,basePath/hoodie-common-<span class="number">0.4</span>.7.jar \</span><br><span class="line">--<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">wxmimperio</span>.<span class="title">spark</span>.<span class="title">hudi</span>.<span class="title">HudiWrite</span>  --<span class="title">master</span> <span class="title">yarn</span> --<span class="title">deploy</span>-<span class="title">mode</span> <span class="title">cluster</span>  \</span></span><br><span class="line"><span class="class">--<span class="title">driver</span>-<span class="title">memory</span> 4<span class="title">g</span> --<span class="title">executor</span>-<span class="title">memory</span> 2<span class="title">g</span> --<span class="title">executor</span>-<span class="title">cores</span> 1 <span class="title">xxxx</span>.<span class="title">jar</span></span></span><br></pre></td></tr></table></figure><h2 id="自定义日期分区规则"><a href="#自定义日期分区规则" class="headerlink" title="自定义日期分区规则"></a>自定义日期分区规则</h2><blockquote><p>通过HIVE_ASSUME_DATE_PARTITION_OPT_KEY参数设定后，默认的Date分区是yyyy/mm/dd，而我的业务场景下是yyyy-mm-dd，需要重写（此部分可以根据实际业务分区场景自行实现<code>PartitionValueExtractor</code>接口）</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DayPartitionValueExtractor</span> <span class="keyword">implements</span> <span class="title">PartitionValueExtractor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> DateTimeFormatter dtfOut;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DayPartitionValueExtractor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.dtfOut = DateTimeFormat.forPattern(<span class="string">"yyyy-MM-dd"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> DateTimeFormatter <span class="title">getDtfOut</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (dtfOut == <span class="keyword">null</span>) &#123;</span><br><span class="line">            dtfOut = DateTimeFormat.forPattern(<span class="string">"yyyy-MM-dd"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dtfOut;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">extractPartitionValuesInPath</span><span class="params">(String partitionPath)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// partition path is expected to be in this format yyyy/mm/dd</span></span><br><span class="line">        String[] splits = partitionPath.split(<span class="string">"-"</span>);</span><br><span class="line">        <span class="keyword">if</span> (splits.length != <span class="number">3</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">                    <span class="string">"Partition path "</span> + partitionPath + <span class="string">" is not in the form yyyy-mm-dd "</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Get the partition part and remove the / as well at the end</span></span><br><span class="line">        <span class="keyword">int</span> year = Integer.parseInt(splits[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">int</span> mm = Integer.parseInt(splits[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">int</span> dd = Integer.parseInt(splits[<span class="number">2</span>]);</span><br><span class="line">        DateTime dateTime = <span class="keyword">new</span> DateTime(year, mm, dd, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> Lists.newArrayList(getDtfOut().print(dateTime));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>添加配置</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.option(DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY(),<span class="string">"com.wxmimperio.spark.hudi.DayPartitionValueExtractor"</span>)</span><br></pre></td></tr></table></figure><h2 id="Job运行"><a href="#Job运行" class="headerlink" title="Job运行"></a>Job运行</h2><p>通过查看Spark Job logs，可以发现：</p><p><strong>1.默认Hudi支持<code>parquet</code>格式输出，关于ORC File的支持有相关Issue: <a href="https://github.com/apache/incubator-hudi/pull/657" target="_blank" rel="noopener">HUDI-57 support orc file</a></strong></p><p><strong>2.会先生成parquet文件，然后用hive.cli相关命令建表，最后sync数据</strong></p><p>sync到hive后，表结构会多几个Hudi的字段：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">`_hoodie_commit_time` string, </span><br><span class="line">`_hoodie_commit_seqno` string, </span><br><span class="line">`_hoodie_record_key` string, </span><br><span class="line">`_hoodie_partition_path` string, </span><br><span class="line">`_hoodie_file_name` string,</span><br></pre></td></tr></table></figure><p><strong>3.分区的File Path与正常Hive的不同</strong></p><p>正常情况下Hive的分区是key=value形式，</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">insert overwrite table new_table <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-08-02'</span>)</span>  select xxx from old_tabl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// hdfs file path: /wxm/hudi/data/hoodie_test/part_date=2019-08-02</span></span><br></pre></td></tr></table></figure><p>Hudi默认情况是: <code>hudi data path/2019-08-02</code>，同时如果是第一次建表，当前分区不会自动添加，第二次运行会自动添加分区并load data，不清楚这是我测试有问题还是Hudi机制就是这样</p><p>已经提交了相关Issue：<a href="https://github.com/apache/incubator-hudi/issues/828" target="_blank" rel="noopener">Synchronizing to hive partition is incorrect</a></p><p><strong>4.java.lang.ClassNotFoundException: com.uber.hoodie.hadoop.HoodieInputFormat</strong></p><p>在我前几次执行Job的时候，通过logs得知parquet文件已经生成，但是在sync hive的时候会报<code>com.uber.hoodie.hadoop.HoodieInputFormat</code>类不存在</p><p>这个类在hoodie-hadoop-mr-0.4.7.jar中，可是spark提交jar的时候，我已经将相关jar添加到了依赖中还是报这个错</p><p>问题在于Hudi建hive表指定的InputFormat是<code>com.uber.hoodie.hadoop.HoodieInputFormat</code>：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: com.uber.hoodie.hive.HoodieHiveSyncException: Failed in executing SQL CREATE EXTERNAL TABLE  IF NOT EXISTS dw.wxm_hoodie_test( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `age` string, `key` string, `name` string, `timestamp` string) PARTITIONED BY (part string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'com.uber.hoodie.hadoop.HoodieInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/wxm/hudi/data'</span><br></pre></td></tr></table></figure><p>需要将hoodie-hadoop-mr-0.4.7.jar、hoodie-common-0.4.7.jar两个包放到<code>$HIVE_HOME/lib</code>下，重启hive cluster</p><p>已经提交了相关Issue：<a href="https://github.com/apache/incubator-hudi/issues/827" target="_blank" rel="noopener">java.lang.ClassNotFoundException: com.uber.hoodie.hadoop.HoodieInputFormat</a></p><h1 id="Read-With-Spark"><a href="#Read-With-Spark" class="headerlink" title="Read With Spark"></a>Read With Spark</h1><h2 id="思路：-1"><a href="#思路：-1" class="headerlink" title="思路："></a>思路：</h2><ol><li><p>直接通过spark用sql查询sync到hive的表（不做介绍，和读hive一样）</p></li><li><p>通过client读取parquet文件</p></li></ol><h2 id="Read-Code"><a href="#Read-Code" class="headerlink" title="Read Code"></a>Read Code</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SparkSession spark = SparkSession</span><br><span class="line">    .builder()</span><br><span class="line">    .appName(<span class="string">"Spark Hudi Read Test"</span>)</span><br><span class="line">    .getOrCreate();</span><br><span class="line"></span><br><span class="line">spark.read()</span><br><span class="line">    .format(<span class="string">"com.uber.hoodie"</span>)</span><br><span class="line">    .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY(), DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL())</span><br><span class="line">    .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY(), <span class="string">"20190809120720"</span>)</span><br><span class="line">    .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY(), <span class="string">"20190809120729"</span>)</span><br><span class="line">    .load(<span class="string">"hudi file path"</span>)</span><br><span class="line">    .limit(<span class="number">10</span>)</span><br><span class="line">    .show();</span><br></pre></td></tr></table></figure><table><thead><tr><th>Options</th><th>描述</th></tr></thead><tbody><tr><td>BEGIN_INSTANTTIME_OPT_KEY</td><td>过滤数据起始时间戳，不包含</td></tr><tr><td>END_INSTANTTIME_OPT_KEY</td><td>过滤数据终止时间戳</td></tr></tbody></table><p>更多Options请参考<a href="https://hudi.apache.org/configurations.html" target="_blank" rel="noopener">Configurations</a></p><h1 id="Query-With-Hive-Cli"><a href="#Query-With-Hive-Cli" class="headerlink" title="Query With Hive Cli"></a>Query With Hive Cli</h1><p>通过hive cli查询需要添加两个依赖</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add jar file:<span class="comment">///basePath/hoodie-hive-bundle-0.4.7.jar;</span></span><br><span class="line">add jar file:<span class="comment">///basePath/hoodie-hadoop-mr-bundle-0.4.7.jar;</span></span><br></pre></td></tr></table></figure><h1 id="Query-With-Presto"><a href="#Query-With-Presto" class="headerlink" title="Query With Presto"></a>Query With Presto</h1><p>需要添加一个依赖到presto hive插件中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp basePath/packaging/hoodie-presto-bundle/target/hoodie-presto-bundle-<span class="number">0.4</span>.7.jar &lt;presto_install&gt;/plugin/hive-hadoop2/</span><br><span class="line"></span><br><span class="line"><span class="comment">// then restart presto cluster</span></span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;Hudi是类似Carbondata的&lt;code&gt;软数据层&lt;/code&gt;，可以支持streaming的写入、读取，同时也支持将metadata同步到hive，提供Spark、Hive、Presto的SQL查询 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://imperio-wxm.github.io/categories/Hadoop/"/>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/Hadoop/BigData/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hudi" scheme="http://imperio-wxm.github.io/tags/Hudi/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch 重建索引 (Modify Mappings)</title>
    <link href="http://imperio-wxm.github.io/2019/08/07/Elasticsearch-Rebuild-Index/"/>
    <id>http://imperio-wxm.github.io/2019/08/07/Elasticsearch-Rebuild-Index/</id>
    <published>2019-08-07T06:17:23.000Z</published>
    <updated>2019-08-08T16:55:03.766Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>Es重建索引是个比较麻烦的事情，过程繁琐且迁移数据的过程也很漫长，一下将介绍如何rebuild index </p></div><a id="more"></a><table><thead><tr><th>software</th><th>version</th></tr></thead><tbody><tr><td>elasticsearch</td><td>6.4.2</td></tr></tbody></table><p>由于es 的mapping结构无法直接更改，所以如果需要修改mapping结构，则必须rebuild一个索引，利用alias机制将新建索引与老索引关联，然后做数据迁移</p><h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><ol><li>Es别名机制不再赘述，规定索引名为indexName-日期，如<code>test_index-190807</code></li><li>两个alias分别为<code>indexName</code>、<code>indexName$</code>，其中indexName是每个索引都有的alias，用于提供read操作；indexName$用于提供write操作（即做到只write当前alias索引，其他alias索引均可提供read）</li></ol><blockquote><p>Alias规则如何确定，可根据实际业务需要确定</p></blockquote><h1 id="思路描述"><a href="#思路描述" class="headerlink" title="思路描述"></a>思路描述</h1><blockquote><p>例如indexName=<code>test_index-old</code></p></blockquote><p><strong>1.按新schema结构建立索引<code>test_index-new</code>（新索引结构必须与旧的兼容，否则数据迁移会出问题）</strong></p><p><strong>2.暂停<code>test_index-old</code>数据写入</strong></p><p><strong>3.移除<code>test_index-old</code>的test_index$ alias；添加<code>test_index-new</code>的test_index$ alias，然后恢复数据持续写入（目的是让当前写服务能迁移到新的索引，且alias的操作非常快/秒级，此过程几乎可以看做服务不中断）</strong></p><p style="color:red">此方法在数据迁移时会查出double的重复数据，想要线上业务完全不受影响，可以将当前 write/read 全部转到一个新的index，alias指向新索引；此时将旧索引当做静态数据，数据迁移完成后瞬间切换alias（即创建两个新索引，一个用于持续读写，另一个用于迁移数据）</p><p><strong>4.<code>test_index-old</code> 数据迁移至 <code>test_index-new</code>（此过程缓慢）</strong></p><p><strong>5.移除<code>test_index-old</code>的test_index alias，添加<code>test_index-new</code>的test_index alias</strong></p><p><strong>6.至此<code>test_index-old</code> 的数据已经 迁移到 <code>test_index-new</code>，完成index的rebuild，可删除<code>test_index-old</code></strong></p><h1 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h1><h2 id="新建索引"><a href="#新建索引" class="headerlink" title="新建索引"></a>新建索引</h2><blockquote><p>通常用curl或者kibana devtools就可以实现，但是如果mappings、settings比较复杂建议用程序先获取old index info，再此基础上做改动，然后再新建；此步骤必须保证新索引结构可以完全兼容旧索引</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通过java rest client</span></span><br><span class="line"></span><br><span class="line">LowLevelClient().performRequest(method, endpoint, requsetBody);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 使用GET方法获取index info</span></span><br><span class="line"><span class="comment">// 2. 解析response 获取settings、mappings，并修改</span></span><br><span class="line"><span class="comment">// 3. 建立新索引</span></span><br><span class="line"></span><br><span class="line">CreateIndexResponse resp = HighLevelClient().indices().create(<span class="keyword">new</span> CreateIndexRequest(index).mapping(<span class="string">"data"</span>, mappings).settings(settings));</span><br></pre></td></tr></table></figure><h2 id="迁移index-Write-Alias"><a href="#迁移index-Write-Alias" class="headerlink" title="迁移index$ Write Alias"></a>迁移index$ Write Alias</h2><blockquote><p>此操作前必须停止索引写入</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 移除Alias</span></span><br><span class="line">POST /_aliases</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"actions"</span>: [&#123;</span><br><span class="line">        <span class="string">"remove"</span>: &#123;</span><br><span class="line">            <span class="string">"index"</span>: <span class="string">"test_index-old"</span>,</span><br><span class="line">            <span class="string">"alias"</span>: <span class="string">"test_index$"</span></span><br><span class="line">       &#125;</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 添加Alias</span></span><br><span class="line">POST /_aliases</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"actions"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"add"</span>: &#123;</span><br><span class="line">        <span class="string">"index"</span>: <span class="string">"test_index-new"</span>,</span><br><span class="line">        <span class="string">"alias"</span>: <span class="string">"test_index$"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>执行结束后开启写索引</p></blockquote><h2 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex?wait_for_completion=<span class="keyword">false</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"source"</span>: &#123;</span><br><span class="line">    <span class="string">"index"</span>: <span class="string">"test_index-old"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"dest"</span>: &#123;</span><br><span class="line">    <span class="string">"index"</span>: <span class="string">"test_index-new"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>此命令会返回taskId，可根据Id查询运行状态</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET _tasks/&#123;taskId&#125;</span><br></pre></td></tr></table></figure><blockquote><p>中途遇到什么问题可以终止task</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PUT _tasks/&#123;taskID&#125;/cancel</span><br></pre></td></tr></table></figure><h2 id="迁移index-Read-Alias"><a href="#迁移index-Read-Alias" class="headerlink" title="迁移index Read Alias"></a>迁移index Read Alias</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 添加Alias</span></span><br><span class="line">POST /_aliases</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"actions"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"add"</span>: &#123;</span><br><span class="line">        <span class="string">"index"</span>: <span class="string">"test_index-new"</span>,</span><br><span class="line">        <span class="string">"alias"</span>: <span class="string">"test_index"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 移除Alias</span></span><br><span class="line">POST /_aliases</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"actions"</span>: [&#123;</span><br><span class="line">        <span class="string">"remove"</span>: &#123;</span><br><span class="line">            <span class="string">"index"</span>: <span class="string">"test_index-old"</span>,</span><br><span class="line">            <span class="string">"alias"</span>: <span class="string">"test_index"</span></span><br><span class="line">       &#125;</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>此时<code>test_index-old</code> 已经被重建为 <code>test_index-new</code></p></blockquote><h2 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h2><blockquote><p>Elasticsearch 的数据迁移过程十分缓慢，通过以下方法可以进行调优（实测：3个Es nodes，30G+数据，60m+ Docs，Task需要30—40min左右结束）</p></blockquote><p><strong>1.source中调整batch_size(defailt: 1000)</strong></p><blockquote><p>批量大小取决于数据、分析和集群配置，但一个好的起点是每批处理5-15MB</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"source"</span>: &#123;</span><br><span class="line">    <span class="string">"index"</span>: <span class="string">"sourceIndex"</span>,</span><br><span class="line">    <span class="string">"size"</span>: <span class="number">5000</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>2.slicing 配置</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex?slices=<span class="number">5</span>&amp;refresh</span><br></pre></td></tr></table></figure><blockquote><p>【slices大小设置注意事项】</p></blockquote><ul><li>slices大小的设置可以手动指定，或者设置slices设置为auto，auto的含义是：针对单索引，slices大小=分片数；针对多索引，slices=分片的最小值</li><li>当slices的数量等于索引中的分片数量时，查询性能最高效。slices大小大于分片数，非但不会提升效率，反而会增加开销</li><li>如果这个slices数字很大(例如500)，建议选择一个较低的数字，因为过大的slices 会影响性能</li></ul><p><strong>3.关闭刷新和副本</strong></p><blockquote><p>在数据迁移过程中关闭refresh_interval、number_of_replicas可以大幅提升性能，迁移完成后恢复默认值</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PUT indexName/_settings</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"refresh_interval"</span>: <span class="string">"-1"</span>,</span><br><span class="line">  <span class="string">"number_of_replicas"</span>: <span class="string">"0"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;Es重建索引是个比较麻烦的事情，过程繁琐且迁移数据的过程也很漫长，一下将介绍如何rebuild index &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://imperio-wxm.github.io/categories/Elasticsearch/"/>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/Elasticsearch/BigData/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Elasticsearch" scheme="http://imperio-wxm.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>ZooKeeper动态扩容、缩容</title>
    <link href="http://imperio-wxm.github.io/2019/07/18/ZooKeeper-ExpansionNode-ReducedNode/"/>
    <id>http://imperio-wxm.github.io/2019/07/18/ZooKeeper-ExpansionNode-ReducedNode/</id>
    <published>2019-07-18T06:19:26.000Z</published>
    <updated>2019-08-15T14:12:35.728Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>Zookeeper是大数据领域中常用的分布式协调工具，如何不中断服务动态的扩充、缩减、迁移node？ </p></div><a id="more"></a><h1 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h1><p>先有一个3台机器的zk集群，需要先对这个集群进行扩容增加节点，随后进行缩容；最终达到节点迁移的目的</p><p><code>ps：zookeeper相关概念、配置、环境搭建不在赘述</code></p><table><thead><tr><th>mid</th><th>node</th></tr></thead><tbody><tr><td>1</td><td>node1</td></tr><tr><td>2</td><td>node2</td></tr><tr><td>3</td><td>node3</td></tr><tr><td>4</td><td>node4</td></tr><tr><td>5</td><td>node5</td></tr><tr><td>6</td><td>node6</td></tr></tbody></table><p>ZooKeeper监控工具推荐用: <a href="https://github.com/DeemOpen/zkui" target="_blank" rel="noopener">zkui</a></p><p>ZooKeeper监控命令推荐(必须提前安装nc):</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查看zk状态</span></span><br><span class="line">echo stat | nc <span class="number">127.0</span>.0.1 <span class="number">2181</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>Command</th><th>Desc</th></tr></thead><tbody><tr><td>conf</td><td>输出相关服务配置的详细信息</td></tr><tr><td>cons</td><td>列出所有连接到服务器的客户端的完全的连接/会话的详细信息,包括 接受 or 发送的包数量、会话 id 、操作延迟、最后的操作执行等等信息</td></tr><tr><td>dump</td><td>列出未经处理的会话和临时节点</td></tr><tr><td>envi</td><td>输出关于服务环境的详细信息（区别于 conf 命令）</td></tr><tr><td>reqs</td><td>列出未经处理的请求</td></tr><tr><td>ruok</td><td>测试服务是否处于正确状态;如果确实如此,那么服务返回 imok ,否则不做任何相应</td></tr><tr><td>stat</td><td>输出关于性能和连接的客户端的列表</td></tr><tr><td>wchs</td><td>列出服务器 watch 的详细信息</td></tr><tr><td>wchc</td><td>通过 session 列出服务器 watch 的详细信息,它的输出是一个与watch 相关的会话的列表</td></tr><tr><td>wchp</td><td>通过路径列出服务器 watch 的详细信息;它输出一个与 session相关的路径</td></tr></tbody></table><h2 id="扩容-amp-缩容"><a href="#扩容-amp-缩容" class="headerlink" title="扩容 &amp; 缩容"></a>扩容 &amp; 缩容</h2><ul><li>思路</li></ul><blockquote><p>原本有node1、2、3 的zk集群，现在要将node4、5、6的节点加入，扩充3个节点</p><p>zk节点数应为3、5、7这样的奇数个，否则无法选举出leader，因此想要添加node4、5、6节点，则：</p></blockquote><ol><li>添加node4、5，使得总节点数从3个扩充到5</li><li>(n-1)/2，5节点zk允许挂掉2个节点，则先下线node1、再上线node6</li><li>此时在线节点有node2、3、4、5、6</li><li>最后再下线node2、3节点</li><li>此时zk节点从node1、2、3切换到了node4、5、6</li></ol><h2 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h2><h3 id="查看node1、2、3状态"><a href="#查看node1、2、3状态" class="headerlink" title="查看node1、2、3状态"></a>查看node1、2、3状态</h3><p>node1(follower)：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Received: <span class="number">247</span></span><br><span class="line">Sent: <span class="number">246</span></span><br><span class="line">Connections: <span class="number">2</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x5500000012</span></span><br><span class="line">Mode: follower</span><br><span class="line">Node count: <span class="number">37658</span></span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span>.8--<span class="number">1</span>, built on <span class="number">02</span>/<span class="number">06</span>/<span class="number">2016</span> <span class="number">03</span>:<span class="number">18</span> GMT</span><br></pre></td></tr></table></figure></p><p>node2(leader)<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Received: <span class="number">19</span></span><br><span class="line">Sent: <span class="number">18</span></span><br><span class="line">Connections: <span class="number">1</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x5500000012</span></span><br><span class="line">Mode: leader</span><br><span class="line">Node count: <span class="number">37658</span></span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span>.8--<span class="number">1</span>, built on <span class="number">02</span>/<span class="number">06</span>/<span class="number">2016</span> <span class="number">03</span>:<span class="number">18</span> GMT</span><br></pre></td></tr></table></figure></p><p>node3(follower)<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Received: <span class="number">1</span></span><br><span class="line">Sent: <span class="number">0</span></span><br><span class="line">Connections: <span class="number">1</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x5500000012</span></span><br><span class="line">Mode: follower</span><br><span class="line">Node count: <span class="number">37658</span></span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span>.8--<span class="number">1</span>, built on <span class="number">02</span>/<span class="number">06</span>/<span class="number">2016</span> <span class="number">03</span>:<span class="number">18</span> GMT</span><br></pre></td></tr></table></figure></p><h3 id="增加node4、5节点"><a href="#增加node4、5节点" class="headerlink" title="增加node4、5节点"></a>增加node4、5节点</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改zoo.cfg</span></span><br><span class="line"></span><br><span class="line">server.1=node1:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server.2=node2:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server.3=node3:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server.4=node4:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server.5=node5:<span class="number">2888</span>:<span class="number">3888</span></span><br></pre></td></tr></table></figure><p>node4(follower)<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Received: <span class="number">1</span></span><br><span class="line">Sent: <span class="number">0</span></span><br><span class="line">Connections: <span class="number">1</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x5500000015</span></span><br><span class="line">Mode: follower</span><br><span class="line">Node count: <span class="number">37658</span></span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span>.8--<span class="number">1</span>, built on <span class="number">02</span>/<span class="number">06</span>/<span class="number">2016</span> <span class="number">03</span>:<span class="number">18</span> GMT</span><br></pre></td></tr></table></figure></p><p>node5(follower)<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Received: <span class="number">1</span></span><br><span class="line">Sent: <span class="number">0</span></span><br><span class="line">Connections: <span class="number">1</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x5500000016</span></span><br><span class="line">Mode: follower</span><br><span class="line">Node count: <span class="number">37658</span></span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span>.8--<span class="number">1</span>, built on <span class="number">02</span>/<span class="number">06</span>/<span class="number">2016</span> <span class="number">03</span>:<span class="number">18</span> GMT</span><br></pre></td></tr></table></figure></p><h3 id="修改node1、2、3配置，重启"><a href="#修改node1、2、3配置，重启" class="headerlink" title="修改node1、2、3配置，重启"></a>修改node1、2、3配置，重启</h3><p><code>ps: 最后在重启leader节点，会触发选举，默认会选择myid最大的为新leader，即node5</code></p><blockquote><p>轮序重启后，原来node2：<code>Mode: leader</code>——&gt;<code>Mode: follower</code>；最后添加的node5：<code>Mode: follower</code>——&gt;<code>Mode: leader</code></p></blockquote><p>至此新增2个节点完成，现在集群一共5个节点</p><h3 id="下线node1、上线node6"><a href="#下线node1、上线node6" class="headerlink" title="下线node1、上线node6"></a>下线node1、上线node6</h3><ul><li>下线node1</li></ul><p>关闭node1，修改node2、3、4，去除node1配置后重启</p><p>最后修改node5，新增node6配置重启，此时node5任然是leader</p><p>至此node1下线，5个节点的zk允许挂掉2个节点</p><ul><li>上线node6</li></ul><p>修改node2、3、4节点，添加node6配置后重启</p><p>上线node6节点，配置与node2、3、4相同</p><p>node6(follower)<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Received: <span class="number">1</span></span><br><span class="line">Sent: <span class="number">0</span></span><br><span class="line">Connections: <span class="number">1</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x5900000002</span></span><br><span class="line">Mode: follower</span><br><span class="line">Node count: <span class="number">37658</span></span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span>.8--<span class="number">1</span>, built on <span class="number">02</span>/<span class="number">06</span>/<span class="number">2016</span> <span class="number">03</span>:<span class="number">18</span> GMT</span><br></pre></td></tr></table></figure></p><p>重启node5，此时会触发选举，选择node6为新leader</p><p>至此5个节点配置完全相同，node6加入集群且为新leader</p><h3 id="下线node2、3"><a href="#下线node2、3" class="headerlink" title="下线node2、3"></a>下线node2、3</h3><p>关闭node2、3，修改node4、5，去除node2、3配置后重启</p><p>修改node6，去除node2、3配置重启，此时会选取myid最大的node5为leader</p><p style="color:red">至此node1、2、3全部下线完成，zookeeper已经从node1、2、3节点迁移到了node4、5、6节点</p><h2 id="其他方案"><a href="#其他方案" class="headerlink" title="其他方案"></a>其他方案</h2><ol><li>修改node4、5、6配置，添加6个节点全部配置，依次启动；<code>此时node4、5、6是无法加入节点的</code></li><li>修改node1、3配置，添加node4、5、6信息，重启；<code>此时会触发选举，根据myid最大的node6为leader；之前的leader node2被踢出集群</code></li><li>下线node1、2，修改node4、5，去除node1、2配置后重启，<code>此时leader依然是node6</code></li><li>修改node6，去除node1、2、3配置重启，<code>此时会触发选举，根据myid选择node5为leader</code></li><li>下线node3，修改node4、5，去除node3配置重启，<code>触发选举，node6为leader</code></li></ol><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;Zookeeper是大数据领域中常用的分布式协调工具，如何不中断服务动态的扩充、缩减、迁移node？ &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/BigData/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="ZooKeeper" scheme="http://imperio-wxm.github.io/tags/ZooKeeper/"/>
    
  </entry>
  
  <entry>
    <title>Tomcat-JDTCompiler-Error</title>
    <link href="http://imperio-wxm.github.io/2019/06/15/Tomcat-JDTCompiler-Error/"/>
    <id>http://imperio-wxm.github.io/2019/06/15/Tomcat-JDTCompiler-Error/</id>
    <published>2019-06-15T02:49:46.000Z</published>
    <updated>2019-07-19T17:08:48.474Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>不同版本的JDK编译出的JSP与Tomcat的兼容性也不相同 </p></div><a id="more"></a><table><thead><tr><th>Software</th><th>Version</th></tr></thead><tbody><tr><td>Jdk</td><td>1.8.0_121</td></tr><tr><td>Tomcat</td><td>7.0.27</td></tr></tbody></table><blockquote><p>最近在维护一个老项目的时候，用Jenkins jdk1.8 编译打包war后，解压放在1.7的Tomcat下</p></blockquote><h2 id="运行Jsp编译报错如下："><a href="#运行Jsp编译报错如下：" class="headerlink" title="运行Jsp编译报错如下："></a>运行Jsp编译报错如下：</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">org.eclipse.jdt.internal.compiler.classfmt.ClassFormatException</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.classfmt.ClassFileReader.&lt;init&gt;(ClassFileReader.java:<span class="number">372</span>)</span><br><span class="line">        at org.apache.jasper.compiler.JDTCompiler$<span class="number">1</span>.findType(JDTCompiler.java:<span class="number">232</span>)</span><br><span class="line">        at org.apache.jasper.compiler.JDTCompiler$<span class="number">1</span>.findType(JDTCompiler.java:<span class="number">188</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.lookup.LookupEnvironment.askForType(LookupEnvironment.java:<span class="number">113</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.lookup.UnresolvedReferenceBinding.resolve(UnresolvedReferenceBinding.java:<span class="number">49</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.lookup.BinaryTypeBinding.resolveType(BinaryTypeBinding.java:<span class="number">122</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.lookup.PackageBinding.getTypeOrPackage(PackageBinding.java:<span class="number">168</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.lookup.Scope.getType(Scope.java:<span class="number">2472</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.ast.TypeDeclaration.resolve(TypeDeclaration.java:<span class="number">1006</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.ast.TypeDeclaration.resolve(TypeDeclaration.java:<span class="number">1258</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.ast.CompilationUnitDeclaration.resolve(CompilationUnitDeclaration.java:<span class="number">539</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.Compiler.process(Compiler.java:<span class="number">763</span>)</span><br><span class="line">        at org.eclipse.jdt.internal.compiler.Compiler.compile(Compiler.java:<span class="number">468</span>)</span><br><span class="line">        at org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:<span class="number">459</span>)</span><br><span class="line">        at org.apache.jasper.compiler.Compiler.compile(Compiler.java:<span class="number">378</span>)</span><br><span class="line">        at org.apache.jasper.compiler.Compiler.compile(Compiler.java:<span class="number">353</span>)</span><br><span class="line">        at org.apache.jasper.compiler.Compiler.compile(Compiler.java:<span class="number">340</span>)</span><br><span class="line">        at org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:<span class="number">646</span>)</span><br><span class="line">        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:<span class="number">357</span>)</span><br><span class="line">        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:<span class="number">390</span>)</span><br><span class="line">        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:<span class="number">334</span>)</span><br><span class="line">        at javax.servlet.http.HttpServlet.service(HttpServlet.java:<span class="number">722</span>)</span><br><span class="line">        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:<span class="number">305</span>)</span><br><span class="line">        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:<span class="number">210</span>)</span><br><span class="line">        at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java:<span class="number">684</span>)</span><br><span class="line">        at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:<span class="number">471</span>)</span><br><span class="line">        at org.apache.catalina.core.ApplicationDispatcher.doForward(ApplicationDispatcher.java:<span class="number">402</span>)</span><br><span class="line">        at org.apache.catalina.core.ApplicationDispatcher.forward(ApplicationDispatcher.java:<span class="number">329</span>)</span><br><span class="line">        at org.apache.struts2.dispatcher.ServletDispatcherResult.doExecute(ServletDispatcherResult.java:<span class="number">154</span>)</span><br><span class="line">        at org.apache.struts2.dispatcher.StrutsResultSupport.execute(StrutsResultSupport.java:<span class="number">186</span>)</span><br><span class="line">        at com.opensymphony.xwork2.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:<span class="number">361</span>)</span><br><span class="line">        at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:<span class="number">265</span>)</span><br><span class="line">        at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:<span class="number">163</span>)</span><br><span class="line">        at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:<span class="number">87</span>)</span><br></pre></td></tr></table></figure><blockquote><p>错误原因是Jdk1.8 编译后，与Tomcat1.7 不兼容</p></blockquote><h2 id="处理："><a href="#处理：" class="headerlink" title="处理："></a>处理：</h2><p><a href="https://mvnrepository.com/artifact/org.eclipse.jdt.core.compiler/ecj" target="_blank" rel="noopener">Download Eclipse ECJ Jar</a></p><ul><li>1.maven 仓库下载最新的 ecj-4.6.1.jar</li><li>2.Tomcat/lib 目录下，移除 ecj-3.7.2.jar</li><li>3.上传 ecj-4.6.1.jar 至Tomcat/lib 重启Tomcat</li></ul><p>问题得以解决！</p><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;不同版本的JDK编译出的JSP与Tomcat的兼容性也不相同 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Java" scheme="http://imperio-wxm.github.io/categories/Java/"/>
    
    
      <category term="Git" scheme="http://imperio-wxm.github.io/tags/Git/"/>
    
      <category term="Tomcat" scheme="http://imperio-wxm.github.io/tags/Tomcat/"/>
    
  </entry>
  
  <entry>
    <title>Gitbook 制作 PDF</title>
    <link href="http://imperio-wxm.github.io/2019/05/27/GitBook-To-Pdf/"/>
    <id>http://imperio-wxm.github.io/2019/05/27/GitBook-To-Pdf/</id>
    <published>2019-05-27T08:12:23.000Z</published>
    <updated>2019-06-01T13:17:12.064Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>通常需要将Gitbook的内容转成PDF格式，方便离线保存 </p></div><a id="more"></a><h1 id="一、安装Caliber应用程序"><a href="#一、安装Caliber应用程序" class="headerlink" title="一、安装Caliber应用程序"></a>一、安装Caliber应用程序</h1><p><a href="https://calibre-ebook.com/download" target="_blank" rel="noopener">官网下载</a></p><ul><li>Windows 环境变量添加：</li></ul><p>添加到系统path末尾:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">;C:\<span class="function">Program <span class="title">Files</span> <span class="params">(x86)</span>\Calibre2</span></span><br></pre></td></tr></table></figure></p><ul><li>验证是否安装成功</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cmd</span></span><br><span class="line">ebook-convert --version</span><br><span class="line"></span><br><span class="line">ebook-convert.exe (calibre <span class="number">3.43</span>.0)</span><br><span class="line">Created by: Kovid Goyal &lt;kovid<span class="meta">@kovidgoyal</span>.net&gt;</span><br></pre></td></tr></table></figure><h1 id="二、GitBook制作PDF"><a href="#二、GitBook制作PDF" class="headerlink" title="二、GitBook制作PDF"></a>二、GitBook制作PDF</h1><ul><li>npm 安装 ebook-convert</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install ebook-convert -g</span><br></pre></td></tr></table></figure><ul><li>Gitbook 根目录下运行：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gitbook pdf</span><br></pre></td></tr></table></figure><p>在根目录下会生成book.pdf</p><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;通常需要将Gitbook的内容转成PDF格式，方便离线保存 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/tags/Linux/"/>
    
      <category term="Git" scheme="http://imperio-wxm.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Cygwin Windows 换行符转换</title>
    <link href="http://imperio-wxm.github.io/2019/04/25/Cygwin-DosFiles-Format/"/>
    <id>http://imperio-wxm.github.io/2019/04/25/Cygwin-DosFiles-Format/</id>
    <published>2019-04-25T08:12:23.000Z</published>
    <updated>2019-06-01T11:45:31.696Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>在Windows下编辑的Shell脚本，使用Cygwin仿Linux环境运行，出现 $’\r’: 未找到命令错误 </p></div><a id="more"></a><p>因为在Dos or Window下回车键实际上输入的是 <code>回车（CR)</code> 和 <code>换行（LF）</code>，而Linux or Unix下回车键只输入 <code>换行（LF）</code>，所以文件在每行都会多了一个 <code>CR</code>，Linux下运行时就会报错找不到命令，需要把Dos文件格式转换为Unix格式</p><h2 id="1-安装dos2unix组件"><a href="#1-安装dos2unix组件" class="headerlink" title="1. 安装dos2unix组件"></a>1. 安装dos2unix组件</h2><p><a href="https://www.cygwin.com/" target="_blank" rel="noopener">Cygwin 官网</a></p><p>下载Cygwin，双击setup-x86_64.exe，选择从本地或者internet安装，选择dos2unix组件进行安装</p><p><img src="https://github.com/imperio-wxm/source/blob/master/source/posts-images/Cygwin-DosFiles-Format/cygwin-dos2unix.png?raw=true" alt="安装插件"></p><ul><li>测试脚本：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// windows下记事本编辑</span></span><br><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">d=`date +%Y%m%d`</span><br><span class="line"></span><br><span class="line">echo $d</span><br></pre></td></tr></table></figure><ul><li>Cygwin直接运行：</li></ul><p><img src="https://github.com/imperio-wxm/source/blob/master/source/posts-images/Cygwin-DosFiles-Format/cygwin-run-normal.png?raw=true" alt="转换前运行"></p><h2 id="2-单文件转换"><a href="#2-单文件转换" class="headerlink" title="2. 单文件转换"></a>2. 单文件转换</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dos2unix test.sh</span><br></pre></td></tr></table></figure><p><img src="https://github.com/imperio-wxm/source/blob/master/source/posts-images/Cygwin-DosFiles-Format/dos2unix-transform.png?raw=true" alt="转换"></p><ul><li>Cygwin转换后运行：</li></ul><p><img src="https://github.com/imperio-wxm/source/blob/master/source/posts-images/Cygwin-DosFiles-Format/cygwin-run-transform.png?raw=true" alt="转换后运行"></p><h2 id="3-目录批量转换"><a href="#3-目录批量转换" class="headerlink" title="3. 目录批量转换"></a>3. 目录批量转换</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find . -type f -exec dos2unix &#123;&#125; \;</span><br></pre></td></tr></table></figure><p>将转换后的文件直接upload到linux服务器上，也能正常运行</p><hr><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;在Windows下编辑的Shell脚本，使用Cygwin仿Linux环境运行，出现 $’\r’: 未找到命令错误 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/tags/Linux/"/>
    
      <category term="Cygwin" scheme="http://imperio-wxm.github.io/tags/Cygwin/"/>
    
  </entry>
  
  <entry>
    <title>SQL中的CUBE、ROLLUP、GROUPING用法</title>
    <link href="http://imperio-wxm.github.io/2019/03/12/SQL-Cube-Rollup-GroupingSets/"/>
    <id>http://imperio-wxm.github.io/2019/03/12/SQL-Cube-Rollup-GroupingSets/</id>
    <published>2019-03-12T12:07:43.000Z</published>
    <updated>2019-06-01T13:18:44.777Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>在SQL语法中，经常用到GROUP BY来做多维度的聚合。但是遇到多个维度并列聚合的方式，通常是将每个维度用GROUP BY统计后，再使用UNION语法将结果集汇总，但是这样的SQL执行计划会在数据INPUT端从存储引擎获取多次，导致重复获取数据，浪费机器资源。CUBE、ROLLUP、GROUPING的语法可以更高效的做到多维度的聚合 </p></div><a id="more"></a><table><thead><tr><th>SoftWare</th><th>Version</th></tr></thead><tbody><tr><td>Presto</td><td>0.149</td></tr><tr><td>Hive</td><td>1.1.0-cdh5.11.1</td></tr><tr><td>Java</td><td>1.8.0_121</td></tr></tbody></table><p><code>此文会以Presto On Hive 的方式演示实例，Mysql不同版本支持不同，Oracle、Sql Server的语法方式略有差异</code></p><h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><h2 id="建测试表"><a href="#建测试表" class="headerlink" title="建测试表"></a>建测试表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`wxm_test_fun`</span>(</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span> <span class="keyword">COMMENT</span> <span class="string">'null'</span>, </span><br><span class="line">  <span class="string">`name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'null'</span>, </span><br><span class="line">  <span class="string">`area_id`</span> <span class="built_in">int</span> <span class="keyword">COMMENT</span> <span class="string">'null'</span>, </span><br><span class="line">  <span class="string">`group_id`</span> <span class="built_in">int</span> <span class="keyword">COMMENT</span> <span class="string">'null'</span>)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'wxm_test_fun'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE </span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'</span> </span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES ( </span><br><span class="line">  <span class="string">'field.delim'</span>=<span class="string">'\t'</span>, </span><br><span class="line">  <span class="string">'serialization.format'</span>=<span class="string">'\t'</span>) </span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> INPUTFORMAT </span><br><span class="line">  <span class="string">'org.apache.hadoop.mapred.TextInputFormat'</span> </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span>;</span><br></pre></td></tr></table></figure><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1wxm1000111</span><br><span class="line">1wxm2001112</span><br><span class="line">2wxm1000111</span><br><span class="line">2wxm2001112</span><br><span class="line">3wxm3000111</span><br><span class="line">3wxm3001112</span><br><span class="line">4wxm4000111</span><br><span class="line">1wxm4000112</span><br><span class="line">4wxm4002111</span><br><span class="line">5wxm1003113</span><br></pre></td></tr></table></figure><h2 id="说明聚合维度"><a href="#说明聚合维度" class="headerlink" title="说明聚合维度"></a>说明聚合维度</h2><blockquote><p>四个维度：id（0），name（1），area_id（2），group_id（3）</p></blockquote><hr><h1 id="CUBE"><a href="#CUBE" class="headerlink" title="CUBE"></a>CUBE</h1><blockquote><p>cube会对所有聚合可能进行计算：CUBE（A,B,C），会计算group by A union group by B group by C union group by （AB） union group by （AC） union group by （BC） union group by （BC） union all （ABC）</p><p>分组的次数=2ⁿ-1；n为待分组的字段个数</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">presto:temp&gt; select id,name,area_id,count(*) count from wxm_test_fun group by cube(id,name,area_id) order by id,name,area_id;</span><br><span class="line"></span><br><span class="line">  id  | name | area_id | count </span><br><span class="line"><span class="comment">------+------+---------+-------</span></span><br><span class="line">    1 | wxm1 |       0 |     1 </span><br><span class="line">    1 | wxm1 | NULL    |     1 </span><br><span class="line">    1 | wxm2 |       1 |     1 </span><br><span class="line">    1 | wxm2 | NULL    |     1 </span><br><span class="line">    1 | wxm4 |       0 |     1 </span><br><span class="line">    1 | wxm4 | NULL    |     1 </span><br><span class="line">    1 | NULL |       0 |     2 </span><br><span class="line">    1 | NULL |       1 |     1 </span><br><span class="line">    1 | NULL | NULL    |     3 </span><br><span class="line">    2 | wxm1 |       0 |     1 </span><br><span class="line">    2 | wxm1 | NULL    |     1 </span><br><span class="line">    2 | wxm2 |       1 |     1 </span><br><span class="line">    2 | wxm2 | NULL    |     1 </span><br><span class="line">    2 | NULL |       0 |     1 </span><br><span class="line">    2 | NULL |       1 |     1 </span><br><span class="line">    2 | NULL | NULL    |     2 </span><br><span class="line">    3 | wxm3 |       0 |     1 </span><br><span class="line">    3 | wxm3 |       1 |     1 </span><br><span class="line">    3 | wxm3 | NULL    |     2 </span><br><span class="line">    3 | NULL |       0 |     1 </span><br><span class="line">    3 | NULL |       1 |     1 </span><br><span class="line">    3 | NULL | NULL    |     2 </span><br><span class="line">    4 | wxm4 |       0 |     1 </span><br><span class="line">    4 | wxm4 |       2 |     1 </span><br><span class="line">    4 | wxm4 | NULL    |     2 </span><br><span class="line">    4 | NULL |       0 |     1 </span><br><span class="line">    4 | NULL |       2 |     1 </span><br><span class="line">    4 | NULL | NULL    |     2 </span><br><span class="line">    5 | wxm1 |       3 |     1 </span><br><span class="line">    5 | wxm1 | NULL    |     1 </span><br><span class="line">    5 | NULL |       3 |     1 </span><br><span class="line">    5 | NULL | NULL    |     1 </span><br><span class="line"> NULL | wxm1 |       0 |     2 </span><br><span class="line"> NULL | wxm1 |       3 |     1 </span><br><span class="line"> NULL | wxm1 | NULL    |     3 </span><br><span class="line"> NULL | wxm2 |       1 |     2 </span><br><span class="line"> NULL | wxm2 | NULL    |     2 </span><br><span class="line"> NULL | wxm3 |       0 |     1 </span><br><span class="line"> NULL | wxm3 |       1 |     1 </span><br><span class="line"> NULL | wxm3 | NULL    |     2 </span><br><span class="line"> NULL | wxm4 |       0 |     2 </span><br><span class="line"> NULL | wxm4 |       2 |     1 </span><br><span class="line"> NULL | wxm4 | NULL    |     3 </span><br><span class="line"> NULL | NULL |       0 |     5 </span><br><span class="line"> NULL | NULL |       1 |     3 </span><br><span class="line"> NULL | NULL |       2 |     1 </span><br><span class="line"> NULL | NULL |       3 |     1 </span><br><span class="line"> NULL | NULL | NULL    |    10 </span><br><span class="line">(48 rows)</span><br><span class="line"></span><br><span class="line">Query 20190312_082310_01544_wwvpi, FINISHED, 2 nodes</span><br><span class="line">Splits: 4 total, 4 done (100.00%)</span><br><span class="line">0:00 [10 rows, 158B] [23 rows/s, 374B/s]</span><br></pre></td></tr></table></figure><blockquote><p>2的3次方减1 = 8个维度的整合</p></blockquote><ul><li>等价于8个union</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">presto:temp&gt; select * from (</span><br><span class="line">          -&gt; select id,null name,null area_id,count(*) count from wxm_test_fun group by id</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select null,name,null,count(*) count from wxm_test_fun group by name</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select null,null,area_id,count(*) count from wxm_test_fun group by area_id</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select id,name,area_id,count(*) count from wxm_test_fun group by id,name,area_id</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select id,name,null,count(*) count from wxm_test_fun group by id,name</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select null,name,area_id,count(*) count from wxm_test_fun group by name,area_id</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select id,null,area_id,count(*) count from wxm_test_fun group by id,area_id</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select null,null,null,count(*) count from wxm_test_fun) order by id,name,area_id;</span><br><span class="line">  id  | name | area_id | count </span><br><span class="line"><span class="comment">------+------+---------+-------</span></span><br><span class="line">    1 | wxm1 |       0 |     1 </span><br><span class="line">    1 | wxm1 | NULL    |     1 </span><br><span class="line">    1 | wxm2 |       1 |     1 </span><br><span class="line">    1 | wxm2 | NULL    |     1 </span><br><span class="line">    1 | wxm4 |       0 |     1 </span><br><span class="line">    1 | wxm4 | NULL    |     1 </span><br><span class="line">    1 | NULL |       0 |     2 </span><br><span class="line">    1 | NULL |       1 |     1 </span><br><span class="line">    1 | NULL | NULL    |     3 </span><br><span class="line">    2 | wxm1 |       0 |     1 </span><br><span class="line">    2 | wxm1 | NULL    |     1 </span><br><span class="line">    2 | wxm2 |       1 |     1 </span><br><span class="line">    2 | wxm2 | NULL    |     1 </span><br><span class="line">    2 | NULL |       0 |     1 </span><br><span class="line">    2 | NULL |       1 |     1 </span><br><span class="line">    2 | NULL | NULL    |     2 </span><br><span class="line">    3 | wxm3 |       0 |     1 </span><br><span class="line">    3 | wxm3 |       1 |     1 </span><br><span class="line">    3 | wxm3 | NULL    |     2 </span><br><span class="line">    3 | NULL |       0 |     1 </span><br><span class="line">    3 | NULL |       1 |     1 </span><br><span class="line">    3 | NULL | NULL    |     2 </span><br><span class="line">    4 | wxm4 |       0 |     1 </span><br><span class="line">    4 | wxm4 |       2 |     1 </span><br><span class="line">    4 | wxm4 | NULL    |     2 </span><br><span class="line">    4 | NULL |       0 |     1 </span><br><span class="line">    4 | NULL |       2 |     1 </span><br><span class="line">    4 | NULL | NULL    |     2 </span><br><span class="line">    5 | wxm1 |       3 |     1 </span><br><span class="line">    5 | wxm1 | NULL    |     1 </span><br><span class="line">    5 | NULL |       3 |     1 </span><br><span class="line">    5 | NULL | NULL    |     1 </span><br><span class="line"> NULL | wxm1 |       0 |     2 </span><br><span class="line"> NULL | wxm1 |       3 |     1 </span><br><span class="line"> NULL | wxm1 | NULL    |     3 </span><br><span class="line"> NULL | wxm2 |       1 |     2 </span><br><span class="line"> NULL | wxm2 | NULL    |     2 </span><br><span class="line"> NULL | wxm3 |       0 |     1 </span><br><span class="line"> NULL | wxm3 |       1 |     1 </span><br><span class="line"> NULL | wxm3 | NULL    |     2 </span><br><span class="line"> NULL | wxm4 |       0 |     2 </span><br><span class="line"> NULL | wxm4 |       2 |     1 </span><br><span class="line"> NULL | wxm4 | NULL    |     3 </span><br><span class="line"> NULL | NULL |       0 |     5 </span><br><span class="line"> NULL | NULL |       1 |     3 </span><br><span class="line"> NULL | NULL |       2 |     1 </span><br><span class="line"> NULL | NULL |       3 |     1 </span><br><span class="line"> NULL | NULL | NULL    |    10 </span><br><span class="line">(48 rows)</span><br><span class="line"></span><br><span class="line">Query 20190312_082943_01568_wwvpi, FINISHED, 2 nodes</span><br><span class="line">Splits: 42 total, 42 done (100.00%)</span><br><span class="line">0:01 [80 rows, 1.23KB] [101 rows/s, 1.57KB/s]</span><br></pre></td></tr></table></figure><h1 id="ROLLUP"><a href="#ROLLUP" class="headerlink" title="ROLLUP"></a>ROLLUP</h1><blockquote><p>与CUBE不同，ROLLUP仅仅只展开第一层的维度聚合：分组的次数=待分组的字段数+1</p><p>cube会对所有聚合可能进行计算：CUBE（A,B,C），会计算group by y （ABC） union group by （AB） group by C union group by NULL</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">presto:temp&gt;  select name,area_id,group_id,count(*) count from wxm_test_fun group by rollup(name,area_id,group_id) order by name,area_id,group_id,count(*);</span><br><span class="line"></span><br><span class="line"> name | area_id | group_id | count </span><br><span class="line"><span class="comment">------+---------+----------+-------</span></span><br><span class="line"> wxm1 |       0 |      111 |     2 </span><br><span class="line"> wxm1 |       0 | NULL     |     2 </span><br><span class="line"> wxm1 |       3 |      113 |     1 </span><br><span class="line"> wxm1 |       3 | NULL     |     1 </span><br><span class="line"> wxm1 | NULL    | NULL     |     3 </span><br><span class="line"> wxm2 |       1 |      112 |     2 </span><br><span class="line"> wxm2 |       1 | NULL     |     2 </span><br><span class="line"> wxm2 | NULL    | NULL     |     2 </span><br><span class="line"> wxm3 |       0 |      111 |     1 </span><br><span class="line"> wxm3 |       0 | NULL     |     1 </span><br><span class="line"> wxm3 |       1 |      112 |     1 </span><br><span class="line"> wxm3 |       1 | NULL     |     1 </span><br><span class="line"> wxm3 | NULL    | NULL     |     2 </span><br><span class="line"> wxm4 |       0 |      111 |     1 </span><br><span class="line"> wxm4 |       0 |      112 |     1 </span><br><span class="line"> wxm4 |       0 | NULL     |     2 </span><br><span class="line"> wxm4 |       2 |      111 |     1 </span><br><span class="line"> wxm4 |       2 | NULL     |     1 </span><br><span class="line"> wxm4 | NULL    | NULL     |     3 </span><br><span class="line"> NULL | NULL    | NULL     |    10 </span><br><span class="line">(20 rows)</span><br><span class="line"></span><br><span class="line">Query 20190312_085457_01652_wwvpi, FINISHED, 2 nodes</span><br><span class="line">Splits: 4 total, 4 done (100.00%)</span><br><span class="line">0:00 [10 rows, 158B] [29 rows/s, 458B/s]</span><br></pre></td></tr></table></figure><ul><li>等价于3 + 1 = 4个union聚合</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">presto:temp&gt; select * from (</span><br><span class="line">          -&gt; select name,null area_id,null group_id,count(*) count from wxm_test_fun group by name</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select name,area_id,null group_id,count(*) count from wxm_test_fun group by name,area_id</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select name,area_id,group_id,count(*) count from wxm_test_fun group by name,area_id,group_id</span><br><span class="line">          -&gt; union </span><br><span class="line">          -&gt; select null name,null area_id,null group_id,count(*) count from wxm_test_fun) order by name,area_id,group_id;</span><br><span class="line">          </span><br><span class="line"> name | area_id | group_id | count </span><br><span class="line"><span class="comment">------+---------+----------+-------</span></span><br><span class="line"> wxm1 |       0 |      111 |     2 </span><br><span class="line"> wxm1 |       0 | NULL     |     2 </span><br><span class="line"> wxm1 |       3 |      113 |     1 </span><br><span class="line"> wxm1 |       3 | NULL     |     1 </span><br><span class="line"> wxm1 | NULL    | NULL     |     3 </span><br><span class="line"> wxm2 |       1 |      112 |     2 </span><br><span class="line"> wxm2 |       1 | NULL     |     2 </span><br><span class="line"> wxm2 | NULL    | NULL     |     2 </span><br><span class="line"> wxm3 |       0 |      111 |     1 </span><br><span class="line"> wxm3 |       0 | NULL     |     1 </span><br><span class="line"> wxm3 |       1 |      112 |     1 </span><br><span class="line"> wxm3 |       1 | NULL     |     1 </span><br><span class="line"> wxm3 | NULL    | NULL     |     2 </span><br><span class="line"> wxm4 |       0 |      111 |     1 </span><br><span class="line"> wxm4 |       0 |      112 |     1 </span><br><span class="line"> wxm4 |       0 | NULL     |     2 </span><br><span class="line"> wxm4 |       2 |      111 |     1 </span><br><span class="line"> wxm4 |       2 | NULL     |     1 </span><br><span class="line"> wxm4 | NULL    | NULL     |     3 </span><br><span class="line"> NULL | NULL    | NULL     |    10 </span><br><span class="line">(20 rows)</span><br><span class="line"></span><br><span class="line">Query 20190312_090034_01674_wwvpi, FINISHED, 2 nodes</span><br><span class="line">Splits: 22 total, 22 done (100.00%)</span><br><span class="line">0:01 [40 rows, 632B] [50 rows/s, 795B/s]</span><br></pre></td></tr></table></figure><h1 id="GROUPING-SETS"><a href="#GROUPING-SETS" class="headerlink" title="GROUPING SETS"></a>GROUPING SETS</h1><blockquote><p>对任意指定分组进行聚合</p><p>GROUPING SETS（（AB）,（A）,（B），（）），仅仅group by （AB） union group by A union group by B union group by null</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">presto:temp&gt; select id,name,area_id,count(*) count from wxm_test_fun group by grouping sets((id,name),name,area_id,()) order by id,name,area_id;</span><br><span class="line"></span><br><span class="line">  id  | name | area_id | count </span><br><span class="line"><span class="comment">------+------+---------+-------</span></span><br><span class="line">    1 | wxm1 | NULL    |     1 </span><br><span class="line">    1 | wxm2 | NULL    |     1 </span><br><span class="line">    1 | wxm4 | NULL    |     1 </span><br><span class="line">    2 | wxm1 | NULL    |     1 </span><br><span class="line">    2 | wxm2 | NULL    |     1 </span><br><span class="line">    3 | wxm3 | NULL    |     2 </span><br><span class="line">    4 | wxm4 | NULL    |     2 </span><br><span class="line">    5 | wxm1 | NULL    |     1 </span><br><span class="line"> NULL | wxm1 | NULL    |     3 </span><br><span class="line"> NULL | wxm2 | NULL    |     2 </span><br><span class="line"> NULL | wxm3 | NULL    |     2 </span><br><span class="line"> NULL | wxm4 | NULL    |     3 </span><br><span class="line"> NULL | NULL |       0 |     5 </span><br><span class="line"> NULL | NULL |       1 |     3 </span><br><span class="line"> NULL | NULL |       2 |     1 </span><br><span class="line"> NULL | NULL |       3 |     1 </span><br><span class="line"> NULL | NULL | NULL    |    10 </span><br><span class="line">(17 rows)</span><br><span class="line"></span><br><span class="line">Query 20190312_091338_01714_wwvpi, FINISHED, 2 nodes</span><br><span class="line">Splits: 4 total, 4 done (100.00%)</span><br><span class="line">0:00 [10 rows, 158B] [30 rows/s, 485B/s]</span><br></pre></td></tr></table></figure><ul><li>等价于grouping sets中的字段union group by</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">presto:temp&gt; select * from (</span><br><span class="line">          -&gt; select id,name,null area_id,count(*) count from wxm_test_fun group by id,name</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select null id,name,null area_id,count(*) count from wxm_test_fun group by name</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select null id,null name,area_id,count(*) count from wxm_test_fun group by area_id</span><br><span class="line">          -&gt; union</span><br><span class="line">          -&gt; select null id,null name,null area_id,count(*) count from wxm_test_fun) order by id,name,area_id;</span><br><span class="line">          </span><br><span class="line">  id  | name | area_id | count </span><br><span class="line"><span class="comment">------+------+---------+-------</span></span><br><span class="line">    1 | wxm1 | NULL    |     1 </span><br><span class="line">    1 | wxm2 | NULL    |     1 </span><br><span class="line">    1 | wxm4 | NULL    |     1 </span><br><span class="line">    2 | wxm1 | NULL    |     1 </span><br><span class="line">    2 | wxm2 | NULL    |     1 </span><br><span class="line">    3 | wxm3 | NULL    |     2 </span><br><span class="line">    4 | wxm4 | NULL    |     2 </span><br><span class="line">    5 | wxm1 | NULL    |     1 </span><br><span class="line"> NULL | wxm1 | NULL    |     3 </span><br><span class="line"> NULL | wxm2 | NULL    |     2 </span><br><span class="line"> NULL | wxm3 | NULL    |     2 </span><br><span class="line"> NULL | wxm4 | NULL    |     3 </span><br><span class="line"> NULL | NULL |       0 |     5 </span><br><span class="line"> NULL | NULL |       1 |     3 </span><br><span class="line"> NULL | NULL |       2 |     1 </span><br><span class="line"> NULL | NULL |       3 |     1 </span><br><span class="line"> NULL | NULL | NULL    |    10 </span><br><span class="line">(17 rows)</span><br><span class="line"></span><br><span class="line">Query 20190312_091542_01721_wwvpi, FINISHED, 2 nodes</span><br><span class="line">Splits: 22 total, 22 done (100.00%)</span><br><span class="line">0:00 [40 rows, 632B] [85 rows/s, 1.33KB/s]</span><br></pre></td></tr></table></figure><h1 id="维度信息获取"><a href="#维度信息获取" class="headerlink" title="维度信息获取"></a>维度信息获取</h1><blockquote><p>通常情况下，为了group by 的灵活性，通常会选用grouping sets，这就会出现一个维度标志问题。并不知道这个聚合数据哪个维度，通常可以用case when语句解决</p></blockquote><ul><li>以此句为例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,area_id,<span class="keyword">count</span>(*) <span class="keyword">count</span> <span class="keyword">from</span> wxm_test_fun <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">grouping</span> <span class="keyword">sets</span>((<span class="keyword">id</span>,<span class="keyword">name</span>),<span class="keyword">name</span>,area_id,()) <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span>,<span class="keyword">name</span>,area_id;</span><br></pre></td></tr></table></figure><p>(id,name)维度为维度01，name为维度02，area_id为维度03，()为全局维度</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">presto:temp&gt; select </span><br><span class="line">          -&gt; case when id is not null and name is not null and area_id is null then '维度01'</span><br><span class="line">          -&gt; when id is null and name is not null and area_id is null then '维度02'</span><br><span class="line">          -&gt; when id is null and name is null and area_id is not null then '维度03'</span><br><span class="line">          -&gt; when id is null and name is null and area_id is null then '全局维度'</span><br><span class="line">          -&gt; end</span><br><span class="line">          -&gt; agg_code,id,name,area_id,count(*) count from wxm_test_fun group by grouping sets((id,name),name,area_id,()) order by id,name,area_id;</span><br><span class="line">          </span><br><span class="line"> agg_code |  id  | name | area_id | count </span><br><span class="line"><span class="comment">----------+------+------+---------+-------</span></span><br><span class="line"> 维度01   |    1 | wxm1 | NULL    |     1 </span><br><span class="line"> 维度01   |    1 | wxm2 | NULL    |     1 </span><br><span class="line"> 维度01   |    1 | wxm4 | NULL    |     1 </span><br><span class="line"> 维度01   |    2 | wxm1 | NULL    |     1 </span><br><span class="line"> 维度01   |    2 | wxm2 | NULL    |     1 </span><br><span class="line"> 维度01   |    3 | wxm3 | NULL    |     2 </span><br><span class="line"> 维度01   |    4 | wxm4 | NULL    |     2 </span><br><span class="line"> 维度01   |    5 | wxm1 | NULL    |     1 </span><br><span class="line"> 维度02   | NULL | wxm1 | NULL    |     3 </span><br><span class="line"> 维度02   | NULL | wxm2 | NULL    |     2 </span><br><span class="line"> 维度02   | NULL | wxm3 | NULL    |     2 </span><br><span class="line"> 维度02   | NULL | wxm4 | NULL    |     3 </span><br><span class="line"> 维度03   | NULL | NULL |       0 |     5 </span><br><span class="line"> 维度03   | NULL | NULL |       1 |     3 </span><br><span class="line"> 维度03   | NULL | NULL |       2 |     1 </span><br><span class="line"> 维度03   | NULL | NULL |       3 |     1 </span><br><span class="line"> 全局维度 | NULL | NULL | NULL    |    10 </span><br><span class="line">(17 rows)</span><br><span class="line"></span><br><span class="line">Query 20190312_092301_01746_wwvpi, FINISHED, 2 nodes</span><br><span class="line">Splits: 4 total, 4 done (100.00%)</span><br><span class="line">0:01 [10 rows, 158B] [15 rows/s, 242B/s]</span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;在SQL语法中，经常用到GROUP BY来做多维度的聚合。但是遇到多个维度并列聚合的方式，通常是将每个维度用GROUP BY统计后，再使用UNION语法将结果集汇总，但是这样的SQL执行计划会在数据INPUT端从存储引擎获取多次，导致重复获取数据，浪费机器资源。CUBE、ROLLUP、GROUPING的语法可以更高效的做到多维度的聚合 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/BigData/"/>
    
      <category term="Hive" scheme="http://imperio-wxm.github.io/categories/BigData/Hive/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hive" scheme="http://imperio-wxm.github.io/tags/Hive/"/>
    
      <category term="SQL" scheme="http://imperio-wxm.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Apache HTTP Server 编译与安装</title>
    <link href="http://imperio-wxm.github.io/2019/03/01/Apache-HTTP-Server-Compile-Install/"/>
    <id>http://imperio-wxm.github.io/2019/03/01/Apache-HTTP-Server-Compile-Install/</id>
    <published>2019-03-01T13:43:30.000Z</published>
    <updated>2019-06-01T13:19:15.194Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>在使用ApacheBench做压力测试的时候出现<code>30s</code>request超时的情况，由于版本老旧，<code>ab -h</code>中发现并没有<code>-s timeout</code>的参数配置，Google上找了半天也没有可用的Binaries版本，于是开始自己编译httpd源码 </p></div><a id="more"></a><ul><li><a href="http://httpd.apache.org/docs/2.4/programs/ab.html" target="_blank" rel="noopener">ApacheBench Doc 文档</a></li></ul><blockquote><p>针对于ab timeout的问题(报错：The timeout specified has expired (70007))，加了<code>-k</code> 参数保证keepalived参数也无用，在2.4.4+版本中加入了<code>-s</code>参数控制timeout；</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-s timeout</span><br><span class="line">Maximum number of seconds to wait before the socket times out. Default is <span class="number">30</span> seconds. Available in <span class="number">2.4</span>.4 and later.</span><br></pre></td></tr></table></figure><table><thead><tr><th>SoftWare</th><th>Version</th></tr></thead><tbody><tr><td>Ubuntu Server x64</td><td>14.04</td></tr><tr><td>pcre</td><td>8.38</td></tr><tr><td>apr-util</td><td>1.6.1</td></tr><tr><td>apr</td><td>1.6.5</td></tr><tr><td>httpd</td><td>2.4.38</td></tr><tr><td>gcc &amp; gcc-c++</td><td>5.4.0</td></tr></tbody></table><blockquote><p>gcc &amp; gcc-c++ 系统自带；如果自己本机没有需要先install下</p></blockquote><h1 id="pcre-编译"><a href="#pcre-编译" class="headerlink" title="pcre 编译"></a>pcre 编译</h1><ul><li><a href="ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/" target="_blank" rel="noopener">pcre 下载</a></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">unzip pcre-<span class="number">8.38</span>.zip </span><br><span class="line">cd pcre-<span class="number">8.38</span></span><br><span class="line"></span><br><span class="line"># 指定编译目录</span><br><span class="line">./configure --prefix=/usr/local/pcre</span><br><span class="line"></span><br><span class="line"># 编译安装</span><br><span class="line">sudo make &amp; sudo make install</span><br></pre></td></tr></table></figure><blockquote><p>注意在make 失败后，一定要使用make clean命令清空环境</p></blockquote><ul><li><code>错误1</code></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xml/apr_xml.c:<span class="number">35</span>:<span class="number">19</span>: fatal error: expat.h: No such file or directory</span><br></pre></td></tr></table></figure><ul><li><code>解决</code></li></ul><p>Centos安装：yum install expat-devel</p><p>ubuntu安装：sudo apt-get install libexpat1-dev</p><h1 id="apr-amp-apr-util-编译"><a href="#apr-amp-apr-util-编译" class="headerlink" title="apr &amp; apr-util 编译"></a>apr &amp; apr-util 编译</h1><ul><li><p><a href="http://apr.apache.org/download.cgi" target="_blank" rel="noopener">apr &amp; apr-util 下载</a></p></li><li><p><a href="https://httpd.apache.org/download.cgi#apache24" target="_blank" rel="noopener">httpd 下载</a></p></li></ul><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apr-util-<span class="number">1.6</span>.1.tar.gz</span><br><span class="line">tar -zxvf apr-<span class="number">1.6</span>.5.tar.gz</span><br><span class="line">tar -zxvf httpd-<span class="number">2.4</span>.38.tar.gz</span><br><span class="line"></span><br><span class="line"># 一定要复制到httpd的srclib目录下，且目录版本号去掉</span><br><span class="line">cp -r apr-<span class="number">1.6</span>.5 httpd-<span class="number">2.4</span>.38/srclib/apr</span><br><span class="line">cp -r apr-util-<span class="number">1.6</span>.1 httpd-<span class="number">2.4</span>.38/srclib/apr-util</span><br></pre></td></tr></table></figure><h2 id="apr-编译"><a href="#apr-编译" class="headerlink" title="apr 编译"></a>apr 编译</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># apr 编译</span><br><span class="line">cd httpd-<span class="number">2.4</span>.38/srclib/apr</span><br><span class="line"></span><br><span class="line"># 指定编译目录</span><br><span class="line">./configure --prefix=/usr/local/apr</span><br><span class="line"></span><br><span class="line"># 编译安装</span><br><span class="line">sudo make &amp; sudo make install</span><br></pre></td></tr></table></figure><h2 id="apr-util-编译"><a href="#apr-util-编译" class="headerlink" title="apr-util 编译"></a>apr-util 编译</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># apr 编译</span><br><span class="line">cd httpd-<span class="number">2.4</span>.38/srclib/apr-util</span><br><span class="line"></span><br><span class="line"># 指定编译目录 并 指定apr依赖</span><br><span class="line">./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/apr </span><br><span class="line"></span><br><span class="line"># 编译安装</span><br><span class="line">sudo make &amp; sudo make install</span><br></pre></td></tr></table></figure><h1 id="httpd-编译"><a href="#httpd-编译" class="headerlink" title="httpd 编译"></a>httpd 编译</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd httpd-<span class="number">2.4</span>.38</span><br><span class="line"></span><br><span class="line"># 指定编译目录 并 指定apr、apr-util、pcre依赖</span><br><span class="line">./configure --prefix=/usr/local/apache-httpd --with-apr=/usr/local/apr --with-apr-util=/usr/local/apr-util --with-pcre=/usr/local/pcre</span><br><span class="line"></span><br><span class="line"># 编译安装（这个过程比较长，耐心等待）</span><br><span class="line">sudo make &amp; sudo make install</span><br></pre></td></tr></table></figure><ul><li><code>错误2</code></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recipe <span class="keyword">for</span> target <span class="string">'htpasswd'</span> failed</span><br></pre></td></tr></table></figure><ul><li><code>解决</code></li></ul><p>是因为<code>apr、apr-util</code>源码没有在<code>httpd-2.4.38/srclib/</code>目录下，cp过去就可以（注意：只需目录名，无需版本号）</p><h1 id="测试安装成功"><a href="#测试安装成功" class="headerlink" title="测试安装成功"></a>测试安装成功</h1><ul><li>ApacheBench 测试</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># 发现已经有了 -s 参数</span><br><span class="line">/usr/local/apache-httpd/bin/ab -h</span><br><span class="line"></span><br><span class="line">Usage: ./ab [options] [http[s]:<span class="comment">//]hostname[:port]/path</span></span><br><span class="line">Options are:</span><br><span class="line">    -n requests     Number of requests to perform</span><br><span class="line">    -c concurrency  Number of multiple requests to make at a time</span><br><span class="line">    -t timelimit    Seconds to max. to spend on benchmarking</span><br><span class="line">                    This implies -n <span class="number">50000</span></span><br><span class="line">    -s timeout      Seconds to max. wait <span class="keyword">for</span> each response</span><br><span class="line">                    Default is <span class="number">30</span> seconds</span><br><span class="line">    -b windowsize   Size of TCP send/receive buffer, in bytes</span><br><span class="line">    -B address      Address to bind to when making outgoing connections</span><br><span class="line">    -p postfile     File containing data to POST. Remember also to set -T</span><br><span class="line">    -u putfile      File containing data to PUT. Remember also to set -T</span><br><span class="line">    -T content-type Content-type header to use <span class="keyword">for</span> POST/PUT data, eg.</span><br><span class="line">                    <span class="string">'application/x-www-form-urlencoded'</span></span><br><span class="line">                    Default is <span class="string">'text/plain'</span></span><br><span class="line">    -v verbosity    How much troubleshooting info to print</span><br><span class="line">    -w              Print out results in HTML tables</span><br><span class="line">    -i              Use HEAD instead of GET</span><br><span class="line">    -x attributes   String to insert as table attributes</span><br><span class="line">    -y attributes   String to insert as tr attributes</span><br><span class="line">    -z attributes   String to insert as td or th attributes</span><br><span class="line">    -C attribute    Add cookie, eg. <span class="string">'Apache=1234'</span>. (repeatable)</span><br><span class="line">    -H attribute    Add Arbitrary header line, eg. <span class="string">'Accept-Encoding: gzip'</span></span><br><span class="line">                    Inserted after all normal header lines. (repeatable)</span><br><span class="line">    -A attribute    Add Basic WWW Authentication, the attributes</span><br><span class="line">                    are a colon separated username and password.</span><br><span class="line">    -P attribute    Add Basic Proxy Authentication, the attributes</span><br><span class="line">                    are a colon separated username and password.</span><br><span class="line">    -X proxy:port   Proxyserver and port number to use</span><br><span class="line">    -V              Print version number and exit</span><br><span class="line">    -k              Use HTTP KeepAlive feature</span><br><span class="line">    -d              Do not show percentiles served table.</span><br><span class="line">    -S              Do not show confidence estimators and warnings.</span><br><span class="line">    -q              Do not show progress when doing more than <span class="number">150</span> requests</span><br><span class="line">    -<span class="function">l              Accept variable document <span class="title">length</span> <span class="params">(use <span class="keyword">this</span> <span class="keyword">for</span> dynamic pages)</span></span></span><br><span class="line"><span class="function">    -g filename     Output collected data to gnuplot format file.</span></span><br><span class="line"><span class="function">    -e filename     Output CSV file with percentages served</span></span><br><span class="line"><span class="function">    -r              Don't exit on socket receive errors.</span></span><br><span class="line"><span class="function">    -m method       Method name</span></span><br><span class="line"><span class="function">    -h              Display usage <span class="title">information</span> <span class="params">(<span class="keyword">this</span> message)</span></span></span><br><span class="line"><span class="function">    -I              Disable TLS Server Name <span class="title">Indication</span> <span class="params">(SNI)</span> extension</span></span><br><span class="line"><span class="function">    -Z ciphersuite  Specify SSL/TLS cipher <span class="title">suite</span> <span class="params">(See openssl ciphers)</span></span></span><br><span class="line"><span class="function">    -f protocol     Specify SSL/TLS protocol</span></span><br><span class="line"><span class="function">                    <span class="params">(TLS1, TLS1<span class="number">.1</span>, TLS1<span class="number">.2</span> or ALL)</span></span></span><br><span class="line"><span class="function">    -E certfile     Specify optional client certificate chain and <span class="keyword">private</span> key</span></span><br></pre></td></tr></table></figure><ul><li>服务测试</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudovim /usr/local/apache-httpd/conf/httpd.conf</span><br><span class="line"></span><br><span class="line"># 添加ServerName</span><br><span class="line">ServerName localhost:<span class="number">80</span></span><br><span class="line"></span><br><span class="line">sudo /usr/local/apache-httpd/bin/apachectl start</span><br><span class="line"></span><br><span class="line">curl localhost</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;</span><br><span class="line"></span><br><span class="line">sudo /usr/local/apache-httpd/bin/apachectl stop</span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;在使用ApacheBench做压力测试的时候出现&lt;code&gt;30s&lt;/code&gt;request超时的情况，由于版本老旧，&lt;code&gt;ab -h&lt;/code&gt;中发现并没有&lt;code&gt;-s timeout&lt;/code&gt;的参数配置，Google上找了半天也没有可用的Binaries版本，于是开始自己编译httpd源码 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/categories/Linux/"/>
    
    
      <category term="Apache" scheme="http://imperio-wxm.github.io/tags/Apache/"/>
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Apache Phoenix RowTimestamp</title>
    <link href="http://imperio-wxm.github.io/2019/02/25/Apache-Phoenix-Row-Timestamp/"/>
    <id>http://imperio-wxm.github.io/2019/02/25/Apache-Phoenix-Row-Timestamp/</id>
    <published>2019-02-25T11:34:12.000Z</published>
    <updated>2019-06-01T13:17:39.048Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>从4.6版本起，Phoenix支持一种时间戳映射到列的方法（即HBase中 column 的RowTimestamp），以便对Phoenix在存储时序数据时查询的优化 </p></div><a id="more"></a><h1 id="Row-Timestamp介绍"><a href="#Row-Timestamp介绍" class="headerlink" title="Row Timestamp介绍"></a>Row Timestamp介绍</h1><blockquote><p>在建表时对特定列指定<code>ROW_TIMESTAMP</code>关键字：</p></blockquote><ul><li>只有当parimary key为以下类型时才可以使用<code>ROW_TIMESTAMP</code>：<code>TIME, DATE, TIMESTAMP, BIGINT, UNSIGNED_LONG</code></li><li>若primary key为组合主键的时候，只有一个column能指定为<code>ROW_TIMESTAMP</code></li><li>此列的值不能为null</li><li>此列的值不能为负</li></ul><p>当使用UPSERT VALUES or UPSERT SELECT语句更新数据时，可以指定<code>ROW_TIMESTAMP</code>的值，如果不指定则默认以服务器时间为此列的时间戳，同时这个时间戳也对应hbase 中row的timestamp</p><p>当查询过滤<code>ROW_TIMESTAMP</code>时，不仅可以做常规的查询优化，同时可以依靠timestamp对数据进行最大最小的范围优化，hbase服务器端可以直接跳过不在时间区间内的hfile文件从而使扫描效率提高，尤其是查询数据尾端的时候</p><p><a href="http://phoenix.apache.org/rowtimestamp.html" target="_blank" rel="noopener">Phenix RowTimestap Doc</a></p><table><thead><tr><th>SoftWare</th><th>Version</th></tr></thead><tbody><tr><td>Hbase</td><td>1.2.0-cdh5.11.1</td></tr><tr><td>Phoenix</td><td>4.13.0-cdh5.11.1</td></tr><tr><td>Java</td><td>1.8.0_121</td></tr></tbody></table><p>部署环境：4 Region Servers</p><h1 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> phoenix_dev.test_ywzx_wuhan_switch (</span><br><span class="line">event_time <span class="keyword">TIMESTAMP</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="comment">/*event_time*/</span>,</span><br><span class="line"><span class="string">"_KEY"</span> <span class="built_in">BIGINT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="comment">/*_key*/</span>,</span><br><span class="line">switch_output_multicastpkts_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_output_multicastpkts_delta*/</span>,</span><br><span class="line">switch_output_broadcastpkts_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_output_broadcastpkts_delta*/</span>,</span><br><span class="line">switch_input_errors_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_input_errors_delta*/</span>,</span><br><span class="line">switch_cpu_utilization <span class="built_in">BIGINT</span> <span class="comment">/*switch_cpu_utilization*/</span>,</span><br><span class="line">switch_sysobjectid <span class="built_in">VARCHAR</span> <span class="comment">/*switch_sysobjectid*/</span>,</span><br><span class="line">switch_memory_utilization <span class="built_in">FLOAT</span> <span class="comment">/*switch_memory_utilization*/</span>,</span><br><span class="line">switch_input_multicastpkts_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_input_multicastpkts_delta*/</span>,</span><br><span class="line">switch_input_bytes_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_input_bytes_delta*/</span>,</span><br><span class="line">switch_output_errors_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_output_errors_delta*/</span>,</span><br><span class="line">switch_output_bytes_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_output_bytes_delta*/</span>,</span><br><span class="line">switch_port_status <span class="built_in">VARCHAR</span> <span class="comment">/*switch_port_status*/</span>,</span><br><span class="line">switch_output_ucastpkts_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_output_ucastpkts_delta*/</span>,</span><br><span class="line">switch_portname <span class="built_in">VARCHAR</span> <span class="comment">/*switch_portname*/</span>,</span><br><span class="line">switch_fan_status <span class="built_in">VARCHAR</span> <span class="comment">/*switch_fan_status*/</span>,</span><br><span class="line">switch_index <span class="built_in">VARCHAR</span> <span class="comment">/*switch_index*/</span>,</span><br><span class="line">switch_host <span class="built_in">VARCHAR</span> <span class="comment">/*switch_host*/</span>,</span><br><span class="line">switch_gims_measurement <span class="built_in">VARCHAR</span> <span class="comment">/*switch_gims_measurement*/</span>,</span><br><span class="line">switch_power_status <span class="built_in">VARCHAR</span> <span class="comment">/*switch_power_status*/</span>,</span><br><span class="line">switch_input_broadcastpkts_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_input_broadcastpkts_delta*/</span>,</span><br><span class="line">switch_input_ucastpkts_delta <span class="built_in">BIGINT</span> <span class="comment">/*switch_input_ucastpkts_delta*/</span>,</span><br><span class="line">switch_errorstatus <span class="built_in">INTEGER</span> <span class="comment">/*switch_errorstatus*/</span>,</span><br><span class="line"><span class="keyword">CONSTRAINT</span> pk PRIMARY <span class="keyword">KEY</span> (event_time <span class="keyword">ASC</span>, <span class="string">"_KEY"</span> <span class="keyword">ASC</span>)</span><br><span class="line">) COMPRESSION = SNAPPY,</span><br><span class="line"> SALT_BUCKETS = <span class="number">4</span>,</span><br><span class="line"> <span class="keyword">VERSIONS</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">UPSERT INTO PHOENIX_DEV.test_ywzx_wuhan_switch (</span><br><span class="line">event_time,</span><br><span class="line">_KEY,</span><br><span class="line">switch_output_multicastpkts_delta,</span><br><span class="line">switch_output_broadcastpkts_delta,</span><br><span class="line">switch_input_errors_delta,</span><br><span class="line">switch_cpu_utilization,</span><br><span class="line">switch_sysobjectid,</span><br><span class="line">switch_memory_utilization,</span><br><span class="line">switch_input_multicastpkts_delta,</span><br><span class="line">switch_input_bytes_delta,</span><br><span class="line">switch_output_errors_delta,</span><br><span class="line">switch_output_bytes_delta,</span><br><span class="line">switch_port_status,</span><br><span class="line">switch_output_ucastpkts_delta,</span><br><span class="line">switch_portname,</span><br><span class="line">switch_fan_status,</span><br><span class="line">switch_index,</span><br><span class="line">switch_host,</span><br><span class="line">switch_gims_measurement,</span><br><span class="line">switch_power_status,</span><br><span class="line">switch_input_broadcastpkts_delta,</span><br><span class="line">switch_input_ucastpkts_delta,</span><br><span class="line">switch_errorstatus</span><br><span class="line">)</span><br><span class="line">VALUES</span><br><span class="line">(</span><br><span class="line">?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>primary key 组成：CONSTRAINT pk PRIMARY KEY (event_time ASC, “_KEY” ASC)</li></ul><p><code>建立一张表结构完全一样的表test_ywzx_wuhan_switch_rowtimestap，给event_time字段添加ROW_TIMESTAMP</code></p><p><code>即给时间主键设置ROW_TIMESTAMP</code></p><ul><li>数据量及大小</li></ul><p>2000w条数据<br>压缩后size：<br>3.5 G  10.4 G  /hbase/data/PHOENIX_DEV/TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP</p><p>event_time时间范围： 2018-10-21 14:03:34.503 —— 2018-12-06 21:10:14.303</p><p>自动split一次，由于salt = 4，所以写入始终向4 Region Servers，查询旧数据则扫描前4个RS，查询新数据扫描后4个RS，全表查询则使用8个RS</p><p><code>select count(*) from PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH where event_time &gt;= to_timestamp(&#39;2018-10-21 00:00:00&#39;) AND event_time &lt;= to_timestamp(&#39;2018-12-05 00:00:00&#39;)</code><br><code>size = 1900w+</code></p><h2 id="测试语句一"><a href="#测试语句一" class="headerlink" title="测试语句一"></a>测试语句一</h2><blockquote><p>无ROW_TIMESTAMP</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| COUNT(1)  |</span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| 20000000  |</span><br><span class="line">+<span class="comment">-----------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">explain</span></span><br><span class="line">+<span class="comment">-------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">|                                                       PLAN                                                        | EST_BYTES_READ  | EST_ROWS_READ  |  EST_INFO_TS   |</span><br><span class="line">+<span class="comment">-------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">72</span>-<span class="keyword">CHUNK</span> <span class="number">18453404</span> <span class="keyword">ROWS</span> <span class="number">20132661420</span> <span class="keyword">BYTES</span> <span class="keyword">PARALLEL</span> <span class="number">4</span>-WAY <span class="keyword">FULL</span> <span class="keyword">SCAN</span> <span class="keyword">OVER</span> PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH  | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> FILTER <span class="keyword">BY</span> <span class="keyword">FIRST</span> <span class="keyword">KEY</span> <span class="keyword">ONLY</span>                                                                               | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> SINGLE <span class="keyword">ROW</span>                                                                              | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">+<span class="comment">-------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>第N次执行</th><th>耗时</th></tr></thead><tbody><tr><td>1</td><td>6.159 seconds</td></tr><tr><td>2</td><td>5.537 seconds</td></tr><tr><td>3</td><td>4.686 seconds</td></tr><tr><td>4</td><td>4.429 seconds</td></tr><tr><td>5</td><td>4.943 seconds</td></tr></tbody></table><blockquote><p>有ROW_TIMESTAMP</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| COUNT(1)  |</span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| 20000000  |</span><br><span class="line">+<span class="comment">-----------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">explain</span></span><br><span class="line">+<span class="comment">--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">|                                                              PLAN                                                              | EST_BYTES_READ  | EST_ROWS_READ  |  EST_INFO_TS   |</span><br><span class="line">+<span class="comment">--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">52</span>-<span class="keyword">CHUNK</span> <span class="number">12686716</span> <span class="keyword">ROWS</span> <span class="number">13841204812</span> <span class="keyword">BYTES</span> <span class="keyword">PARALLEL</span> <span class="number">4</span>-WAY <span class="keyword">FULL</span> <span class="keyword">SCAN</span> <span class="keyword">OVER</span> PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP  | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">ROW</span> <span class="keyword">TIMESTAMP</span> FILTER [<span class="number">0</span>, <span class="number">9223372036854775807</span>)                                                                              | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> FILTER <span class="keyword">BY</span> <span class="keyword">FIRST</span> <span class="keyword">KEY</span> <span class="keyword">ONLY</span>                                                                                            | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> SINGLE <span class="keyword">ROW</span>                                                                                           | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">+<span class="comment">--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>第N次执行</th><th>耗时</th></tr></thead><tbody><tr><td>1</td><td>21.726 seconds</td></tr><tr><td>2</td><td>20.906 seconds</td></tr><tr><td>3</td><td>19.494 seconds</td></tr><tr><td>4</td><td>19.514 seconds</td></tr><tr><td>5</td><td>19.478 seconds</td></tr></tbody></table><h2 id="测试语句二"><a href="#测试语句二" class="headerlink" title="测试语句二"></a>测试语句二</h2><blockquote><p>无ROW_TIMESTAMP</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(SWITCH_HOST),SWITCH_HOST,<span class="keyword">count</span>(SWITCH_POWER_STATUS),SWITCH_POWER_STATUS <span class="keyword">from</span> PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH <span class="keyword">where</span> event_time &gt;= to_timestamp(<span class="string">'2018-10-21 00:00:00'</span>) <span class="keyword">AND</span> event_time &lt;= to_timestamp(<span class="string">'2018-12-05 00:00:00'</span>) <span class="keyword">group</span> <span class="keyword">by</span> SWITCH_HOST,SWITCH_POWER_STATUS <span class="keyword">limit</span> <span class="number">5000</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">explain</span></span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">|                                                                                       PLAN                                                                                        | EST_BYTES_READ  | EST_ROWS_READ  |  EST_INFO_TS   |</span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">72</span>-<span class="keyword">CHUNK</span> <span class="number">18453404</span> <span class="keyword">ROWS</span> <span class="number">20132661420</span> <span class="keyword">BYTES</span> <span class="keyword">PARALLEL</span> <span class="number">72</span>-WAY <span class="keyword">RANGE</span> <span class="keyword">SCAN</span> <span class="keyword">OVER</span> PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH [<span class="number">0</span>,<span class="string">'2018-10-20 16:00:00.000'</span>] - [<span class="number">3</span>,<span class="string">'2018-12-04 16:00:00.000'</span>]  | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> <span class="keyword">DISTINCT</span> <span class="keyword">ROWS</span> <span class="keyword">BY</span> [SWITCH_HOST, SWITCH_POWER_STATUS]                                                                                                     | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="keyword">MERGE</span> <span class="keyword">SORT</span>                                                                                                                                                                 | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">5000</span> <span class="keyword">ROW</span> <span class="keyword">LIMIT</span>                                                                                                                                                             | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>第N次执行</th><th>耗时</th></tr></thead><tbody><tr><td>1</td><td>5.714 seconds</td></tr><tr><td>2</td><td>5.355 seconds</td></tr><tr><td>3</td><td>4.98 seconds</td></tr><tr><td>4</td><td>4.922 seconds</td></tr><tr><td>5</td><td>5.02 seconds</td></tr></tbody></table><blockquote><p>有ROW_TIMESTAMP</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(SWITCH_HOST),SWITCH_HOST,<span class="keyword">count</span>(SWITCH_POWER_STATUS),SWITCH_POWER_STATUS <span class="keyword">from</span> PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP <span class="keyword">where</span> event_time &gt;= to_timestamp(<span class="string">'2018-10-21 00:00:00'</span>) <span class="keyword">AND</span> event_time &lt;= to_timestamp(<span class="string">'2018-12-05 00:00:00'</span>) <span class="keyword">group</span> <span class="keyword">by</span> SWITCH_HOST,SWITCH_POWER_STATUS <span class="keyword">limit</span> <span class="number">5000</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">explain</span></span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">|                                                                                              PLAN                                                                                              | EST_BYTES_READ  | EST_ROWS_READ  |  EST_INFO_TS   |</span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">52</span>-<span class="keyword">CHUNK</span> <span class="number">12686716</span> <span class="keyword">ROWS</span> <span class="number">13841204812</span> <span class="keyword">BYTES</span> <span class="keyword">PARALLEL</span> <span class="number">52</span>-WAY <span class="keyword">RANGE</span> <span class="keyword">SCAN</span> <span class="keyword">OVER</span> PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP [<span class="number">0</span>,<span class="string">'2018-10-20 16:00:00.000'</span>] - [<span class="number">3</span>,<span class="string">'2018-12-04 16:00:00.000'</span>]  | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">ROW</span> <span class="keyword">TIMESTAMP</span> FILTER [<span class="number">1540051200000</span>, <span class="number">1543939200001</span>)                                                                                                                                        | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> <span class="keyword">DISTINCT</span> <span class="keyword">ROWS</span> <span class="keyword">BY</span> [SWITCH_HOST, SWITCH_POWER_STATUS]                                                                                                                  | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="keyword">MERGE</span> <span class="keyword">SORT</span>                                                                                                                                                                              | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">5000</span> <span class="keyword">ROW</span> <span class="keyword">LIMIT</span>                                                                                                                                                                          | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>第N次执行</th><th>耗时</th></tr></thead><tbody><tr><td>1</td><td>19.391 seconds</td></tr><tr><td>2</td><td>18.249 seconds</td></tr><tr><td>3</td><td>18.195 seconds</td></tr><tr><td>4</td><td>18.567 seconds</td></tr><tr><td>5</td><td>18.247 seconds</td></tr></tbody></table><h2 id="测试语句三"><a href="#测试语句三" class="headerlink" title="测试语句三"></a>测试语句三</h2><blockquote><p>无ROW_TIMESTAMP</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> (<span class="keyword">SELECT</span> <span class="keyword">CAST</span>(<span class="keyword">floor</span>(event_time, <span class="string">'MINUTE'</span>, <span class="number">1</span>) <span class="keyword">AS</span> <span class="keyword">timestamp</span>) <span class="keyword">AS</span> log_time, switch_host, <span class="keyword">MAX</span>(switch_cpu_utilization) <span class="keyword">AS</span> cpu_utilization <span class="keyword">FROM</span> PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH <span class="keyword">AS</span> ywzx_wuhan_switch <span class="keyword">WHERE</span> switch_gims_measurement = <span class="string">'ddd'</span> <span class="keyword">AND</span> switch_host = <span class="string">'fff'</span> <span class="keyword">AND</span> ywzx_wuhan_switch.event_time &gt;= to_timestamp(<span class="string">'2018-10-21 00:00:00'</span>) <span class="keyword">AND</span> ywzx_wuhan_switch.event_time &lt;= to_timestamp(<span class="string">'2018-12-05 00:00:00'</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> log_time, switch_host <span class="keyword">ORDER</span> <span class="keyword">BY</span>  switch_host,log_time <span class="keyword">desc</span>) <span class="keyword">LIMIT</span> <span class="number">5000</span> <span class="keyword">offset</span> <span class="number">0</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">explain</span> </span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">|                                                                                       PLAN                                                                                        | EST_BYTES_READ  | EST_ROWS_READ  |  EST_INFO_TS   |</span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">72</span>-<span class="keyword">CHUNK</span> <span class="number">18453404</span> <span class="keyword">ROWS</span> <span class="number">20132661420</span> <span class="keyword">BYTES</span> <span class="keyword">PARALLEL</span> <span class="number">72</span>-WAY <span class="keyword">RANGE</span> <span class="keyword">SCAN</span> <span class="keyword">OVER</span> PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH [<span class="number">0</span>,<span class="string">'2018-10-20 16:00:00.000'</span>] - [<span class="number">3</span>,<span class="string">'2018-12-04 16:00:00.000'</span>]  | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> FILTER <span class="keyword">BY</span> (SWITCH_GIMS_MEASUREMENT = <span class="string">'ddd'</span> <span class="keyword">AND</span> SWITCH_HOST = <span class="string">'fff'</span>)                                                                                                    | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> <span class="keyword">DISTINCT</span> <span class="keyword">ROWS</span> <span class="keyword">BY</span> [TO_TIMESTAMP(<span class="keyword">FLOOR</span>(<span class="keyword">TO_DATE</span>(EVENT_TIME))), SWITCH_HOST]                                                                                | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="keyword">MERGE</span> <span class="keyword">SORT</span>                                                                                                                                                                 | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> TOP <span class="number">5000</span> <span class="keyword">ROWS</span> SORTED <span class="keyword">BY</span> [SWITCH_HOST, TO_TIMESTAMP(<span class="keyword">FLOOR</span>(<span class="keyword">TO_DATE</span>(EVENT_TIME))) <span class="keyword">DESC</span>]                                                                                       | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>第N次执行</th><th>耗时</th></tr></thead><tbody><tr><td>1</td><td>6.917 seconds</td></tr><tr><td>2</td><td>5.914 seconds</td></tr><tr><td>3</td><td>5.929 seconds</td></tr><tr><td>4</td><td>6.091 seconds</td></tr><tr><td>5</td><td>6.135 seconds</td></tr></tbody></table><blockquote><p>有ROW_TIMESTAMP</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> (<span class="keyword">SELECT</span> <span class="keyword">CAST</span>(<span class="keyword">floor</span>(event_time, <span class="string">'MINUTE'</span>, <span class="number">1</span>) <span class="keyword">AS</span> <span class="keyword">timestamp</span>) <span class="keyword">AS</span> log_time, switch_host, <span class="keyword">MAX</span>(switch_cpu_utilization) <span class="keyword">AS</span> cpu_utilization <span class="keyword">FROM</span> PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP <span class="keyword">AS</span> ywzx_wuhan_switch <span class="keyword">WHERE</span> switch_gims_measurement = <span class="string">'ddd'</span> <span class="keyword">AND</span> switch_host = <span class="string">'fff'</span> <span class="keyword">AND</span> ywzx_wuhan_switch.event_time &gt;= to_timestamp(<span class="string">'2018-10-21 00:00:00'</span>) <span class="keyword">AND</span> ywzx_wuhan_switch.event_time &lt;= to_timestamp(<span class="string">'2018-12-05 00:00:00'</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> log_time, switch_host <span class="keyword">ORDER</span> <span class="keyword">BY</span>  switch_host,log_time <span class="keyword">desc</span>) <span class="keyword">LIMIT</span> <span class="number">5000</span> <span class="keyword">offset</span> <span class="number">0</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">explain</span> </span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">|                                                                                              PLAN                                                                                              | EST_BYTES_READ  | EST_ROWS_READ  |  EST_INFO_TS   |</span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">52</span>-<span class="keyword">CHUNK</span> <span class="number">12686716</span> <span class="keyword">ROWS</span> <span class="number">13841204812</span> <span class="keyword">BYTES</span> <span class="keyword">PARALLEL</span> <span class="number">52</span>-WAY <span class="keyword">RANGE</span> <span class="keyword">SCAN</span> <span class="keyword">OVER</span> PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP [<span class="number">0</span>,<span class="string">'2018-10-20 16:00:00.000'</span>] - [<span class="number">3</span>,<span class="string">'2018-12-04 16:00:00.000'</span>]  | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">ROW</span> <span class="keyword">TIMESTAMP</span> FILTER [<span class="number">1540051200000</span>, <span class="number">1543939200001</span>)                                                                                                                                        | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> FILTER <span class="keyword">BY</span> (SWITCH_GIMS_MEASUREMENT = <span class="string">'ddd'</span> <span class="keyword">AND</span> SWITCH_HOST = <span class="string">'fff'</span>)                                                                                                                 | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> <span class="keyword">DISTINCT</span> <span class="keyword">ROWS</span> <span class="keyword">BY</span> [TO_TIMESTAMP(<span class="keyword">FLOOR</span>(<span class="keyword">TO_DATE</span>(EVENT_TIME))), SWITCH_HOST]                                                                                             | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="keyword">MERGE</span> <span class="keyword">SORT</span>                                                                                                                                                                              | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> TOP <span class="number">5000</span> <span class="keyword">ROWS</span> SORTED <span class="keyword">BY</span> [SWITCH_HOST, TO_TIMESTAMP(<span class="keyword">FLOOR</span>(<span class="keyword">TO_DATE</span>(EVENT_TIME))) <span class="keyword">DESC</span>]                                                                                                    | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>第N次执行</th><th>耗时</th></tr></thead><tbody><tr><td>1</td><td>19.875 seconds</td></tr><tr><td>2</td><td>20.301 seconds</td></tr><tr><td>3</td><td>18.88 seconds</td></tr><tr><td>4</td><td>19.706 seconds</td></tr><tr><td>5</td><td>19.302 seconds</td></tr></tbody></table><h2 id="测试语句四"><a href="#测试语句四" class="headerlink" title="测试语句四"></a>测试语句四</h2><blockquote><p>无ROW_TIMESTAMP</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">count</span>(*) <span class="keyword">FROM</span> (<span class="keyword">SELECT</span> <span class="keyword">CAST</span>(<span class="keyword">floor</span>(event_time, <span class="string">'MINUTE'</span>, <span class="number">1</span>) <span class="keyword">AS</span> <span class="keyword">timestamp</span>) <span class="keyword">AS</span> log_time, switch_host, <span class="keyword">MAX</span>(switch_cpu_utilization) <span class="keyword">AS</span> cpu_utilization <span class="keyword">FROM</span> PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH <span class="keyword">AS</span> TEST_YWZX_WUHAN_SWITCH <span class="keyword">WHERE</span> switch_gims_measurement = <span class="string">'ddd'</span> <span class="keyword">AND</span> switch_host = <span class="string">'fff'</span> <span class="keyword">AND</span> TEST_YWZX_WUHAN_SWITCH.event_time &gt;= to_timestamp(<span class="string">'2018-10-21 00:00:00'</span>) <span class="keyword">AND</span> TEST_YWZX_WUHAN_SWITCH.event_time &lt;= to_timestamp(<span class="string">'2018-12-05 00:00:00'</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">CAST</span>(<span class="keyword">floor</span>(event_time, <span class="string">'MINUTE'</span>, <span class="number">1</span>) <span class="keyword">AS</span> <span class="keyword">timestamp</span>), switch_host);</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| COUNT(1)  |</span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| 60357     |</span><br><span class="line">+<span class="comment">-----------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">explain</span> </span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">|                                                                                       PLAN                                                                                        | EST_BYTES_READ  | EST_ROWS_READ  |  EST_INFO_TS   |</span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">72</span>-<span class="keyword">CHUNK</span> <span class="number">18453404</span> <span class="keyword">ROWS</span> <span class="number">20132661420</span> <span class="keyword">BYTES</span> <span class="keyword">PARALLEL</span> <span class="number">72</span>-WAY <span class="keyword">RANGE</span> <span class="keyword">SCAN</span> <span class="keyword">OVER</span> PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH [<span class="number">0</span>,<span class="string">'2018-10-20 16:00:00.000'</span>] - [<span class="number">3</span>,<span class="string">'2018-12-04 16:00:00.000'</span>]  | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> FILTER <span class="keyword">BY</span> (SWITCH_GIMS_MEASUREMENT = <span class="string">'ddd'</span> <span class="keyword">AND</span> SWITCH_HOST = <span class="string">'fff'</span>)                                                                                                    | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> <span class="keyword">DISTINCT</span> <span class="keyword">ROWS</span> <span class="keyword">BY</span> [TO_TIMESTAMP(<span class="keyword">FLOOR</span>(<span class="keyword">TO_DATE</span>(EVENT_TIME))), SWITCH_HOST]                                                                                | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="keyword">MERGE</span> <span class="keyword">SORT</span>                                                                                                                                                                 | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> SINGLE <span class="keyword">ROW</span>                                                                                                                                                  | <span class="number">20132661420</span>     | <span class="number">18453404</span>       | <span class="number">1550646317452</span>  |</span><br><span class="line">+<span class="comment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>第N次执行</th><th>耗时</th></tr></thead><tbody><tr><td>1</td><td>6.888 seconds</td></tr><tr><td>2</td><td>5.896 seconds</td></tr><tr><td>3</td><td>5.263 seconds</td></tr><tr><td>4</td><td>5.576 seconds</td></tr><tr><td>5</td><td>5.665 seconds</td></tr></tbody></table><blockquote><p>有ROW_TIMESTAMP</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">SELECT</span> <span class="keyword">count</span>(*) <span class="keyword">FROM</span> (<span class="keyword">SELECT</span> <span class="keyword">CAST</span>(<span class="keyword">floor</span>(event_time, <span class="string">'MINUTE'</span>, <span class="number">1</span>) <span class="keyword">AS</span> <span class="keyword">timestamp</span>) <span class="keyword">AS</span> log_time, switch_host, <span class="keyword">MAX</span>(switch_cpu_utilization) <span class="keyword">AS</span> cpu_utilization <span class="keyword">FROM</span> PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP <span class="keyword">AS</span> TEST_YWZX_WUHAN_SWITCH <span class="keyword">WHERE</span> switch_gims_measurement = <span class="string">'ddd'</span> <span class="keyword">AND</span> switch_host = <span class="string">'fff'</span> <span class="keyword">AND</span> TEST_YWZX_WUHAN_SWITCH.event_time &gt;= to_timestamp(<span class="string">'2018-10-21 00:00:00'</span>) <span class="keyword">AND</span> TEST_YWZX_WUHAN_SWITCH.event_time &lt;= to_timestamp(<span class="string">'2018-12-05 00:00:00'</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">CAST</span>(<span class="keyword">floor</span>(event_time, <span class="string">'MINUTE'</span>, <span class="number">1</span>) <span class="keyword">AS</span> <span class="keyword">timestamp</span>), switch_host);</span><br><span class="line"> </span><br><span class="line"> +<span class="comment">-----------+</span></span><br><span class="line">| COUNT(1)  |</span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| 60332     |</span><br><span class="line">+<span class="comment">-----------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">explain</span> </span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">|                                                                                              PLAN                                                                                              | EST_BYTES_READ  | EST_ROWS_READ  |  EST_INFO_TS   |</span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="number">52</span>-<span class="keyword">CHUNK</span> <span class="number">12686716</span> <span class="keyword">ROWS</span> <span class="number">13841204812</span> <span class="keyword">BYTES</span> <span class="keyword">PARALLEL</span> <span class="number">52</span>-WAY <span class="keyword">RANGE</span> <span class="keyword">SCAN</span> <span class="keyword">OVER</span> PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP [<span class="number">0</span>,<span class="string">'2018-10-20 16:00:00.000'</span>] - [<span class="number">3</span>,<span class="string">'2018-12-04 16:00:00.000'</span>]  | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">ROW</span> <span class="keyword">TIMESTAMP</span> FILTER [<span class="number">1540051200000</span>, <span class="number">1543939200001</span>)                                                                                                                                        | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> FILTER <span class="keyword">BY</span> (SWITCH_GIMS_MEASUREMENT = <span class="string">'ddd'</span> <span class="keyword">AND</span> SWITCH_HOST = <span class="string">'fff'</span>)                                                                                                                 | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">|     <span class="keyword">SERVER</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> <span class="keyword">DISTINCT</span> <span class="keyword">ROWS</span> <span class="keyword">BY</span> [TO_TIMESTAMP(<span class="keyword">FLOOR</span>(<span class="keyword">TO_DATE</span>(EVENT_TIME))), SWITCH_HOST]                                                                                             | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="keyword">MERGE</span> <span class="keyword">SORT</span>                                                                                                                                                                              | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">| <span class="keyword">CLIENT</span> <span class="keyword">AGGREGATE</span> <span class="keyword">INTO</span> SINGLE <span class="keyword">ROW</span>                                                                                                                                                               | <span class="number">13841204812</span>     | <span class="number">12686716</span>       | <span class="number">1550647235912</span>  |</span><br><span class="line">+<span class="comment">------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>第N次执行</th><th>耗时</th></tr></thead><tbody><tr><td>1</td><td>19.967 seconds</td></tr><tr><td>2</td><td>18.854 seconds</td></tr><tr><td>3</td><td>19.637 seconds</td></tr><tr><td>4</td><td>18.701 seconds</td></tr><tr><td>5</td><td>18.986 seconds</td></tr></tbody></table><p><code>[JIRA 问题追踪](https://issues.apache.org/jira/browse/PHOENIX-5157)</code></p><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;从4.6版本起，Phoenix支持一种时间戳映射到列的方法（即HBase中 column 的RowTimestamp），以便对Phoenix在存储时序数据时查询的优化 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="Phoenix" scheme="http://imperio-wxm.github.io/categories/Phoenix/"/>
    
      <category term="HBase" scheme="http://imperio-wxm.github.io/categories/Phoenix/HBase/"/>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/Phoenix/HBase/BigData/"/>
    
    
      <category term="Phoenix" scheme="http://imperio-wxm.github.io/tags/Phoenix/"/>
    
      <category term="HBase" scheme="http://imperio-wxm.github.io/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>Hive 优雅的统计表（分区）Size and RowCount</title>
    <link href="http://imperio-wxm.github.io/2019/02/21/Hive-Table-Statistics/"/>
    <id>http://imperio-wxm.github.io/2019/02/21/Hive-Table-Statistics/</id>
    <published>2019-02-21T14:07:43.000Z</published>
    <updated>2019-06-01T13:18:11.047Z</updated>
    
    <content type="html"><![CDATA[<div class="note success"><p>在Hadoop平台运维监控中，Hive表的统计信息通常是作为集群数据质量的关键依据；通常以Hive表的大小、行数、分区数等信息来衡量集群数据量的增减趋势；本文以Hive表Size大小和数据行数作为重点，对于超大表统计时避免过多的资源消耗 </p></div><a id="more"></a><h2 id="两种常规方式统计："><a href="#两种常规方式统计：" class="headerlink" title="两种常规方式统计："></a>两种常规方式统计：</h2><ul><li>方式一：</li></ul><p>size 统计：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> size = FileSystem.getContentSummary(<span class="keyword">new</span> Path(tablePath)).getLength();</span><br></pre></td></tr></table></figure></p><p>rowCount统计：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">select <span class="title">count</span><span class="params">(*)</span> from tableName</span>;</span><br></pre></td></tr></table></figure><ul><li>方式二：</li></ul><p>size统计：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -du -s tablePath</span><br></pre></td></tr></table></figure><p>rowCount统计（非Orc、Parquet）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -text tablePath | wc -l</span><br></pre></td></tr></table></figure><blockquote><p>常规方式在获取rowCount时都有局限性（例如OrcFile无法用hadoop fs -text），并且效率不高</p></blockquote><h2 id="ANALYZE-方式统计"><a href="#ANALYZE-方式统计" class="headerlink" title="ANALYZE 方式统计"></a>ANALYZE 方式统计</h2><blockquote><p>ANALYZE TABLE tablename [PARTITION(partcol1[=val1], partcol2[=val2], …)] COMPUTE STATISTICS [noscan];</p></blockquote><p><code>考虑到表的统计值通常是要通过计算后记录到某个地方，执行ANALYZE的时候才会快速显示；为此探究什么情况下导入Hive数据，ANALYZE的结果准确</code></p><p><code>下文会以TextFile、SequenceFile、OrcFile、ParquetFile，四种常见的Hive表文件格式来做测试</code></p><blockquote><p>Hive数据导入途径：</p></blockquote><p><strong><em>1. Load file to hive table</em></strong></p><p><strong><em>2. Add partition and mv file to partition path</em></strong></p><p><strong><em>3. Insert into</em></strong></p><blockquote><p>测试数据文件</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 数据文件 count = 20</span></span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br><span class="line"><span class="number">1</span>wxmimperio1</span><br><span class="line"><span class="number">2</span>wxmimperio2</span><br></pre></td></tr></table></figure><h3 id="TextFile"><a href="#TextFile" class="headerlink" title="TextFile"></a>TextFile</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create table </span></span><br><span class="line">CREATE TABLE `analyze_text` (`id` string, `name` string) COMMENT <span class="string">'test'</span> <span class="function">PARTITIONED <span class="title">BY</span> <span class="params">(`part_date` string)</span> </span></span><br><span class="line"><span class="function">ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH <span class="title">SERDEPROPERTIES</span> <span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">'field.delim'</span> = <span class="string">'\t'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">'serialization.format'</span> = <span class="string">'\t'</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span> STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span>;</span><br></pre></td></tr></table></figure><ul><li><ol><li>Load file to table</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; LOAD DATA LOCAL INPATH <span class="string">'/home/hadoop/wxm/test.txt'</span> <span class="function">OVERWRITE INTO TABLE analyze_text <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">Loading data to table dw.<span class="function">analyze_text <span class="title">partition</span> <span class="params">(part_date=<span class="number">2019</span><span class="number">-02</span><span class="number">-21</span>)</span></span></span><br><span class="line"><span class="function">Partition dw.analyze_text</span>&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">0</span>, totalSize=<span class="number">298</span>, rawDataSize=<span class="number">0</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">1.035</span> seconds</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_text <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_text&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">0</span>, totalSize=<span class="number">298</span>, rawDataSize=<span class="number">0</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.463</span> seconds</span><br></pre></td></tr></table></figure><p><code>Load 操作实际上是执行了 mv 操作，将文件移动到表目录下面；ANALYZE 只能查看到numFiles（文件数）和totalSize（分区总大小）</code></p><ul><li><ol><li>Add partition and mv file to partition path</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add partition</span></span><br><span class="line"><span class="function">alter table analyze_text add <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-20'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// cp files</span></span><br><span class="line">hadoop fs -cp hdfs:<span class="comment">//sdg/user/hive/warehouse/dw.db/analyze_text/part_date=2019-02-21/* hdfs://sdg/user/hive/warehouse/dw.db/analyze_text/part_date=2019-02-20/</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_text <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-20'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_text&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">20</span>&#125; stats: [numFiles=<span class="number">1</span>, totalSize=<span class="number">298</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.202</span> seconds</span><br></pre></td></tr></table></figure><p><code>和Load操作一样，ANALYZE 只能查看到numFiles（文件数）和totalSize（分区总大小）</code></p><ul><li><ol><li>Insert into</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">insert into table analyze_text <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-19'</span>)</span> select `id`,`name` from analyze_text_copy where part_date</span>=<span class="string">'2019-02-21'</span>;</span><br><span class="line"></span><br><span class="line">Query ID = hadoop_20190221143232_fbc19f00-d0af-<span class="number">4278</span>-a644-<span class="number">924</span>c92994a75</span><br><span class="line">Total jobs = <span class="number">3</span></span><br><span class="line">Launching Job <span class="number">1</span> out of <span class="number">3</span></span><br><span class="line">Number of reduce tasks is set to <span class="number">0</span> since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">Starting Job = job_1535945194143_1278, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1278/</span></span><br><span class="line"><span class="string">Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job  -kill job_1535945194143_1278</span></span><br><span class="line"><span class="string">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">2019-02-21 14:32:54,087 Stage-1 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">2019-02-21 14:33:01,381 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.3 sec</span></span><br><span class="line"><span class="string">MapReduce Total cumulative CPU time: 3 seconds 300 msec</span></span><br><span class="line"><span class="string">Ended Job = job_1535945194143_1278</span></span><br><span class="line"><span class="string">Stage-4 is selected by condition resolver.</span></span><br><span class="line"><span class="string">Stage-3 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Stage-5 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Moving data to: hdfs://sdg/user/hive/warehouse/dw.db/analyze_text/part_date=2019-02-19/.hive-staging_hive_2019-02-21_14-32-46_291_8677282162206970479-1/-ext-10000</span></span><br><span class="line"><span class="string">Loading data to table dw.analyze_text partition (part_date=2019-02-19)</span></span><br><span class="line"><span class="string">Partition dw.analyze_text&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=280, rawDataSize=260]</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-1: Map: 1   Cumulative CPU: 3.3 sec   HDFS Read: 3699 HDFS Write: 373 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 3 seconds 300 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">Time taken: 20.567 seconds</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_text <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-19'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_text&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">19</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">20</span>, totalSize=<span class="number">280</span>, rawDataSize=<span class="number">260</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.197</span> seconds</span><br></pre></td></tr></table></figure><p><code>由于insert into 用了 MapReduce，在计算的过程中就已经将表的统计信息记录了下来，所以numRows、rawDataSize都有</code></p><h3 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create table </span></span><br><span class="line">CREATE TABLE `analyze_sequence_test` (`id` string, `name` string) <span class="function">PARTITIONED <span class="title">BY</span> <span class="params">(`part_date` string)</span> </span></span><br><span class="line"><span class="function">ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH <span class="title">SERDEPROPERTIES</span> <span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">'field.delim'</span> = <span class="string">'\t'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">'serialization.format'</span> = <span class="string">'\t'</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span> STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'</span>;</span><br></pre></td></tr></table></figure><ul><li><ol><li>Load file to table</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; LOAD DATA LOCAL INPATH <span class="string">'/home/hadoop/wxm/analyze_sequence_test_file'</span> <span class="function">OVERWRITE INTO TABLE analyze_sequence_test <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">Loading data to table dw.<span class="function">analyze_sequence_test <span class="title">partition</span> <span class="params">(part_date=<span class="number">2019</span><span class="number">-02</span><span class="number">-21</span>)</span></span></span><br><span class="line"><span class="function">Partition dw.analyze_sequence_test</span>&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">0</span>, totalSize=<span class="number">607</span>, rawDataSize=<span class="number">0</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">1.525</span> seconds</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_sequence_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_sequence_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">0</span>, totalSize=<span class="number">607</span>, rawDataSize=<span class="number">0</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.547</span> seconds</span><br></pre></td></tr></table></figure><p><code>stats: [numFiles=1, numRows=0, totalSize=607, rawDataSize=0]</code></p><ul><li><ol><li>Add partition and mv file to partition path</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add partition</span></span><br><span class="line"><span class="function">alter table analyze_sequence_test add <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-20'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// cp files</span></span><br><span class="line">hadoop fs -cp hdfs:<span class="comment">//sdg/user/hive/warehouse/dw.db/analyze_sequence_test/part_date=2019-02-19/* hdfs://sdg/user/hive/warehouse/dw.db/analyze_sequence_test/part_date=2019-02-20/</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ANALYZE TABLE analyze_sequence_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-20'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_sequence_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">20</span>&#125; stats: [numFiles=<span class="number">1</span>, totalSize=<span class="number">607</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.875</span> seconds</span><br></pre></td></tr></table></figure><p><code>由于sequenceFile文件的head中没有存储row的相关信息，所以获取不到</code></p><ul><li><ol><li>Insert into</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  <span class="function">insert into table analyze_sequence_test <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-19'</span>)</span> select `id`,`name` from analyze_text_copy where part_date</span>=<span class="string">'2019-02-21'</span>;</span><br><span class="line"></span><br><span class="line">Query ID = hadoop_20190221144141_f9ffbdc3-<span class="number">9e68</span>-<span class="number">43</span>d4-af7c-b8f883e84d3a</span><br><span class="line">Total jobs = <span class="number">3</span></span><br><span class="line">Launching Job <span class="number">1</span> out of <span class="number">3</span></span><br><span class="line">Number of reduce tasks is set to <span class="number">0</span> since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">Starting Job = job_1535945194143_1279, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1279/</span></span><br><span class="line"><span class="string">Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job  -kill job_1535945194143_1279</span></span><br><span class="line"><span class="string">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">2019-02-21 14:41:12,146 Stage-1 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">2019-02-21 14:41:18,387 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.94 sec</span></span><br><span class="line"><span class="string">MapReduce Total cumulative CPU time: 2 seconds 940 msec</span></span><br><span class="line"><span class="string">Ended Job = job_1535945194143_1279</span></span><br><span class="line"><span class="string">Stage-4 is selected by condition resolver.</span></span><br><span class="line"><span class="string">Stage-3 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Stage-5 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Moving data to: hdfs://sdg/user/hive/warehouse/dw.db/analyze_sequence_test/part_date=2019-02-19/.hive-staging_hive_2019-02-21_14-41-04_329_3570329183914133571-1/-ext-10000</span></span><br><span class="line"><span class="string">Loading data to table dw.analyze_sequence_test partition (part_date=2019-02-19)</span></span><br><span class="line"><span class="string">Partition dw.analyze_sequence_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=607, rawDataSize=260]</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-1: Map: 1   Cumulative CPU: 2.94 sec   HDFS Read: 3954 HDFS Write: 709 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 2 seconds 940 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">Time taken: 20.673 seconds</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_sequence_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-19'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_sequence_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">19</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">20</span>, totalSize=<span class="number">607</span>, rawDataSize=<span class="number">260</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.191</span> seconds</span><br></pre></td></tr></table></figure><p><code>stats: [numFiles=1, numRows=20, totalSize=607, rawDataSize=260]</code></p><h3 id="OrcFile"><a href="#OrcFile" class="headerlink" title="OrcFile"></a>OrcFile</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create table</span></span><br><span class="line">CREATE TABLE `analyze_orc_test` (`id` string, `name` string) <span class="function">PARTITIONED <span class="title">BY</span> <span class="params">(`part_date` string)</span> </span></span><br><span class="line"><span class="function">ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' </span></span><br><span class="line"><span class="function">STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'</span>;</span><br></pre></td></tr></table></figure><ul><li><ol><li>Load file to table</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; LOAD DATA LOCAL INPATH <span class="string">'/home/hadoop/wxm/analyze_orc_test_file'</span> <span class="function">OVERWRITE INTO TABLE analyze_orc_test <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">Loading data to table dw.<span class="function">analyze_orc_test <span class="title">partition</span> <span class="params">(part_date=<span class="number">2019</span><span class="number">-02</span><span class="number">-21</span>)</span></span></span><br><span class="line"><span class="function">Partition dw.analyze_orc_test</span>&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">0</span>, totalSize=<span class="number">365</span>, rawDataSize=<span class="number">0</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.991</span> seconds</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_orc_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_orc_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">20</span>, totalSize=<span class="number">365</span>, rawDataSize=<span class="number">3600</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.661</span> seconds</span><br></pre></td></tr></table></figure><p><code>stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600]</code></p><ul><li><ol><li>Add partition and mv file to partition path</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add partition</span></span><br><span class="line"><span class="function">alter table analyze_orc_test add <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-20'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// cp files</span></span><br><span class="line">hadoop fs -cp hdfs:<span class="comment">//sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/* hdfs://sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-20/</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_orc_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-20'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_orc_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">20</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">20</span>, totalSize=<span class="number">365</span>, rawDataSize=<span class="number">3600</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.469</span> seconds</span><br></pre></td></tr></table></figure><p><code>stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600]</code></p><ul><li><ol><li>Insert into</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">insert into table analyze_orc_test <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-19'</span>)</span> select `id`,`name` from analyze_text_copy where part_date</span>=<span class="string">'2019-02-21'</span>;</span><br><span class="line"></span><br><span class="line">Query ID = hadoop_20190221145353_70e75ca0-<span class="number">210f</span>-<span class="number">48</span>d1-bbd0-b4b0ed01c0cf</span><br><span class="line">Total jobs = <span class="number">1</span></span><br><span class="line">Launching Job <span class="number">1</span> out of <span class="number">1</span></span><br><span class="line">Number of reduce tasks is set to <span class="number">0</span> since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">Starting Job = job_1535945194143_1281, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1281/</span></span><br><span class="line"><span class="string">Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job  -kill job_1535945194143_1281</span></span><br><span class="line"><span class="string">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">2019-02-21 14:53:53,124 Stage-1 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">2019-02-21 14:54:00,420 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.6 sec</span></span><br><span class="line"><span class="string">MapReduce Total cumulative CPU time: 3 seconds 600 msec</span></span><br><span class="line"><span class="string">Ended Job = job_1535945194143_1281</span></span><br><span class="line"><span class="string">Stage-4 is selected by condition resolver.</span></span><br><span class="line"><span class="string">Stage-3 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Stage-5 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Moving data to: hdfs://sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/.hive-staging_hive_2019-02-21_14-53-45_113_8780230061527813380-1/-ext-10000</span></span><br><span class="line"><span class="string">Loading data to table dw.analyze_orc_test partition (part_date=2019-02-19)</span></span><br><span class="line"><span class="string">Partition dw.analyze_orc_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600]</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-1: Map: 1   Cumulative CPU: 3.6 sec   HDFS Read: 3947 HDFS Write: 463 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 3 seconds 600 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">Time taken: 18.592 seconds</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_orc_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-19'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_orc_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">19</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">20</span>, totalSize=<span class="number">365</span>, rawDataSize=<span class="number">3600</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.274</span> seconds</span><br></pre></td></tr></table></figure><p><code>stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600]</code></p><h3 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create table</span></span><br><span class="line">CREATE TABLE `analyze_parquet_test` (`id` string, `name` string) <span class="function">PARTITIONED <span class="title">BY</span> <span class="params">(`part_date` string)</span> </span></span><br><span class="line"><span class="function">ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' </span></span><br><span class="line"><span class="function">STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span>;</span><br></pre></td></tr></table></figure><ul><li><ol><li>Load file to table</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  LOAD DATA LOCAL INPATH <span class="string">'/home/hadoop/wxm/analyze_parquet_test_file'</span> <span class="function">OVERWRITE INTO TABLE analyze_parquet_test <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">Loading data to table dw.<span class="function">analyze_parquet_test <span class="title">partition</span> <span class="params">(part_date=<span class="number">2019</span><span class="number">-02</span><span class="number">-21</span>)</span></span></span><br><span class="line"><span class="function">Partition dw.analyze_parquet_test</span>&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">0</span>, totalSize=<span class="number">390</span>, rawDataSize=<span class="number">0</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.972</span> seconds</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_parquet_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_parquet_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">0</span>, totalSize=<span class="number">390</span>, rawDataSize=<span class="number">0</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.576</span> seconds</span><br></pre></td></tr></table></figure><p><code>stats: [numFiles=1, numRows=0, totalSize=390, rawDataSize=0]</code></p><ul><li><ol><li>Add partition and mv file to partition path</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add partition</span></span><br><span class="line"><span class="function">alter table analyze_parquet_test add <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-20'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// cp files</span></span><br><span class="line">hadoop fs -cp hdfs:<span class="comment">//sdg/user/hive/warehouse/dw.db/analyze_parquet_test/part_date=2019-02-19/* hdfs://sdg/user/hive/warehouse/dw.db/analyze_parquet_test/part_date=2019-02-20/</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_parquet_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-20'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_parquet_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">20</span>&#125; stats: [numFiles=<span class="number">1</span>, totalSize=<span class="number">390</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.799</span> seconds</span><br></pre></td></tr></table></figure><p><code>stats: [numFiles=1, totalSize=390]</code></p><ul><li><ol><li>Insert into</li></ol></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">insert into table analyze_parquet_test <span class="title">partition</span><span class="params">(part_date=<span class="string">'2019-02-19'</span>)</span> select `id`,`name` from analyze_text_copy where part_date</span>=<span class="string">'2019-02-21'</span>;</span><br><span class="line"></span><br><span class="line">Query ID = hadoop_20190221150303_86874b16-b4c6-<span class="number">4e6</span>d-a2e6-c1a02aaae3b8</span><br><span class="line">Total jobs = <span class="number">3</span></span><br><span class="line">Launching Job <span class="number">1</span> out of <span class="number">3</span></span><br><span class="line">Number of reduce tasks is set to <span class="number">0</span> since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">Starting Job = job_1535945194143_1282, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1282/</span></span><br><span class="line"><span class="string">Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job  -kill job_1535945194143_1282</span></span><br><span class="line"><span class="string">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">2019-02-21 15:03:27,069 Stage-1 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">2019-02-21 15:03:35,527 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.98 sec</span></span><br><span class="line"><span class="string">MapReduce Total cumulative CPU time: 4 seconds 980 msec</span></span><br><span class="line"><span class="string">Ended Job = job_1535945194143_1282</span></span><br><span class="line"><span class="string">Stage-4 is selected by condition resolver.</span></span><br><span class="line"><span class="string">Stage-3 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Stage-5 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">Moving data to: hdfs://sdg/user/hive/warehouse/dw.db/analyze_parquet_test/part_date=2019-02-19/.hive-staging_hive_2019-02-21_15-03-17_975_7489031492557604858-1/-ext-10000</span></span><br><span class="line"><span class="string">Loading data to table dw.analyze_parquet_test partition (part_date=2019-02-19)</span></span><br><span class="line"><span class="string">Partition dw.analyze_parquet_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=390, rawDataSize=40]</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-1: Map: 1   Cumulative CPU: 4.98 sec   HDFS Read: 4039 HDFS Write: 490 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 4 seconds 980 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">Time taken: 22.794 seconds</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_parquet_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-19'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_parquet_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">19</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">20</span>, totalSize=<span class="number">390</span>, rawDataSize=<span class="number">40</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.226</span> seconds</span><br></pre></td></tr></table></figure><p><code>stats: [numFiles=1, numRows=20, totalSize=390, rawDataSize=40]</code></p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>通过上述测试可以看出,OrcFile文件的三种数据导入方式可以直接通过ANALYZE获取完整信息，其他文件格式默认只能获取到Size，除非<code>insert into</code>触发了<code>MapReduce</code>时才能获取RowCount</p><p>ANALYZE耗时非常短，文件的统计数据一定是存储在某个地方不需要RunTime去计算<code>（查询了Hive MetaStore，并没有发现表Row的统计存储）</code>，猜想原因在于OrcFile的自描述Header里存储了对Row的统计信息，ANALYZE会直接获取自描述文件中的统计信息，可以看下文件的Dump</p><h2 id="Hive-Orc-File-Dump"><a href="#Hive-Orc-File-Dump" class="headerlink" title="Hive Orc File Dump"></a>Hive Orc File Dump</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC" target="_blank" rel="noopener">Hive Orc Doc</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Orc Dump</span></span><br><span class="line"></span><br><span class="line">hive --orcfiledump hdfs:<span class="comment">//sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/000000_0</span></span><br><span class="line"></span><br><span class="line"><span class="function">Java <span class="title">HotSpot</span><span class="params">(TM)</span> 64-Bit Server VM warning: ignoring option MaxPermSize</span>=<span class="number">512</span>M; support was removed in <span class="number">8.0</span></span><br><span class="line"><span class="function">Java <span class="title">HotSpot</span><span class="params">(TM)</span> 64-Bit Server VM warning: ignoring option MaxPermSize</span>=<span class="number">512</span>M; support was removed in <span class="number">8.0</span></span><br><span class="line">Structure <span class="keyword">for</span> hdfs:<span class="comment">//sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/000000_0</span></span><br><span class="line">File Version: <span class="number">0.12</span> with HIVE_8732</span><br><span class="line"><span class="number">19</span>/<span class="number">02</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">54</span>:<span class="number">02</span> INFO orc.ReaderImpl: Reading ORC rows from hdfs:<span class="comment">//sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/000000_0 with &#123;include: null, offset: 0, length: 9223372036854775807&#125;</span></span><br><span class="line">Rows: <span class="number">20</span></span><br><span class="line">Compression: ZLIB</span><br><span class="line">Compression size: <span class="number">262144</span></span><br><span class="line">Type: struct&lt;_col0:string,_col1:string&gt;</span><br><span class="line"></span><br><span class="line">Stripe Statistics:</span><br><span class="line">  Stripe <span class="number">1</span>:</span><br><span class="line">    Column <span class="number">0</span>: count: <span class="number">20</span> hasNull: <span class="keyword">false</span></span><br><span class="line">    Column <span class="number">1</span>: count: <span class="number">20</span> hasNull: <span class="keyword">false</span> min: <span class="number">1</span> max: <span class="number">2</span> sum: <span class="number">20</span></span><br><span class="line">    Column <span class="number">2</span>: count: <span class="number">20</span> hasNull: <span class="keyword">false</span> min: wxmimperio1 max: wxmimperio2 sum: <span class="number">220</span></span><br><span class="line"></span><br><span class="line">File Statistics:</span><br><span class="line">  Column <span class="number">0</span>: count: <span class="number">20</span> hasNull: <span class="keyword">false</span></span><br><span class="line">  Column <span class="number">1</span>: count: <span class="number">20</span> hasNull: <span class="keyword">false</span> min: <span class="number">1</span> max: <span class="number">2</span> sum: <span class="number">20</span></span><br><span class="line">  Column <span class="number">2</span>: count: <span class="number">20</span> hasNull: <span class="keyword">false</span> min: wxmimperio1 max: wxmimperio2 sum: <span class="number">220</span></span><br><span class="line"></span><br><span class="line">Stripes:</span><br><span class="line">  Stripe: offset: <span class="number">3</span> data: <span class="number">52</span> rows: <span class="number">20</span> tail: <span class="number">57</span> index: <span class="number">77</span></span><br><span class="line">    Stream: column <span class="number">0</span> section ROW_INDEX start: <span class="number">3</span> length <span class="number">11</span></span><br><span class="line">    Stream: column <span class="number">1</span> section ROW_INDEX start: <span class="number">14</span> length <span class="number">26</span></span><br><span class="line">    Stream: column <span class="number">2</span> section ROW_INDEX start: <span class="number">40</span> length <span class="number">40</span></span><br><span class="line">    Stream: column <span class="number">1</span> section DATA start: <span class="number">80</span> length <span class="number">8</span></span><br><span class="line">    Stream: column <span class="number">1</span> section LENGTH start: <span class="number">88</span> length <span class="number">6</span></span><br><span class="line">    Stream: column <span class="number">1</span> section DICTIONARY_DATA start: <span class="number">94</span> length <span class="number">5</span></span><br><span class="line">    Stream: column <span class="number">2</span> section DATA start: <span class="number">99</span> length <span class="number">8</span></span><br><span class="line">    Stream: column <span class="number">2</span> section LENGTH start: <span class="number">107</span> length <span class="number">6</span></span><br><span class="line">    Stream: column <span class="number">2</span> section DICTIONARY_DATA start: <span class="number">113</span> length <span class="number">19</span></span><br><span class="line">    Encoding column <span class="number">0</span>: DIRECT</span><br><span class="line">    Encoding column <span class="number">1</span>: DICTIONARY_V2[<span class="number">2</span>]</span><br><span class="line">    Encoding column <span class="number">2</span>: DICTIONARY_V2[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">File length: <span class="number">365</span> bytes</span><br><span class="line">Padding length: <span class="number">0</span> bytes</span><br><span class="line">Padding ratio: <span class="number">0</span>%</span><br></pre></td></tr></table></figure><p><code>可以看到Dump信息里面有Rows: 20的信息，所以可以确定ANALYZE 命令是分析了file的meta</code></p><h2 id="Parquet-File-Dump"><a href="#Parquet-File-Dump" class="headerlink" title="Parquet File Dump"></a>Parquet File Dump</h2><p><a href="https://github.com/apache/parquet-mr/tree/master/parquet-tools" target="_blank" rel="noopener">Parquet Tools</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar parquet-tools-<span class="number">1.5</span>.0.jar dump hdfs:<span class="comment">//sdg/user/hive/warehouse/dw.db/analyze_parquet_test/part_date=2019-02-19/000000_0</span></span><br><span class="line"></span><br><span class="line">row group <span class="number">0</span> </span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">id:    BINARY UNCOMPRESSED DO:<span class="number">0</span> FPO:<span class="number">4</span> SZ:<span class="number">61</span>/<span class="number">61</span>/<span class="number">1.00</span> VC:<span class="number">20</span> ENC:BIT_PACK [more]...</span><br><span class="line">name:  BINARY UNCOMPRESSED DO:<span class="number">0</span> FPO:<span class="number">65</span> SZ:<span class="number">101</span>/<span class="number">101</span>/<span class="number">1.00</span> VC:<span class="number">20</span> ENC:BIT_P [more]...</span><br><span class="line"></span><br><span class="line">    id TV=<span class="number">20</span> RL=<span class="number">0</span> DL=<span class="number">1</span> DS:   <span class="number">2</span> DE:PLAIN_DICTIONARY</span><br><span class="line">    ----------------------------------------------------------------------------</span><br><span class="line">    page <span class="number">0</span>:                   DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:<span class="number">11</span> [more]...</span><br><span class="line"></span><br><span class="line">    name TV=<span class="number">20</span> RL=<span class="number">0</span> DL=<span class="number">1</span> DS: <span class="number">2</span> DE:PLAIN_DICTIONARY</span><br><span class="line">    ----------------------------------------------------------------------------</span><br><span class="line">    page <span class="number">0</span>:                   DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:<span class="number">11</span> [more]...</span><br><span class="line"></span><br><span class="line">BINARY id </span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">*** row group <span class="number">1</span> of <span class="number">1</span>, values <span class="number">1</span> to <span class="number">20</span> *** </span><br><span class="line">value <span class="number">1</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">2</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">3</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">4</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">5</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">6</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">7</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">8</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">9</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">10</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">11</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">12</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">13</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">14</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">15</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">16</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">17</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">18</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line">value <span class="number">19</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">1</span></span><br><span class="line">value <span class="number">20</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:<span class="number">2</span></span><br><span class="line"></span><br><span class="line">BINARY name </span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">*** row group <span class="number">1</span> of <span class="number">1</span>, values <span class="number">1</span> to <span class="number">20</span> *** </span><br><span class="line">value <span class="number">1</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">2</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">3</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">4</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">5</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">6</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">7</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">8</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">9</span>:  R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">10</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">11</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">12</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">13</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">14</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">15</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">16</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">17</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">18</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br><span class="line">value <span class="number">19</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio1</span><br><span class="line">value <span class="number">20</span>: R:<span class="number">0</span> D:<span class="number">1</span> V:wxmimperio2</span><br></pre></td></tr></table></figure><p><code>虽然看到Dump文件中有TV=20，但是不像ORC File 有完整的Row信息，meta还是基于列的，所以读取不到</code></p><h2 id="MapReduce-统计"><a href="#MapReduce-统计" class="headerlink" title="MapReduce 统计"></a>MapReduce 统计</h2><blockquote><p>我们想要获取完整的表统计信息，可以将<code>ANALYZE</code>命令的<code>noscan</code>去掉执行，则会触发一个MapReduce，这个MR会对表文件做一个统计，并将结果存储到Hive MetaStore中，后续在用<code>ANALYZE</code>分析就会直接得到结果</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_parquet_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span> COMPUTE STATISTICS</span>;</span><br><span class="line"></span><br><span class="line">Query ID = hadoop_20190221173030_d9cc4e9c-<span class="number">73f</span>6-<span class="number">403</span>e-bfbc-c3cf381b373d</span><br><span class="line">Total jobs = <span class="number">1</span></span><br><span class="line">Launching Job <span class="number">1</span> out of <span class="number">1</span></span><br><span class="line">Number of reduce tasks is set to <span class="number">0</span> since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">Starting Job = job_1535945194143_1284, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1284/</span></span><br><span class="line"><span class="string">Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job  -kill job_1535945194143_1284</span></span><br><span class="line"><span class="string">Hadoop job information for Stage-0: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">2019-02-21 17:30:54,865 Stage-0 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">2019-02-21 17:31:02,248 Stage-0 map = 100%,  reduce = 0%, Cumulative CPU 4.21 sec</span></span><br><span class="line"><span class="string">MapReduce Total cumulative CPU time: 4 seconds 210 msec</span></span><br><span class="line"><span class="string">Ended Job = job_1535945194143_1284</span></span><br><span class="line"><span class="string">Partition dw.analyze_parquet_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=20, totalSize=390, rawDataSize=60]</span></span><br><span class="line"><span class="string">MapReduce Jobs Launched: </span></span><br><span class="line"><span class="string">Stage-Stage-0: Map: 1   Cumulative CPU: 4.21 sec   HDFS Read: 2931 HDFS Write: 100 SUCCESS</span></span><br><span class="line"><span class="string">Total MapReduce CPU Time Spent: 4 seconds 210 msec</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">Time taken: 21.214 seconds</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="function">ANALYZE TABLE analyze_parquet_test <span class="title">PARTITION</span><span class="params">(part_date=<span class="string">'2019-02-21'</span>)</span> COMPUTE STATISTICS noscan</span>;</span><br><span class="line"></span><br><span class="line">Partition dw.analyze_parquet_test&#123;part_date=<span class="number">2019</span>-<span class="number">02</span>-<span class="number">21</span>&#125; stats: [numFiles=<span class="number">1</span>, numRows=<span class="number">20</span>, totalSize=<span class="number">390</span>, rawDataSize=<span class="number">60</span>]</span><br><span class="line">OK</span><br><span class="line">Time taken: <span class="number">0.219</span> seconds</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们可以看到利用<code>ANALYZE</code>命令可以快速帮助我们分析一张表的基本统计信息，但也有缺点：<code>如果是分区表，写分区则会列出这个分区的统计，不写分区会列出所有分区的数据列表；并没有一个队整张表进行的汇总统计</code>，所以通常需要配合最开始提到的两种常用方法</p><p>对于OrcFile则直接可以用<code>ANALYZE</code>命令分析，如果是TextFile、SequenceFile、ParquetFile，则需要确保Hive 表的数据加载方式是通过MapReduce，其他方式Load数据的则需要执行<code>noscan 的ANALYZE（可以准备离线任务，每天对非MR Load的数据表执行一次）；也可以在执行ETL的时候，将统计作为一个回调自动完成</code></p><ul><li>Size</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> size = FileSystem.getContentSummary(<span class="keyword">new</span> Path(tablePath)).getLength();</span><br></pre></td></tr></table></figure><p><code>此方法可以获取一个表路径、分区路径以及具体文件的Size大小；Size是压缩后的值</code></p><p><code>全表的统计可以将tablePath截止到tableName就可以，获取某个分区的统计，则tablePath精确到分区目录</code></p><ul><li>RowCount</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ANALYZE TABLE tablName [PARTITION(part)] COMPUTE STATISTICS noscan;</span><br></pre></td></tr></table></figure><p><code>此方法会列出所有表的分区统计，解析这些统计并将numRows加总就是全表的RowCount；如果是非分区表或只想获取当前分区的统计，则只解析一条记录</code></p><hr><blockquote><p>转载请注明出处：<a href="https://github.com/imperio-wxm" target="_blank" rel="noopener">https://github.com/imperio-wxm</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;&lt;p&gt;在Hadoop平台运维监控中，Hive表的统计信息通常是作为集群数据质量的关键依据；通常以Hive表的大小、行数、分区数等信息来衡量集群数据量的增减趋势；本文以Hive表Size大小和数据行数作为重点，对于超大表统计时避免过多的资源消耗 &lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/BigData/"/>
    
      <category term="Hive" scheme="http://imperio-wxm.github.io/categories/BigData/Hive/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hive" scheme="http://imperio-wxm.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot-Java8Date</title>
    <link href="http://imperio-wxm.github.io/2019/02/19/SpringBoot-Java8Date/"/>
    <id>http://imperio-wxm.github.io/2019/02/19/SpringBoot-Java8Date/</id>
    <published>2019-02-19T12:52:12.000Z</published>
    <updated>2019-02-19T16:09:37.358Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在SpringBoot和Feign使用Java8的Date类型，如LocalDate、LocalDateTime，会出现Format不符合常规的情况</p></blockquote><a id="more"></a><table><thead><tr><th>Software</th><th>Version</th></tr></thead><tbody><tr><td>SpringBoot</td><td>1.5.13.RELEASE</td></tr><tr><td>Java</td><td>1.8.0_201</td></tr></tbody></table><h1 id="默认时间格式显示"><a href="#默认时间格式显示" class="headerlink" title="默认时间格式显示"></a>默认时间格式显示</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DefaultTimeBean</span></span><br><span class="line"><span class="keyword">private</span> LocalDate localDate;</span><br><span class="line"><span class="keyword">private</span> LocalDateTime localDateTime;</span><br><span class="line"><span class="keyword">private</span> LocalTime localTime;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">DefaultTimeBean</span><span class="params">(LocalDate localDate, LocalDateTime localDateTime, LocalTime localTime)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.localDate = localDate;</span><br><span class="line">    <span class="keyword">this</span>.localDateTime = localDateTime;</span><br><span class="line">    <span class="keyword">this</span>.localTime = localTime;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// getter and setter</span></span><br></pre></td></tr></table></figure><ul><li>通过Rest获取</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@GetMapping</span>(<span class="string">"/getDefaultJava8Time"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> DefaultTimeBean <span class="title">getDefaultJava8Time</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DefaultTimeBean(LocalDate.now(), LocalDateTime.now(), LocalTime.now());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>结果</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 全部是以数组的形式返回，这不符合我们的开发规范，要在序列化/反序列化的时候就进行formart</span></span><br><span class="line">&#123;</span><br><span class="line">localDate: [<span class="number">2019</span>, <span class="number">2</span>, <span class="number">19</span>],</span><br><span class="line">localDateTime: [<span class="number">2019</span>, <span class="number">2</span>, <span class="number">19</span>, <span class="number">15</span>, <span class="number">49</span>, <span class="number">10</span>, <span class="number">964000000</span>],</span><br><span class="line">localTime: [<span class="number">15</span>, <span class="number">49</span>, <span class="number">10</span>, <span class="number">964000000</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="格式化时间显示"><a href="#格式化时间显示" class="headerlink" title="格式化时间显示"></a>格式化时间显示</h1><ul><li>Maven</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 不需要写version</span></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jackson-datatype-jsr310&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><ul><li>application.yml</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spring:</span><br><span class="line">  jackson:</span><br><span class="line">    serialization:</span><br><span class="line">      WRITE_DATES_AS_TIMESTAMPS: <span class="keyword">false</span></span><br></pre></td></tr></table></figure><ul><li>注解配置format规则</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TimeBean</span></span><br><span class="line"><span class="meta">@DateTimeFormat</span>(iso = DateTimeFormat.ISO.DATE)</span><br><span class="line"><span class="keyword">private</span> LocalDate localDate;</span><br><span class="line"></span><br><span class="line"><span class="meta">@DateTimeFormat</span>(pattern = <span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line"><span class="meta">@JsonFormat</span>(pattern = <span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line"><span class="keyword">private</span> LocalDateTime localDateTime;</span><br><span class="line"></span><br><span class="line"><span class="meta">@JsonFormat</span>(pattern = <span class="string">"HH:mm:ss"</span>)</span><br><span class="line"><span class="meta">@DateTimeFormat</span>(iso = DateTimeFormat.ISO.TIME)</span><br><span class="line"><span class="keyword">private</span> LocalTime localTime;</span><br></pre></td></tr></table></figure><ul><li>通过Rest获取</li></ul><blockquote><p>Get</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@GetMapping</span>(<span class="string">"/getFormatJava8Time"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> TimeBean <span class="title">getFormatJava8Time</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> TimeBean(LocalDate.now(), LocalDateTime.now(), LocalTime.now());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>结果</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    localDate: <span class="string">"2019-02-19"</span>,</span><br><span class="line">    localDateTime: <span class="string">"2019-02-19 15:57:08"</span>,</span><br><span class="line">    localTime: <span class="string">"15:57:08"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Post</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@PostMapping</span>(<span class="string">"/formatJava8Time"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> TimeBean <span class="title">postDateTime</span><span class="params">(@RequestBody TimeBean timeBean)</span> </span>&#123;</span><br><span class="line">System.out.println(timeBean.getLocalDate());</span><br><span class="line">System.out.println(timeBean.getLocalDateTime());</span><br><span class="line">System.out.println(timeBean.getLocalTime());</span><br><span class="line"><span class="keyword">return</span> timeBean;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>结果</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"localDate"</span>: <span class="string">"2019-05-02"</span>,</span><br><span class="line">    <span class="string">"localDateTime"</span>: <span class="string">"2019-06-15 12:23:59"</span>,</span><br><span class="line">    <span class="string">"localTime"</span>: <span class="string">"15:12:23"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在SpringBoot和Feign使用Java8的Date类型，如LocalDate、LocalDateTime，会出现Format不符合常规的情况&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="SpringBoot" scheme="http://imperio-wxm.github.io/categories/SpringBoot/"/>
    
      <category term="Java" scheme="http://imperio-wxm.github.io/categories/SpringBoot/Java/"/>
    
    
      <category term="Java" scheme="http://imperio-wxm.github.io/tags/Java/"/>
    
      <category term="SpringBoot" scheme="http://imperio-wxm.github.io/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>Cassandra集群运维[Add &amp; Remove Nodes]</title>
    <link href="http://imperio-wxm.github.io/2018/10/20/Cassandra-Cluster-Operation/"/>
    <id>http://imperio-wxm.github.io/2018/10/20/Cassandra-Cluster-Operation/</id>
    <published>2018-10-20T14:47:36.000Z</published>
    <updated>2018-10-20T15:09:15.940Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在对Cassandra进行维护的时候，通常需要扩集群或者迁移数据，涉及到添加、移除节点。</p></blockquote><a id="more"></a><blockquote><p>Cassandra Version: Apache Cassandra 3.0.6</p></blockquote><h1 id="Add-Nodes"><a href="#Add-Nodes" class="headerlink" title="Add Nodes"></a>Add Nodes</h1><p>Virtual nodes (vnodes) greatly simplify adding nodes to an existing cluster:<br>Calculating tokens and assigning them to each node is no longer required.<br>Rebalancing a cluster is no longer necessary because a node joining the cluster assumes responsibility for an even portion of the data.</p><blockquote><p style="color:red">确保新加节点和现有集群的Cassandra 版本一致</p></blockquote><hr><h2 id="【操作步骤】"><a href="#【操作步骤】" class="headerlink" title="【操作步骤】"></a>【操作步骤】</h2><h3 id="在新的机器上部署cassandra，但不要启动"><a href="#在新的机器上部署cassandra，但不要启动" class="headerlink" title="在新的机器上部署cassandra，但不要启动"></a>在新的机器上部署cassandra，但不要启动</h3><p>通常都是从现有集群的一台机器上scp cassandra目录到新机器</p><h3 id="基于现有集群所用的snitch算法修改配置文件"><a href="#基于现有集群所用的snitch算法修改配置文件" class="headerlink" title="基于现有集群所用的snitch算法修改配置文件"></a>基于现有集群所用的<code>snitch</code>算法修改配置文件</h3><blockquote><p style="color:blue">cassandra-topology.properties or the cassandra-rackdc.properties</p></blockquote><ul><li>使用<code>PropertyFileSnitch</code>算法配置：cassandra-topology.properties</li><li>使用<code>GossipingPropertyFileSnitch</code>, <code>Ec2Snitch</code>, <code>Ec2MultiRegionSnitch</code>, and <code>GoogleCloudSnitch</code>算法配置：cassandra-rackdc.properties</li></ul><blockquote><p>ps: 这两个配置与机架和多数据中心有关，如果是同机架单数据中心则不用配置</p></blockquote><h3 id="修改配置cassandra-yaml文件"><a href="#修改配置cassandra-yaml文件" class="headerlink" title="修改配置cassandra.yaml文件"></a>修改配置<code>cassandra.yaml</code>文件</h3><table><thead><tr><th style="text-align:left">name</th><th>desc</th></tr></thead><tbody><tr><td style="text-align:left">auto_bootstrap</td><td>默认文件中是没有这个参数的，如果没有默认为true；如果有且为false修改为true</td></tr><tr><td style="text-align:left">cluster_name</td><td>需要加入的集群名称</td></tr><tr><td style="text-align:left">listen_address/broadcast_address</td><td>用来与集群内其他节点通信的ip，通常为本机真实ip，不要填写127.0.0.1或localhost</td></tr><tr><td style="text-align:left">endpoint_snitch</td><td>用于定位节点和路由请求的算法，与现有集群保持一致</td></tr><tr><td style="text-align:left">num_tokens</td><td>节点中vnodes的数量，与现有集群配置保持一致，如果当前机器配置更高可以按比例增加这个值，可以有更好的性能</td></tr><tr><td style="text-align:left">seed_provider</td><td>种子节点，至少保证有一个现有集群的节点，-seeds列表表示了新节点与现有集群通过哪些节点通信（种子节点无法引导，所以不要仅仅把要加入的新节点配置进去，也不要将集群所有节点配置成种子节点）</td></tr></tbody></table><h3 id="启动新节点Cassandra"><a href="#启动新节点Cassandra" class="headerlink" title="启动新节点Cassandra"></a>启动新节点Cassandra</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/cassandra</span><br></pre></td></tr></table></figure><ul><li>初始化system相关信息</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">INFO  06:14:41 Initializing system.IndexInfo</span><br><span class="line">INFO  06:14:42 Initializing system.batches</span><br><span class="line">INFO  06:14:42 Initializing system.paxos</span><br><span class="line">INFO  06:14:42 Initializing system.local</span><br><span class="line">INFO  06:14:42 Initializing system.peers</span><br><span class="line">INFO  06:14:42 Initializing system.peer_events</span><br><span class="line">INFO  06:14:42 Initializing system.range_xfers</span><br><span class="line">INFO  06:14:42 Initializing system.compaction_history</span><br><span class="line">INFO  06:14:42 Initializing system.sstable_activity</span><br><span class="line">INFO  06:14:42 Initializing system.size_estimates</span><br><span class="line">INFO  06:14:42 Initializing system.available_ranges</span><br><span class="line">INFO  06:14:42 Initializing system.views_builds_in_progress</span><br><span class="line">INFO  06:14:42 Initializing system.built_views</span><br><span class="line">INFO  06:14:42 Initializing system.hints</span><br><span class="line">INFO  06:14:42 Initializing system.batchlog</span><br><span class="line">......</span><br></pre></td></tr></table></figure><ul><li>寻找现有集群节点</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">INFO  06:14:44 Node /xx.xxx.xx.xx is now part of the cluster</span><br><span class="line">INFO  06:14:44 Node /xx.xxx.xx.xx is now part of the cluster</span><br><span class="line">INFO  06:14:44 Node /xx.xxx.xx.xx is now part of the cluster</span><br><span class="line">INFO  06:14:44 Handshaking version with /xx.xxx.xx.xx</span><br><span class="line">INFO  06:14:44 Handshaking version with /xx.xxx.xx.xx</span><br><span class="line">INFO  06:14:44 InetAddress /xx.xxx.xx.xx is now UP</span><br><span class="line">INFO  06:14:44 InetAddress /xx.xxx.xx.xx is now UP</span><br><span class="line">INFO  06:14:44 InetAddress /xx.xxx.xx.xx is now UP</span><br></pre></td></tr></table></figure><ul><li>新节点加入集群</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO  06:14:45 JOINING: waiting for ring information</span><br><span class="line">INFO  06:14:45 Updating topology for all endpoints that have changed</span><br></pre></td></tr></table></figure><ul><li>同步schema</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">INFO  06:14:49 Initializing system_traces.events</span><br><span class="line">INFO  06:14:49 Initializing system_traces.sessions</span><br><span class="line">INFO  06:14:49 Initializing system_distributed.parent_repair_history</span><br><span class="line">INFO  06:14:49 Initializing system_distributed.repair_history</span><br><span class="line">INFO  06:14:49 Initializing system_auth.resource_role_permissons_index</span><br><span class="line">INFO  06:14:49 Initializing system_auth.role_members</span><br><span class="line">INFO  06:14:49 Initializing system_auth.role_permissions</span><br><span class="line">INFO  06:14:49 Initializing system_auth.roles</span><br><span class="line">INFO  06:14:49 JOINING: waiting for schema information to complete</span><br></pre></td></tr></table></figure><ul><li>Copy Schema数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Executing streaming plan for Bootstrap</span><br><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Starting streaming to /xx.xxx.xx.xx</span><br><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Starting streaming to /xx.xxx.xx.xx</span><br><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Starting streaming to /xx.xxx.xx.xx</span><br><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4, ID#0] Beginning stream session with /xx.xxx.xx.xx</span><br><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4, ID#0] Beginning stream session with /xx.xxx.xx.xx</span><br><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4, ID#0] Beginning stream session with /xx.xxx.xx.xx</span><br><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4 ID#0] Prepare completed. Receiving 48 files(358160851 bytes), sending 0 files(0 bytes)</span><br><span class="line">INFO  06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4 ID#0] Prepare completed. Receiving 35 files(132483825 bytes), sending 0 files(0 bytes)</span><br><span class="line">INFO  06:15:23 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4 ID#0] Prepare completed. Receiving 46 files(174538642 bytes), sending 0 files(0 bytes)</span><br><span class="line">INFO  06:16:54 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Session with /xx.xxx.xx.xx is complete</span><br><span class="line">INFO  06:17:38 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Session with /xx.xxx.xx.xx is complete</span><br><span class="line">INFO  06:20:28 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Session with /xx.xxx.xx.xx is complete</span><br><span class="line">INFO  06:20:28 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] All sessions completed</span><br></pre></td></tr></table></figure><ul><li>节点切换成NORMAL</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO  06:20:29 Node /xx.xxx.xx.xx state jump to NORMAL</span><br><span class="line">INFO  06:20:29 Waiting for gossip to settle before accepting client requests...</span><br></pre></td></tr></table></figure><h3 id="查看节点同步状态"><a href="#查看节点同步状态" class="headerlink" title="查看节点同步状态"></a>查看节点同步状态</h3><blockquote><p>./bin/nodetool status</p></blockquote><ul><li>数据同步期间节点的状态：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Datacenter: dc1</span><br><span class="line">===============</span><br><span class="line">Status=Up/Down</span><br><span class="line">|/ State=Normal/Leaving/Joining/Moving</span><br><span class="line">--  Address       Load       Tokens       Owns (effective)  Host ID                               Rack</span><br><span class="line">UN  xx.xxx.xx.xx  1.53 GB    256          100.0%            30ed942d-6827-469b-aab9-7fb649c6c3d7  rack1</span><br><span class="line">UN  xx.xxx.xx.xx  1.38 GB    256          100.0%            96736106-e95d-4c54-aabf-41666071bc59  rack1</span><br><span class="line">UN  xx.xxx.xx.xx  1.07 GB    256          100.0%            4351af17-2e68-4b46-a78f-fad900e44d13  rack1</span><br><span class="line">UJ  新加节点      57.87 MB   256          ?                 f3f590ac-9835-47bb-b4d8-6e17ea2916ac  rack1</span><br></pre></td></tr></table></figure><ul><li>数据同步结束后的状态：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Datacenter: dc1</span><br><span class="line">===============</span><br><span class="line">Status=Up/Down</span><br><span class="line">|/ State=Normal/Leaving/Joining/Moving</span><br><span class="line">--  Address       Load       Tokens       Owns (effective)  Host ID                               Rack</span><br><span class="line">UN  xx.xxx.xx.xx  1.53 GB    256          69.2%             30ed942d-6827-469b-aab9-7fb649c6c3d7  rack1</span><br><span class="line">UN  xx.xxx.xx.xx  1.38 GB    256          79.3%             96736106-e95d-4c54-aabf-41666071bc59  rack1</span><br><span class="line">UN  xx.xxx.xx.xx  1.07 GB    256          78.0%             4351af17-2e68-4b46-a78f-fad900e44d13  rack1</span><br><span class="line">UN  xx.xxx.xx.xx  581.43 MB  256          73.5%             f3f590ac-9835-47bb-b4d8-6e17ea2916ac  rack1</span><br></pre></td></tr></table></figure><h3 id="运行nodetool-cleanup"><a href="#运行nodetool-cleanup" class="headerlink" title="运行nodetool cleanup"></a>运行nodetool cleanup</h3><blockquote><p>nodetool options cleanup [keyspace_name [table_name] […] ]</p></blockquote><p>在所有新节点都加入集群并且数据同步完成后，在之前旧的每一个节点上运行nodetool cleanup操作删除keys。<br>在做操作时保证一个节点结束后再运行下一个节点，不要并发执行，这样可以安全地推迟清理</p><hr><h1 id="Reomve-Nodes"><a href="#Reomve-Nodes" class="headerlink" title="Reomve Nodes"></a>Reomve Nodes</h1><h2 id="UN状态的节点下线"><a href="#UN状态的节点下线" class="headerlink" title="UN状态的节点下线"></a>UN状态的节点下线</h2><p>在要下线的节点运行<code>nodetool decommission</code>命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nodetool &lt;options&gt; decommission</span><br></pre></td></tr></table></figure><p>该命令会将当前节点的range和请求交给其他节点管理，并且将数据同步给其他节点</p><h2 id="DN状态的节点下线"><a href="#DN状态的节点下线" class="headerlink" title="DN状态的节点下线"></a>DN状态的节点下线</h2><p>在任何存活的节点运行<code>nodetool removenode</code>命令</p><p>该命令会将当前集群下线的节点移除，并且将数据同步给其他节点</p><ul><li>查看节点状态：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Datacenter: DC1</span><br><span class="line">===============</span><br><span class="line">Status=Up/Down</span><br><span class="line">|/ State=Normal/Leaving/Joining/Moving</span><br><span class="line">--  <span class="function">Address        Load       Tokens  <span class="title">Owns</span> <span class="params">(effective)</span>  Host ID                               Rack</span></span><br><span class="line"><span class="function">UN  192.168.2.101  112.82 KB  256     31.7%             420129fc-0d84-42b0-be41-ef7dd3a8ad06  RAC1</span></span><br><span class="line"><span class="function">DN  192.168.2.103  91.11 KB   256     33.9%             d0844a21-3698-4883-ab66-9e2fd5150edd  RAC1</span></span><br><span class="line"><span class="function">UN  192.168.2.102  124.42 KB  256     32.6%             8d5ed9f4-7764-4dbd-bad8-43fddce94b7c  RAC1</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> nodetool &lt;options&gt; removenode -- &lt;status&gt; | &lt;force&gt; | &lt;ID&gt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> nodetool removenode d0844a21-3698-4883-ab66-9e2fd5150edd</span><br></pre></td></tr></table></figure><h2 id="节点下线失败"><a href="#节点下线失败" class="headerlink" title="节点下线失败"></a>节点下线失败</h2><blockquote><p> nodetool assassinate</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nodetool [options] assassinate &lt;ip_address&gt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nodetool -u cassandra -pw cassandra assassinate 192.168.100.2</span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在对Cassandra进行维护的时候，通常需要扩集群或者迁移数据，涉及到添加、移除节点。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/BigData/"/>
    
      <category term="Cassandra" scheme="http://imperio-wxm.github.io/categories/BigData/Cassandra/"/>
    
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/tags/Linux/"/>
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Cassandra" scheme="http://imperio-wxm.github.io/tags/Cassandra/"/>
    
  </entry>
  
  <entry>
    <title>记一次HBase Drop的错误</title>
    <link href="http://imperio-wxm.github.io/2018/09/24/HBase-DropTable-Error/"/>
    <id>http://imperio-wxm.github.io/2018/09/24/HBase-DropTable-Error/</id>
    <published>2018-09-24T04:14:56.000Z</published>
    <updated>2018-09-24T04:26:06.988Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>hbsae表进行数据清洗，在清空数据的时候发生错误，引发后续问题</p></blockquote><a id="more"></a><p><code>HBase version: 1.2.0-cdh5.11.1</code></p><ul><li>HBase 清空数据</li></ul><ol><li>truncate ‘TableName’（清除数据，并且清除了分区）</li><li>truncate_preserve ‘TableName’（清除数据，不清除分区)</li></ol><p>由于我想保留分区，所以选择了 <code>truncate_preserve</code></p><h2 id="问题发生："><a href="#问题发生：" class="headerlink" title="问题发生："></a>问题发生：</h2><p>用hbase shell 执行truncate_preserve ‘TableName’，中途网络问题ssh突然断开连接</p><ul><li>shell显示：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Truncating <span class="string">'TableName'</span> table (it may take a <span class="keyword">while</span>):</span><br><span class="line"> - Disabling table...</span><br><span class="line"> - Truncating table...</span><br></pre></td></tr></table></figure><p>后重新连接ssh hbase shell，list 发现表名存在，但是scan、disable、drop命令都报Table not found</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Table TableName does not exist.</span><br><span class="line"></span><br><span class="line">Start disable of named table:</span><br><span class="line">  hbase&gt; disable <span class="string">'t1'</span></span><br><span class="line">  hbase&gt; disable <span class="string">'ns1:t1'</span></span><br></pre></td></tr></table></figure><p>HBase Web UI 上也存在这张表，但是点进去有报错信息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hbase.client.HBaseAdmin.checkTableExistence(HBaseAdmin.java:<span class="number">1499</span>)</span><br><span class="line">org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled(HBaseAdmin.java:<span class="number">1510</span>)</span><br><span class="line">org.apache.hadoop.hbase.generated.master.table_jsp._jspService(table_jsp.java:<span class="number">192</span>)</span><br><span class="line">org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:<span class="number">98</span>)</span><br><span class="line">javax.servlet.http.HttpServlet.service(HttpServlet.java:<span class="number">820</span>)</span><br><span class="line">org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:<span class="number">511</span>)</span><br><span class="line">org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:<span class="number">1221</span>)</span><br><span class="line">org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:<span class="number">113</span>)</span><br><span class="line">org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:<span class="number">1212</span>)</span><br><span class="line">org.apache.hadoop.hbase.http.ClickjackingPreventionFilter.doFilter(ClickjackingPreventionFilter.java:<span class="number">48</span>)</span><br><span class="line">org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:<span class="number">1212</span>)</span><br><span class="line">org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:<span class="number">1354</span>)</span><br><span class="line">org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:<span class="number">1212</span>)</span><br><span class="line">org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:<span class="number">49</span>)</span><br><span class="line">org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:<span class="number">1212</span>)</span><br><span class="line">org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:<span class="number">49</span>)</span><br><span class="line">org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:<span class="number">1212</span>)</span><br><span class="line">org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:<span class="number">399</span>)</span><br><span class="line">org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:<span class="number">216</span>)</span><br><span class="line">org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:<span class="number">182</span>)</span><br><span class="line">org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:<span class="number">767</span>)</span><br><span class="line">org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:<span class="number">450</span>)</span><br><span class="line">org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:<span class="number">230</span>)</span><br><span class="line">org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:<span class="number">152</span>)</span><br><span class="line">org.mortbay.jetty.Server.handle(Server.java:<span class="number">326</span>)</span><br><span class="line">org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:<span class="number">542</span>)</span><br><span class="line">org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:<span class="number">928</span>)</span><br><span class="line">org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:<span class="number">549</span>)</span><br><span class="line">org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:<span class="number">212</span>)</span><br><span class="line">org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:<span class="number">404</span>)</span><br><span class="line">org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:<span class="number">410</span>)</span><br><span class="line">org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:<span class="number">582</span>)</span><br></pre></td></tr></table></figure><h2 id="问题推断"><a href="#问题推断" class="headerlink" title="问题推断"></a>问题推断</h2><p>第一反应是这张表到底存不存在？</p><p>到HDFS上查看文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /hbase/data/<span class="keyword">default</span>/TableName</span><br></pre></td></tr></table></figure><blockquote><p>发现文件目录存在，但是里面没有任何文件，是空目录。推断可能是文件已经删掉了，但是缓存中或者zk中还是有这张表的信息，因为突然中止导致table meta没有生成</p></blockquote><h2 id="尝试恢复"><a href="#尝试恢复" class="headerlink" title="尝试恢复"></a>尝试恢复</h2><ul><li>snapshot 恢复（失败）</li></ul><p>因为这张表之前做过snapshot备份，想从snapshot恢复</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clone_snapshot <span class="string">'TableName_Bak'</span>, <span class="string">'TableName'</span></span><br><span class="line">restore_snapshot <span class="string">'TableName_Bak'</span></span><br></pre></td></tr></table></figure><p>发现一只会卡在<code>restore_snapshot &#39;TableName_Bak&#39;</code>，应该是找不到这张表的meta</p><ul><li><p>hbck修复（失败）</p></li><li><p>想着通过meta修复，可以自动生成desc文件</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#修复 meta</span><br><span class="line">hbase hbck -fixMeta</span><br><span class="line"></span><br><span class="line">#重新分配rs</span><br><span class="line">hbase hbck -fixAssignments</span><br></pre></td></tr></table></figure><p>执行这两条语句后发现日志中均没有该表名，也没有任何异常，问题依旧</p><ul><li>zookeeper删除信息（成功）</li></ul><p>登录hbase zk：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">zkCli.sh</span><br><span class="line"></span><br><span class="line">ls /hbase/table</span><br><span class="line">rmr  /hbase/table/TableName 相关信息</span><br><span class="line"></span><br><span class="line">ls /hbase/table-lock</span><br><span class="line">rmr /hbase/table-lock/TableName 相关信息</span><br></pre></td></tr></table></figure><p>重启hbase 集群后这张表已经不存在，重新建表后正常</p><hr><blockquote><p>转载请注明出处</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;hbsae表进行数据清洗，在清空数据的时候发生错误，引发后续问题&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/BigData/"/>
    
      <category term="HBase" scheme="http://imperio-wxm.github.io/categories/BigData/HBase/"/>
    
    
      <category term="HBase" scheme="http://imperio-wxm.github.io/tags/HBase/"/>
    
      <category term="Linux" scheme="http://imperio-wxm.github.io/tags/Linux/"/>
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Struts2-Upgrade</title>
    <link href="http://imperio-wxm.github.io/2018/08/27/Struts2-Upgrade/"/>
    <id>http://imperio-wxm.github.io/2018/08/27/Struts2-Upgrade/</id>
    <published>2018-08-27T15:12:08.000Z</published>
    <updated>2018-08-27T15:57:23.675Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>遇到总结一下项目中有关Struts2升级中遇到的坑。项目大概开始于09年左右，维护近10年，由于Struts2安全漏洞问题决定升级版本，由于版本跨度比较大，一些方法已经弃用或配置变更等</p></blockquote><a id="more"></a><h2 id="Main-Maven-Dependency"><a href="#Main-Maven-Dependency" class="headerlink" title="Main Maven Dependency"></a>Main Maven Dependency</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// old</span></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.struts&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;struts2-core&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.1.8.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// new</span></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.struts&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;struts2-core&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.5.17&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="Struts2-xml"><a href="#Struts2-xml" class="headerlink" title="Struts2.xml"></a>Struts2.xml</h2><ul><li>old</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;filter&gt;</span><br><span class="line">    &lt;filter-name&gt;ActionContextCleanUp&lt;/filter-name&gt;</span><br><span class="line">    &lt;filter-<span class="class"><span class="keyword">class</span>&gt;<span class="title">org</span>.<span class="title">apache</span>.<span class="title">struts2</span>.<span class="title">dispatcher</span>.<span class="title">ActionContextCleanUp</span>&lt;/<span class="title">filter</span>-<span class="title">class</span>&gt;</span></span><br><span class="line"><span class="class">&lt;/<span class="title">filter</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&lt;<span class="title">filter</span>-<span class="title">mapping</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">filter</span>-<span class="title">name</span>&gt;<span class="title">ActionContextCleanUp</span>&lt;/<span class="title">filter</span>-<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">url</span>-<span class="title">pattern</span>&gt;/*&lt;/<span class="title">url</span>-<span class="title">pattern</span>&gt;</span></span><br><span class="line"><span class="class">&lt;/<span class="title">filter</span>-<span class="title">mapping</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&lt;<span class="title">filter</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">filter</span>-<span class="title">name</span>&gt;<span class="title">struts</span>&lt;/<span class="title">filter</span>-<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">filter</span>-<span class="title">class</span>&gt;<span class="title">org</span>.<span class="title">apache</span>.<span class="title">struts2</span>.<span class="title">dispatcher</span>.<span class="title">FilterDispatcher</span>&lt;/<span class="title">filter</span>-<span class="title">class</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">init</span>-<span class="title">param</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">param</span>-<span class="title">name</span>&gt;<span class="title">struts</span>.<span class="title">i18n</span>.<span class="title">encoding</span>&lt;/<span class="title">param</span>-<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">param</span>-<span class="title">value</span>&gt;<span class="title">UTF</span>-8&lt;/<span class="title">param</span>-<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">init</span>-<span class="title">param</span>&gt;</span></span><br><span class="line"><span class="class">&lt;/<span class="title">filter</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&lt;<span class="title">filter</span>-<span class="title">mapping</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">filter</span>-<span class="title">name</span>&gt;<span class="title">struts</span>&lt;/<span class="title">filter</span>-<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">url</span>-<span class="title">pattern</span>&gt;*.<span class="title">action</span>&lt;/<span class="title">url</span>-<span class="title">pattern</span>&gt; </span></span><br><span class="line"><span class="class">&lt;/<span class="title">filter</span>-<span class="title">mapping</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>new </li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;filter&gt;</span><br><span class="line">    &lt;filter-name&gt;struts2&lt;/filter-name&gt;</span><br><span class="line">    &lt;filter-<span class="class"><span class="keyword">class</span>&gt;<span class="title">org</span>.<span class="title">apache</span>.<span class="title">struts2</span>.<span class="title">dispatcher</span>.<span class="title">filter</span>.<span class="title">StrutsPrepareAndExecuteFilter</span>&lt;/<span class="title">filter</span>-<span class="title">class</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">init</span>-<span class="title">param</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">param</span>-<span class="title">name</span>&gt;<span class="title">struts</span>.<span class="title">i18n</span>.<span class="title">encoding</span>&lt;/<span class="title">param</span>-<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">        &lt;<span class="title">param</span>-<span class="title">value</span>&gt;<span class="title">UTF</span>-8&lt;/<span class="title">param</span>-<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;/<span class="title">init</span>-<span class="title">param</span>&gt;</span></span><br><span class="line"><span class="class">&lt;/<span class="title">filter</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&lt;<span class="title">filter</span>-<span class="title">mapping</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">filter</span>-<span class="title">name</span>&gt;<span class="title">struts2</span>&lt;/<span class="title">filter</span>-<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">url</span>-<span class="title">pattern</span>&gt;*.<span class="title">action</span>&lt;/<span class="title">url</span>-<span class="title">pattern</span>&gt;</span></span><br><span class="line"><span class="class">&lt;/<span class="title">filter</span>-<span class="title">mapping</span>&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&lt;<span class="title">filter</span>-<span class="title">mapping</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">filter</span>-<span class="title">name</span>&gt;<span class="title">struts2</span>&lt;/<span class="title">filter</span>-<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="class">    &lt;<span class="title">url</span>-<span class="title">pattern</span>&gt;/<span class="title">project</span>/*&lt;/<span class="title">url</span>-<span class="title">pattern</span>&gt;</span></span><br><span class="line"><span class="class">&lt;/<span class="title">filter</span>-<span class="title">mapping</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>在Struts2 2.5.17中<code>org.apache.struts2.dispatcher.FilterDispatcher</code> 和 <code>org.apache.struts2.dispatcher.ActionContextCleanUp</code> 被废除，用 <code>org.apache.struts2.dispatcher.filter.StrutsPrepareAndExecuteFilter</code> 替换</p></blockquote><h2 id="升级过程中遇到的报错："><a href="#升级过程中遇到的报错：" class="headerlink" title="升级过程中遇到的报错："></a>升级过程中遇到的报错：</h2><blockquote><p>java.lang.NoSuchMethodError: ognl.SimpleNode.isEvalChain(Lognl/OgnlContext;)Z</p></blockquote><ul><li>问题：<code>ognl</code> 的jar包冲突</li></ul><p>需要查询项目中Jar依赖关系，排除无用Jar包。Ognl版本至少在<code>3.0.6</code>以上</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 我遇到的是xwork中有低版本的ognl包，故排除</span></span><br><span class="line">&lt;exclusion&gt;</span><br><span class="line">    &lt;artifactId&gt;ognl&lt;/artifactId&gt;</span><br><span class="line">    &lt;groupId&gt;opensymphony&lt;/groupId&gt;</span><br><span class="line">&lt;/exclusion&gt;</span><br></pre></td></tr></table></figure><blockquote><p>There is no Action mapped for namespace [/] and action name [user!add] associated with context path</p></blockquote><ul><li>问题：由于2.5.17安全机制，过滤器必须指定mapped规则</li></ul><ol><li>粗粒度—动态方法调用</li></ol><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// struts.xml配置添加</span></span><br><span class="line"> &lt;struts&gt;</span><br><span class="line">    &lt;constant name=<span class="string">"struts.enable.DynamicMethodInvocation"</span> value=<span class="string">"true"</span>/&gt;</span><br><span class="line">    ......</span><br><span class="line">    &lt;<span class="keyword">package</span> name=<span class="string">"default"</span> extends=<span class="string">"struts-default"</span>&gt;</span><br><span class="line">        ......</span><br><span class="line">        &lt;global-allowed-methods&gt;regex:.*&lt;/global-allowed-methods&gt;</span><br><span class="line">        ......</span><br><span class="line">    &lt;/package&gt;</span><br><span class="line">&lt;/struts&gt;</span><br></pre></td></tr></table></figure><ol><li>细粒度-动态方法调用</li></ol><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// action配置</span></span><br><span class="line"></span><br><span class="line"> &lt;action name=<span class="string">"login_*"</span> method=<span class="string">"&#123;2&#125;"</span>  <span class="class"><span class="keyword">class</span></span>=<span class="string">"com.wxmimperio.struts.&#123;1&#125;Action"</span>&gt; </span><br><span class="line">    .......</span><br><span class="line">    &lt;result name="success"&gt;/pages/success.jsp&lt;/result&gt; </span><br><span class="line">    &lt;result name="error"&gt;/pages/error.jsp&lt;/result&gt; </span><br><span class="line">    ......</span><br><span class="line">    &lt;allowed-methods&gt;regex:.*&lt;/allowed-methods&gt;</span><br><span class="line">    ......</span><br><span class="line">&lt;/action&gt;</span><br></pre></td></tr></table></figure><p>这里在action的name中通配了一个login_*，它对应映射的是method属性。如果在客户端发生一个这样的请 求：login_init.action、login_show.action等，这时服务器就会自动调用这个action中的init()方法或 show()方法。这里的method=”{1}”代表是第一个星号，如果有多个星号，就要根据顺序来通配{1},{2},{3}….</p><p><code>allowed-methods</code>中可以用逗号分隔指定方法名，也可以用正则匹配。</p><blockquote><p>错误： Struts2 与 Servlet 冲突</p></blockquote><p>表现在当struts.xml如下配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;filter-mapping&gt;</span><br><span class="line">    &lt;filter-name&gt;struts&lt;/filter-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;<span class="comment">/*&lt;/url-pattern&gt;</span></span><br><span class="line"><span class="comment">&lt;/filter-mapping&gt;</span></span><br></pre></td></tr></table></figure><p>struts拦截器会拦截/*下所有路径，所以自定义的servlet无法被mapped到，导致请求根本无法响应</p><p>解决方案如下：</p><ol><li>修改servlet的相关配置，统一在servlet后面加上.servlet</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;servlet&gt;  </span><br><span class="line">    &lt;servlet-name&gt;jqueryAjaxServlet&lt;/servlet-name&gt;  </span><br><span class="line">    &lt;servlet-<span class="class"><span class="keyword">class</span>&gt;<span class="title">com</span>.<span class="title">clzhang</span>.<span class="title">sample</span>.<span class="title">struts2</span>.<span class="title">servlet</span>.<span class="title">jQueryAjaxServlet</span>&lt;/<span class="title">servlet</span>-<span class="title">class</span>&gt;  </span></span><br><span class="line"><span class="class">&lt;/<span class="title">servlet</span>&gt; </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&lt;<span class="title">servlet</span>-<span class="title">mapping</span>&gt;  </span></span><br><span class="line"><span class="class">    &lt;<span class="title">servlet</span>-<span class="title">name</span>&gt;<span class="title">jqueryAjaxServlet</span>&lt;/<span class="title">servlet</span>-<span class="title">name</span>&gt;  </span></span><br><span class="line"><span class="class">    &lt;<span class="title">url</span>-<span class="title">pattern</span>&gt;/<span class="title">servlet</span>/<span class="title">jqueryAjax</span>.<span class="title">servlet</span>&lt;/<span class="title">url</span>-<span class="title">pattern</span>&gt;  </span></span><br><span class="line"><span class="class">&lt;/<span class="title">servlet</span>-<span class="title">mapping</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li>修改拦截页面配置，就是将struts的相关拦截配置一下</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;filter-mapping&gt;</span><br><span class="line">    &lt;filter-name&gt;struts2&lt;/filter-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;*.action&lt;/url-pattern&gt;</span><br><span class="line">&lt;/filter-mapping&gt;</span><br><span class="line">&lt;filter-mapping&gt;</span><br><span class="line">    &lt;filter-name&gt;struts2&lt;/filter-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;</span><br><span class="line">&lt;/filter-mapping&gt;</span><br><span class="line">&lt;filter-mapping&gt;</span><br><span class="line">    &lt;filter-name&gt;struts2&lt;/filter-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;*.jsp&lt;/url-pattern&gt;</span><br><span class="line">&lt;/filter-mapping&gt;</span><br><span class="line">&lt;filter-mapping&gt;</span><br><span class="line">    &lt;filter-name&gt;struts2&lt;/filter-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;/user<span class="comment">/*&lt;/url-pattern&gt;</span></span><br><span class="line"><span class="comment">&lt;/filter-mapping&gt;</span></span><br></pre></td></tr></table></figure><ol><li>修改struts.xml文件中的后缀映射</li></ol><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;constant name="struts.action.extension" value="action"&gt;&lt;/constant&gt;</span><br></pre></td></tr></table></figure><hr><blockquote><p>转载请注明出处</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;遇到总结一下项目中有关Struts2升级中遇到的坑。项目大概开始于09年左右，维护近10年，由于Struts2安全漏洞问题决定升级版本，由于版本跨度比较大，一些方法已经弃用或配置变更等&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Java" scheme="http://imperio-wxm.github.io/categories/Java/"/>
    
      <category term="Struts2" scheme="http://imperio-wxm.github.io/categories/Java/Struts2/"/>
    
    
      <category term="Java" scheme="http://imperio-wxm.github.io/tags/Java/"/>
    
      <category term="Web" scheme="http://imperio-wxm.github.io/tags/Web/"/>
    
      <category term="Struts2" scheme="http://imperio-wxm.github.io/tags/Struts2/"/>
    
  </entry>
  
  <entry>
    <title>PinPoint-Deploy[部署]</title>
    <link href="http://imperio-wxm.github.io/2018/06/30/pinpoint-deploy/"/>
    <id>http://imperio-wxm.github.io/2018/06/30/pinpoint-deploy/</id>
    <published>2018-06-30T12:54:58.000Z</published>
    <updated>2018-07-04T16:24:07.736Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Pinpoint is an APM (Application Performance Management) tool for large-scale distributed systems written in Java.</p></blockquote><a id="more"></a><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><ul><li>架构图</li></ul><p><img src="http://naver.github.io/pinpoint/images/pinpoint-architecture.png" alt="架构图"></p><p>组件：</p><ul><li><p>pinpoint-collector-1.7.3.war （数据收集）</p></li><li><p>pinpoint-web-1.7.3.war （页面展示）</p></li><li><p>pinpoint-agent-1.7.3.tar.gz （数据采集）</p></li></ul><p>所需环境</p><p><a href="http://naver.github.io/pinpoint/installation.html#quick-overview-of-installation" target="_blank" rel="noopener">版本适配信息</a></p><ul><li>Tomcat-8.5.32（web container）</li><li>Hbase-1.2.6 （for storage）</li></ul><h2 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h2><ol><li><p>安装HBase</p><ul><li>创建监控所需的HBase 表</li></ul></li><li><p>下载最新PinPoint执行文件（或自行git clone —&gt; maven build）</p><ul><li>The current stable version is v1.7.3.</li><li><a href="https://github.com/naver/pinpoint/releases/tag/1.7.3" target="_blank" rel="noopener">DownLoad</a></li></ul></li><li><p>部署 Pinpoint Collector</p><ul><li>解压 <code>pinpoint-collector-$VERSION.war</code> 到 <code>Tomcat</code> 容器</li><li>修改 <code>pinpoint-collector.properties</code>, <code>hbase.properties</code> 文件</li></ul></li><li><p>部署 Pinpoint Web </p><ul><li>解压 <code>pinpoint-web-$VERSION.war</code> 到 <code>Tomcat</code> 容器</li><li>修改 <code>pinpoint-web</code>, <code>hbase.properties</code> 文件</li></ul></li><li><p>启动Tomcat</p></li><li><p>部署 Pinpoint Agent</p><ul><li>解压 pinpoint-agent 压缩包</li><li>设置 <code>-javaagent:$AGENT_PATH/pinpoint-bootstrap-$VERSION.jar</code> JVM参数到 App Jar 启动参数</li><li>设置 <code>-Dpinpoint.agentId</code> and <code>-Dpinpoint.applicationName</code> 到 App Jar 启动参数</li><li>启动Java App</li></ul></li></ol><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><ul><li>Tomcat 两个webapp实例</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">&lt;Service name="Catalina1"&gt;</span><br><span class="line">    &lt;Connector port="8081" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt;</span><br><span class="line">    &lt;Engine name="Catalina1" defaultHost="localhost"&gt;</span><br><span class="line">      &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt;</span><br><span class="line">        &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm" resourceName="UserDatabase"/&gt;</span><br><span class="line">      &lt;/Realm&gt;</span><br><span class="line">      &lt;Host name="localhost"  appBase="/home/wxmimperio/software/apache-tomcat-8.5.32/pinpoint-web" unpackWARs="true" autoDeploy="true"&gt;</span><br><span class="line">        &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt;</span><br><span class="line">      &lt;/Host&gt;</span><br><span class="line">    &lt;/Engine&gt;</span><br><span class="line">  &lt;/Service&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;Service name="Catalina2"&gt;</span><br><span class="line">    &lt;Connector port="8082" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt;  </span><br><span class="line">    &lt;Engine name="Catalina2" defaultHost="localhost"&gt;</span><br><span class="line">      &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt;</span><br><span class="line">        &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm" resourceName="UserDatabase"/&gt;</span><br><span class="line">      &lt;/Realm&gt;</span><br><span class="line">      &lt;Host name="localhost"  appBase="/home/wxmimperio/software/apache-tomcat-8.5.32/pinpoint-collector" unpackWARs="true" autoDeploy="true"&gt;</span><br><span class="line">        &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt;</span><br><span class="line">      &lt;/Host&gt;</span><br><span class="line">    &lt;/Engine&gt;</span><br><span class="line">  &lt;/Service&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><ul><li>初始化Hbase 监控表</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> https://github.com/naver/pinpoint/blob/master/hbase/scripts/hbase-create.hbase</span><br><span class="line"></span><br><span class="line">vim hbase-create.hbase</span><br><span class="line"></span><br><span class="line">hbase shell hbase-create.hbase</span><br></pre></td></tr></table></figure><ul><li>pinpoint-web</li></ul><p>解压war，修改配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> unzip pinpoint-web-1.7.3.war -d pinpoint-web-1.7.3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> /apache-tomcat-8.5.32/pinpoint-web/pinpoint-web-1.7.3/WEB-INF/classes</span><br><span class="line"></span><br><span class="line">vim hbase.properties</span><br><span class="line"><span class="meta">#</span> 设置hbase地址</span><br><span class="line">hbase.client.host=192.168.1.110</span><br><span class="line">hbase.client.port=2181</span><br><span class="line"></span><br><span class="line">vim pinpoint-web.properties</span><br><span class="line"><span class="meta">#</span> 关闭集群模式</span><br><span class="line">cluster.enable=false</span><br></pre></td></tr></table></figure><ul><li>pinpoint-collector</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> unzip pinpoint-collector-1.7.3.war -d pinpoint-collector-1.7.3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> /apache-tomcat-8.5.32/pinpoint-collector/pinpoint-collector-1.7.3/WEB-INF/classes</span><br><span class="line"></span><br><span class="line">vim hbase.properties</span><br><span class="line"><span class="meta">#</span> 设置hbase地址</span><br><span class="line">hbase.client.host=192.168.1.110</span><br><span class="line">hbase.client.port=2181</span><br><span class="line"></span><br><span class="line">vim pinpoint-collector.properties</span><br><span class="line"><span class="meta">#</span> 关闭集群模式</span><br><span class="line">cluster.enable=false</span><br></pre></td></tr></table></figure><ul><li>重启Tomcat</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./startup.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> http://192.168.1.110:8081/pinpoint-web-1.7.3/#/main</span><br></pre></td></tr></table></figure><ul><li>部署应用</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -javaagent:/home/wxmimperio/software/pinpoint/pinpoint-agent-1.7.3/pinpoint-bootstrap-1.7.3.jar -Dpinpoint.agentId=wxm-consumer -Dpinpoint.applicationName=wxm-consumer -jar spring-boot-test-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p><img src="http://naver.github.io/pinpoint/images/ss_server-map.png" alt="应用拓扑"></p><p><img src="http://naver.github.io/pinpoint/images/ss_call-stack.png" alt="应用Trace"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Pinpoint is an APM (Application Performance Management) tool for large-scale distributed systems written in Java.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://imperio-wxm.github.io/categories/BigData/"/>
    
      <category term="MicroService" scheme="http://imperio-wxm.github.io/categories/BigData/MicroService/"/>
    
    
      <category term="大数据" scheme="http://imperio-wxm.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="监控" scheme="http://imperio-wxm.github.io/tags/%E7%9B%91%E6%8E%A7/"/>
    
      <category term="微服务" scheme="http://imperio-wxm.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
</feed>
