<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kudu-Deploy-Non-ClouderaManager]]></title>
    <url>%2F2019%2F08%2F23%2FKudu-Deploy-Non-ClouderaManager%2F</url>
    <content type="text"><![CDATA[CDH 5.10.x开始支持Kudu 1.2.0+的管理，但是由于版本匹配问题，很多时候想用比较新的Kudu但是CDH版本不允许，此时就需要脱离CM手动搭建集群 环境与前期准备 Kudu与CDH版本的依赖关系 CDH版本支持 RPMS查询 (具体版本可更换url上的版本号进行查看) CDH5.16.2-Kudu 1.7.0 root用户 在安装部署Kudu的时候必须是root用户，过程中还会涉及新建kudu用户 安装依赖与配置 依赖安装 1234sudo yum install autoconf automake cyrus-sasl-devel cyrus-sasl-gssapi \ cyrus-sasl-plain flex gcc gcc-c++ gdb git java-1.8.0-openjdk-devel \ krb5-server krb5-workstation libtool make openssl-devel patch \ pkgconfig redhat-lsb-core rsync unzip vim-common which 修改系统最大文件句柄数 1234vim /etc/security/limits.conf * soft nofile 65535* hard nofile 65535 域名互通 如果节点之间有域名，则必须添加hosts，使得各节点之间可以用域名互通 下载并安装RPM安装包CDH5.16.2-Kudu 1.7.0下载 12345678910111213141516171819// 依次wgetkudu-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el6.x86_64.rpmkudu-client-devel-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el6.x86_64.rpmkudu-client0-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el6.x86_64.rpmkudu-debuginfo-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el6.x86_64.rpmkudu-master-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el6.x86_64.rpmkudu-tserver-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el6.x86_64.rpm// 下载目录rpm -ivh --nodeps *Preparing... ########################################### [100%]1:kudu ########################################### [ 17%]2:kudu-client0 ########################################### [ 33%]3:kudu-client-devel ########################################### [ 50%]4:kudu-master ########################################### [ 67%]5:kudu-tserver ########################################### [ 83%]6:kudu-debuginfo ########################################### [100%] 创建数据目录，用户授权 由于Kudu对文件系统要求很高，坏盘会导致节点Crash，元数据损坏会导致节点无法重启，所以尽量将 data、wal、metadata、logs 分多盘符存储 1234567mkdir -p /app/kudu/master/datamkdir -p /app/kudu/master/logsmkdir -p /app/kudu/master/metadatamkdir -p /app/kudu/master/wal// 授权必须为kudu用户chown -R kudu:kudu kudu/ Master and Tserver config Master 123456789101112vim /etc/kudu/conf/master.gflagfile--fromenv=rpc_bind_addresses--fromenv=log_dir--fs_wal_dir=/app/kudu/master/wal--fs_data_dirs=/app/kudu/master/data--fs_metadata_dir=/app/kudu/master/metadata--log_dir=/app/kudu/master/logs--master_addresses=xxxx:7051,xxxx:7051,xxxx:7051--block_cache_capacity_mb=4096--max_log_size=40 Tserver 123456789101112vim /etc/kudu/conf/tserver.gflagfile--fromenv=rpc_bind_addresses--fromenv=log_dir--fs_wal_dir=/app/kudu/tserver/wal--fs_data_dirs=/app/kudu/tserver/data--fs_metadata_dir=/app/kudu/tserver/metadata--log_dir=/app/kudu/tserver/logs--tserver_master_addrs=xxxx:7051,xxxx:7051,xxxx:7051--block_cache_capacity_mb=4096--max_log_size=40 NTP同步 Kudu对NTP同步要求很高，不同步会导致节点Crash 123sudo yum install ntpsudo /etc/init.d/ntpd restart 启动、停止、重启1234567891011// start/etc/init.d/kudu-master start/etc/init.d/kudu-tserver start// restart/etc/init.d/kudu-master restart/etc/init.d/kudu-tserver restart// stop/etc/init.d/kudu-master stop/etc/init.d/kudu-tserver stop Web UI12345// master管理界面http://ip:8051/masters// tablet server管理界面http://ip:8050/ 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>Kudu</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kudu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hudi Getting Started]]></title>
    <url>%2F2019%2F08%2F09%2FHudi-Introduction%2F</url>
    <content type="text"><![CDATA[Hudi是类似Carbondata的软数据层，可以支持streaming的写入、读取，同时也支持将metadata同步到hive，提供Spark、Hive、Presto的SQL查询 software version Spark 2.4.0.cloudera1 Hadoop 2.6.0-cdh5.11.1 Hive 1.1.0-cdh5.11.1 Hudi 0.4.7 编译可以查看官方文档的Quickstart进行编译： Quickstart Github clone 最新的tag 支持Java 8+ 支持Hive 1+（Hive 2+ 据说有不少问题，可以通过Github Issue查询） 支持Spark2.x+ 支持Apache、CDH Hadoop 编译可选项 123cd incubator-hudi-hoodie-0.4.7mvn clean install -DskipITs -DskipTests -Dhadoop.version=2.6.0-cdh5.11.1 -Dhive.version=1.1.0-cdh5.11.1 参数 -DskipITs用来跳过integration test，test中含有docker test，没有docker环境无法编译通过 -DskipITs相关issue 编译成功 123456789101112131415161718192021222324252627282930313233[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary for Hoodie 0.4.7:[INFO] [INFO] Hoodie ............................................. SUCCESS [ 2.894 s][INFO] hoodie-common ...................................... SUCCESS [ 20.807 s][INFO] hoodie-hadoop-mr ................................... SUCCESS [ 2.372 s][INFO] hoodie-hive ........................................ SUCCESS [ 1.500 s][INFO] hoodie-timeline-service ............................ SUCCESS [ 15.561 s][INFO] hoodie-client ...................................... SUCCESS [ 5.584 s][INFO] hoodie-spark ....................................... SUCCESS [ 36.749 s][INFO] hoodie-utilities ................................... SUCCESS [ 35.936 s][INFO] hoodie-cli ......................................... SUCCESS [ 14.495 s][INFO] hoodie-hadoop-mr-bundle ............................ SUCCESS [ 2.126 s][INFO] hoodie-hive-bundle ................................. SUCCESS [ 23.160 s][INFO] hoodie-spark-bundle ................................ SUCCESS [02:43 min][INFO] hoodie-presto-bundle ............................... SUCCESS [ 23.024 s][INFO] hoodie-hadoop-docker ............................... SUCCESS [ 0.616 s][INFO] hoodie-hadoop-base-docker .......................... SUCCESS [ 0.491 s][INFO] hoodie-hadoop-namenode-docker ...................... SUCCESS [ 0.079 s][INFO] hoodie-hadoop-datanode-docker ...................... SUCCESS [ 0.076 s][INFO] hoodie-hadoop-history-docker ....................... SUCCESS [ 0.072 s][INFO] hoodie-hadoop-hive-docker .......................... SUCCESS [ 0.763 s][INFO] hoodie-hadoop-sparkbase-docker ..................... SUCCESS [ 0.095 s][INFO] hoodie-hadoop-sparkmaster-docker ................... SUCCESS [ 0.089 s][INFO] hoodie-hadoop-sparkworker-docker ................... SUCCESS [ 0.087 s][INFO] hoodie-hadoop-sparkadhoc-docker .................... SUCCESS [ 0.085 s][INFO] hoodie-integ-test .................................. SUCCESS [ 1.102 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 05:51 min[INFO] Finished at: 2019-08-09T15:46:24+08:00[INFO] ------------------------------------------------------------------------ Write With Spark思路： 从hive的一张表查询数据 转换成DF后写出到Hudi的新表 将这个新表sync到hive的metadata Write Code123456789101112131415161718192021222324String warehouseLocation = new File("spark-warehouse").getAbsolutePath();SparkSession spark = SparkSession .builder() .config("spark.sql.warehouse.dir", warehouseLocation) .enableHiveSupport() .appName("Spark Hudi Write Test") .getOrCreate();Dataset&lt;Row&gt; hiveQuery = spark.sql(" select * from dw.xxx where part_date='2019-08-02'");hiveQuery.write() .format("com.uber.hoodie") .option(DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY(), true) .option(DataSourceWriteOptions.HIVE_URL_OPT_KEY(), "jdbc:hive2://xxx:10000") .option(DataSourceWriteOptions.HIVE_DATABASE_OPT_KEY(), "dw") .option(DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY(), true) .option(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY(), "hoodie_wxm_test") .option(DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY(), "part_date") .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "key") .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "part_date") .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "event_time") .option(HoodieWriteConfig.TABLE_NAME, "hoodie_wxm_test") .mode(SaveMode.Append) .save("hudi data path"); Options 描述 HIVE_ASSUME_DATE_PARTITION_OPT_KEY 如果hive以日期分区，则日期的format格式，默认：yyyy/mm/dd HIVE_URL_OPT_KEY hive metastore url HIVE_DATABASE_OPT_KEY sync 到hive的库名 HIVE_SYNC_ENABLED_OPT_KEY 是否将hudi表的元数据sync到hive HIVE_TABLE_OPT_KEY sync 到hive 的表名 HIVE_PARTITION_FIELDS_OPT_KEY sync 到hive表的分区键从哪个列中提取 RECORDKEY_FIELD_OPT_KEY hudi 中recordKey从哪个列中提取 PARTITIONPATH_FIELD_OPT_KEY hudi 中分区键从哪个列中提取 PRECOMBINE_FIELD_OPT_KEY hudi 中预合并从哪个列中提取 HoodieWriteConfig.TABLE_NAME hudi 的表名 更多Options请参考Configurations Submit Job12345// 需要将 hoodie-hadoop-mr-0.4.7.jar、hoodie-spark-0.4.7.jar、hoodie-hive-0.4.7.jar、hoodie-common-0.4.7.jar 引入到spark shell 提交依赖中spark2-submit --jars basePath/hoodie-hadoop-mr-0.4.7.jar,basePath/hoodie-spark-0.4.7.jar,basePath/hoodie-hive-0.4.7.jar,basePath/hoodie-common-0.4.7.jar \--class com.wxmimperio.spark.hudi.HudiWrite --master yarn --deploy-mode cluster \--driver-memory 4g --executor-memory 2g --executor-cores 1 xxxx.jar 自定义日期分区规则 通过HIVE_ASSUME_DATE_PARTITION_OPT_KEY参数设定后，默认的Date分区是yyyy/mm/dd，而我的业务场景下是yyyy-mm-dd，需要重写（此部分可以根据实际业务分区场景自行实现PartitionValueExtractor接口） 12345678910111213141516171819202122232425262728293031public class DayPartitionValueExtractor implements PartitionValueExtractor &#123; private transient DateTimeFormatter dtfOut; public DayPartitionValueExtractor() &#123; this.dtfOut = DateTimeFormat.forPattern("yyyy-MM-dd"); &#125; private DateTimeFormatter getDtfOut() &#123; if (dtfOut == null) &#123; dtfOut = DateTimeFormat.forPattern("yyyy-MM-dd"); &#125; return dtfOut; &#125; @Override public List&lt;String&gt; extractPartitionValuesInPath(String partitionPath) &#123; // partition path is expected to be in this format yyyy/mm/dd String[] splits = partitionPath.split("-"); if (splits.length != 3) &#123; throw new IllegalArgumentException( "Partition path " + partitionPath + " is not in the form yyyy-mm-dd "); &#125; // Get the partition part and remove the / as well at the end int year = Integer.parseInt(splits[0]); int mm = Integer.parseInt(splits[1]); int dd = Integer.parseInt(splits[2]); DateTime dateTime = new DateTime(year, mm, dd, 0, 0); return Lists.newArrayList(getDtfOut().print(dateTime)); &#125;&#125; 添加配置 1.option(DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY(),"com.wxmimperio.spark.hudi.DayPartitionValueExtractor") Job运行通过查看Spark Job logs，可以发现： 1.默认Hudi支持parquet格式输出，关于ORC File的支持有相关Issue: HUDI-57 support orc file 2.会先生成parquet文件，然后用hive.cli相关命令建表，最后sync数据 sync到hive后，表结构会多几个Hudi的字段： 12345`_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, 3.分区的File Path与正常Hive的不同 正常情况下Hive的分区是key=value形式， 123insert overwrite table new_table partition(part_date='2019-08-02') select xxx from old_tabl;// hdfs file path: /wxm/hudi/data/hoodie_test/part_date=2019-08-02 Hudi默认情况是: hudi data path/2019-08-02，同时如果是第一次建表，当前分区不会自动添加，第二次运行会自动添加分区并load data，不清楚这是我测试有问题还是Hudi机制就是这样 已经提交了相关Issue：Synchronizing to hive partition is incorrect 4.java.lang.ClassNotFoundException: com.uber.hoodie.hadoop.HoodieInputFormat 在我前几次执行Job的时候，通过logs得知parquet文件已经生成，但是在sync hive的时候会报com.uber.hoodie.hadoop.HoodieInputFormat类不存在 这个类在hoodie-hadoop-mr-0.4.7.jar中，可是spark提交jar的时候，我已经将相关jar添加到了依赖中还是报这个错 问题在于Hudi建hive表指定的InputFormat是com.uber.hoodie.hadoop.HoodieInputFormat： 1INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: com.uber.hoodie.hive.HoodieHiveSyncException: Failed in executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS dw.wxm_hoodie_test( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `age` string, `key` string, `name` string, `timestamp` string) PARTITIONED BY (part string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'com.uber.hoodie.hadoop.HoodieInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/wxm/hudi/data' 需要将hoodie-hadoop-mr-0.4.7.jar、hoodie-common-0.4.7.jar两个包放到$HIVE_HOME/lib下，重启hive cluster 已经提交了相关Issue：java.lang.ClassNotFoundException: com.uber.hoodie.hadoop.HoodieInputFormat Read With Spark思路： 直接通过spark用sql查询sync到hive的表（不做介绍，和读hive一样） 通过client读取parquet文件 Read Code12345678910111213SparkSession spark = SparkSession .builder() .appName("Spark Hudi Read Test") .getOrCreate();spark.read() .format("com.uber.hoodie") .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY(), DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL()) .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY(), "20190809120720") .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY(), "20190809120729") .load("hudi file path") .limit(10) .show(); Options 描述 BEGIN_INSTANTTIME_OPT_KEY 过滤数据起始时间戳，不包含 END_INSTANTTIME_OPT_KEY 过滤数据终止时间戳 更多Options请参考Configurations Query With Hive Cli通过hive cli查询需要添加两个依赖 12add jar file:///basePath/hoodie-hive-bundle-0.4.7.jar;add jar file:///basePath/hoodie-hadoop-mr-bundle-0.4.7.jar; Query With Presto需要添加一个依赖到presto hive插件中 123cp basePath/packaging/hoodie-presto-bundle/target/hoodie-presto-bundle-0.4.7.jar &lt;presto_install&gt;/plugin/hive-hadoop2/// then restart presto cluster 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>Hadoop</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hudi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 重建索引 (Modify Mappings)]]></title>
    <url>%2F2019%2F08%2F07%2FElasticsearch-Rebuild-Index%2F</url>
    <content type="text"><![CDATA[Es重建索引是个比较麻烦的事情，过程繁琐且迁移数据的过程也很漫长，一下将介绍如何rebuild index software version elasticsearch 6.4.2 由于es 的mapping结构无法直接更改，所以如果需要修改mapping结构，则必须rebuild一个索引，利用alias机制将新建索引与老索引关联，然后做数据迁移 基本概念 Es别名机制不再赘述，规定索引名为indexName-日期，如test_index-190807 两个alias分别为indexName、indexName$，其中indexName是每个索引都有的alias，用于提供read操作；indexName$用于提供write操作（即做到只write当前alias索引，其他alias索引均可提供read） Alias规则如何确定，可根据实际业务需要确定 思路描述 例如indexName=test_index-old 1.按新schema结构建立索引test_index-new（新索引结构必须与旧的兼容，否则数据迁移会出问题） 2.暂停test_index-old数据写入 3.移除test_index-old的test_index$ alias；添加test_index-new的test_index$ alias，然后恢复数据持续写入（目的是让当前写服务能迁移到新的索引，且alias的操作非常快/秒级，此过程几乎可以看做服务不中断） 此方法在数据迁移时会查出double的重复数据，想要线上业务完全不受影响，可以将当前 write/read 全部转到一个新的index，alias指向新索引；此时将旧索引当做静态数据，数据迁移完成后瞬间切换alias（即创建两个新索引，一个用于持续读写，另一个用于迁移数据） 4.test_index-old 数据迁移至 test_index-new（此过程缓慢） 5.移除test_index-old的test_index alias，添加test_index-new的test_index alias 6.至此test_index-old 的数据已经 迁移到 test_index-new，完成index的rebuild，可删除test_index-old 操作步骤新建索引 通常用curl或者kibana devtools就可以实现，但是如果mappings、settings比较复杂建议用程序先获取old index info，再此基础上做改动，然后再新建；此步骤必须保证新索引结构可以完全兼容旧索引 123456789// 通过java rest clientLowLevelClient().performRequest(method, endpoint, requsetBody);// 1. 使用GET方法获取index info// 2. 解析response 获取settings、mappings，并修改// 3. 建立新索引CreateIndexResponse resp = HighLevelClient().indices().create(new CreateIndexRequest(index).mapping("data", mappings).settings(settings)); 迁移index$ Write Alias 此操作前必须停止索引写入 12345678910// 移除AliasPOST /_aliases&#123; "actions": [&#123; "remove": &#123; "index": "test_index-old", "alias": "test_index$" &#125; &#125;]&#125; 123456789101112// 添加AliasPOST /_aliases&#123; "actions": [ &#123; "add": &#123; "index": "test_index-new", "alias": "test_index$" &#125; &#125; ]&#125; 执行结束后开启写索引 数据迁移123456789POST _reindex?wait_for_completion=false&#123; "source": &#123; "index": "test_index-old" &#125;, "dest": &#123; "index": "test_index-new" &#125;&#125; 此命令会返回taskId，可根据Id查询运行状态 1GET _tasks/&#123;taskId&#125; 中途遇到什么问题可以终止task 1PUT _tasks/&#123;taskID&#125;/cancel 迁移index Read Alias123456789101112// 添加AliasPOST /_aliases&#123; "actions": [ &#123; "add": &#123; "index": "test_index-new", "alias": "test_index" &#125; &#125; ]&#125; 12345678910// 移除AliasPOST /_aliases&#123; "actions": [&#123; "remove": &#123; "index": "test_index-old", "alias": "test_index" &#125; &#125;]&#125; 此时test_index-old 已经被重建为 test_index-new 调优 Elasticsearch 的数据迁移过程十分缓慢，通过以下方法可以进行调优（实测：3个Es nodes，30G+数据，60m+ Docs，Task需要30—40min左右结束） 1.source中调整batch_size(defailt: 1000) 批量大小取决于数据、分析和集群配置，但一个好的起点是每批处理5-15MB 1234"source": &#123; "index": "sourceIndex", "size": 5000&#125; 2.slicing 配置 1POST _reindex?slices=5&amp;refresh 【slices大小设置注意事项】 slices大小的设置可以手动指定，或者设置slices设置为auto，auto的含义是：针对单索引，slices大小=分片数；针对多索引，slices=分片的最小值 当slices的数量等于索引中的分片数量时，查询性能最高效。slices大小大于分片数，非但不会提升效率，反而会增加开销 如果这个slices数字很大(例如500)，建议选择一个较低的数字，因为过大的slices 会影响性能 3.关闭刷新和副本 在数据迁移过程中关闭refresh_interval、number_of_replicas可以大幅提升性能，迁移完成后恢复默认值 12345PUT indexName/_settings&#123; "refresh_interval": "-1", "number_of_replicas": "0"&#125; 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>Elasticsearch</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper动态扩容、缩容]]></title>
    <url>%2F2019%2F07%2F18%2FZooKeeper-ExpansionNode-ReducedNode%2F</url>
    <content type="text"><![CDATA[Zookeeper是大数据领域中常用的分布式协调工具，如何不中断服务动态的扩充、缩减、迁移node？ 环境说明先有一个3台机器的zk集群，需要先对这个集群进行扩容增加节点，随后进行缩容；最终达到节点迁移的目的 ps：zookeeper相关概念、配置、环境搭建不在赘述 mid node 1 node1 2 node2 3 node3 4 node4 5 node5 6 node6 ZooKeeper监控工具推荐用: zkui ZooKeeper监控命令推荐(必须提前安装nc): 12// 查看zk状态echo stat | nc 127.0.0.1 2181 Command Desc conf 输出相关服务配置的详细信息 cons 列出所有连接到服务器的客户端的完全的连接/会话的详细信息,包括 接受 or 发送的包数量、会话 id 、操作延迟、最后的操作执行等等信息 dump 列出未经处理的会话和临时节点 envi 输出关于服务环境的详细信息（区别于 conf 命令） reqs 列出未经处理的请求 ruok 测试服务是否处于正确状态;如果确实如此,那么服务返回 imok ,否则不做任何相应 stat 输出关于性能和连接的客户端的列表 wchs 列出服务器 watch 的详细信息 wchc 通过 session 列出服务器 watch 的详细信息,它的输出是一个与watch 相关的会话的列表 wchp 通过路径列出服务器 watch 的详细信息;它输出一个与 session相关的路径 扩容 &amp; 缩容 思路 原本有node1、2、3 的zk集群，现在要将node4、5、6的节点加入，扩充3个节点 zk节点数应为3、5、7这样的奇数个，否则无法选举出leader，因此想要添加node4、5、6节点，则： 添加node4、5，使得总节点数从3个扩充到5 (n-1)/2，5节点zk允许挂掉2个节点，则先下线node1、再上线node6 此时在线节点有node2、3、4、5、6 最后再下线node2、3节点 此时zk节点从node1、2、3切换到了node4、5、6 操作步骤查看node1、2、3状态node1(follower)：123456789Received: 247Sent: 246Connections: 2Outstanding: 0Zxid: 0x5500000012Mode: followerNode count: 37658Environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT node2(leader)123456789Received: 19Sent: 18Connections: 1Outstanding: 0Zxid: 0x5500000012Mode: leaderNode count: 37658Environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT node3(follower)123456789Received: 1Sent: 0Connections: 1Outstanding: 0Zxid: 0x5500000012Mode: followerNode count: 37658Environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT 增加node4、5节点1234567// 修改zoo.cfgserver.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888server.4=node4:2888:3888server.5=node5:2888:3888 node4(follower)123456789Received: 1Sent: 0Connections: 1Outstanding: 0Zxid: 0x5500000015Mode: followerNode count: 37658Environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT node5(follower)123456789Received: 1Sent: 0Connections: 1Outstanding: 0Zxid: 0x5500000016Mode: followerNode count: 37658Environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT 修改node1、2、3配置，重启ps: 最后在重启leader节点，会触发选举，默认会选择myid最大的为新leader，即node5 轮序重启后，原来node2：Mode: leader——&gt;Mode: follower；最后添加的node5：Mode: follower——&gt;Mode: leader 至此新增2个节点完成，现在集群一共5个节点 下线node1、上线node6 下线node1 关闭node1，修改node2、3、4，去除node1配置后重启 最后修改node5，新增node6配置重启，此时node5任然是leader 至此node1下线，5个节点的zk允许挂掉2个节点 上线node6 修改node2、3、4节点，添加node6配置后重启 上线node6节点，配置与node2、3、4相同 node6(follower)123456789Received: 1Sent: 0Connections: 1Outstanding: 0Zxid: 0x5900000002Mode: followerNode count: 37658Environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT 重启node5，此时会触发选举，选择node6为新leader 至此5个节点配置完全相同，node6加入集群且为新leader 下线node2、3关闭node2、3，修改node4、5，去除node2、3配置后重启 修改node6，去除node2、3配置重启，此时会选取myid最大的node5为leader 至此node1、2、3全部下线完成，zookeeper已经从node1、2、3节点迁移到了node4、5、6节点 其他方案 修改node4、5、6配置，添加6个节点全部配置，依次启动；此时node4、5、6是无法加入节点的 修改node1、3配置，添加node4、5、6信息，重启；此时会触发选举，根据myid最大的node6为leader；之前的leader node2被踢出集群 下线node1、2，修改node4、5，去除node1、2配置后重启，此时leader依然是node6 修改node6，去除node1、2、3配置重启，此时会触发选举，根据myid选择node5为leader 下线node3，修改node4、5，去除node3配置重启，触发选举，node6为leader 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>ZooKeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat-JDTCompiler-Error]]></title>
    <url>%2F2019%2F06%2F15%2FTomcat-JDTCompiler-Error%2F</url>
    <content type="text"><![CDATA[不同版本的JDK编译出的JSP与Tomcat的兼容性也不相同 Software Version Jdk 1.8.0_121 Tomcat 7.0.27 最近在维护一个老项目的时候，用Jenkins jdk1.8 编译打包war后，解压放在1.7的Tomcat下 运行Jsp编译报错如下：1234567891011121314151617181920212223242526272829303132333435org.eclipse.jdt.internal.compiler.classfmt.ClassFormatException at org.eclipse.jdt.internal.compiler.classfmt.ClassFileReader.&lt;init&gt;(ClassFileReader.java:372) at org.apache.jasper.compiler.JDTCompiler$1.findType(JDTCompiler.java:232) at org.apache.jasper.compiler.JDTCompiler$1.findType(JDTCompiler.java:188) at org.eclipse.jdt.internal.compiler.lookup.LookupEnvironment.askForType(LookupEnvironment.java:113) at org.eclipse.jdt.internal.compiler.lookup.UnresolvedReferenceBinding.resolve(UnresolvedReferenceBinding.java:49) at org.eclipse.jdt.internal.compiler.lookup.BinaryTypeBinding.resolveType(BinaryTypeBinding.java:122) at org.eclipse.jdt.internal.compiler.lookup.PackageBinding.getTypeOrPackage(PackageBinding.java:168) at org.eclipse.jdt.internal.compiler.lookup.Scope.getType(Scope.java:2472) at org.eclipse.jdt.internal.compiler.ast.TypeDeclaration.resolve(TypeDeclaration.java:1006) at org.eclipse.jdt.internal.compiler.ast.TypeDeclaration.resolve(TypeDeclaration.java:1258) at org.eclipse.jdt.internal.compiler.ast.CompilationUnitDeclaration.resolve(CompilationUnitDeclaration.java:539) at org.eclipse.jdt.internal.compiler.Compiler.process(Compiler.java:763) at org.eclipse.jdt.internal.compiler.Compiler.compile(Compiler.java:468) at org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:459) at org.apache.jasper.compiler.Compiler.compile(Compiler.java:378) at org.apache.jasper.compiler.Compiler.compile(Compiler.java:353) at org.apache.jasper.compiler.Compiler.compile(Compiler.java:340) at org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:646) at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:357) at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:390) at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:334) at javax.servlet.http.HttpServlet.service(HttpServlet.java:722) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210) at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java:684) at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:471) at org.apache.catalina.core.ApplicationDispatcher.doForward(ApplicationDispatcher.java:402) at org.apache.catalina.core.ApplicationDispatcher.forward(ApplicationDispatcher.java:329) at org.apache.struts2.dispatcher.ServletDispatcherResult.doExecute(ServletDispatcherResult.java:154) at org.apache.struts2.dispatcher.StrutsResultSupport.execute(StrutsResultSupport.java:186) at com.opensymphony.xwork2.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:361) at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:265) at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:163) at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:87) 错误原因是Jdk1.8 编译后，与Tomcat1.7 不兼容 处理：Download Eclipse ECJ Jar 1.maven 仓库下载最新的 ecj-4.6.1.jar 2.Tomcat/lib 目录下，移除 ecj-3.7.2.jar 3.上传 ecj-4.6.1.jar 至Tomcat/lib 重启Tomcat 问题得以解决！ 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitbook 制作 PDF]]></title>
    <url>%2F2019%2F05%2F27%2FGitBook-To-Pdf%2F</url>
    <content type="text"><![CDATA[通常需要将Gitbook的内容转成PDF格式，方便离线保存 一、安装Caliber应用程序官网下载 Windows 环境变量添加： 添加到系统path末尾:1;C:\Program Files (x86)\Calibre2 验证是否安装成功 12345// cmdebook-convert --versionebook-convert.exe (calibre 3.43.0)Created by: Kovid Goyal &lt;kovid@kovidgoyal.net&gt; 二、GitBook制作PDF npm 安装 ebook-convert 1npm install ebook-convert -g Gitbook 根目录下运行： 1gitbook pdf 在根目录下会生成book.pdf 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cygwin Windows 换行符转换]]></title>
    <url>%2F2019%2F04%2F25%2FCygwin-DosFiles-Format%2F</url>
    <content type="text"><![CDATA[在Windows下编辑的Shell脚本，使用Cygwin仿Linux环境运行，出现 $’\r’: 未找到命令错误 因为在Dos or Window下回车键实际上输入的是 回车（CR) 和 换行（LF），而Linux or Unix下回车键只输入 换行（LF），所以文件在每行都会多了一个 CR，Linux下运行时就会报错找不到命令，需要把Dos文件格式转换为Unix格式 1. 安装dos2unix组件Cygwin 官网 下载Cygwin，双击setup-x86_64.exe，选择从本地或者internet安装，选择dos2unix组件进行安装 测试脚本： 123456// windows下记事本编辑#!/bin/shd=`date +%Y%m%d`echo $d Cygwin直接运行： 2. 单文件转换1dos2unix test.sh Cygwin转换后运行： 3. 目录批量转换1find . -type f -exec dos2unix &#123;&#125; \; 将转换后的文件直接upload到linux服务器上，也能正常运行 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Cygwin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL中的CUBE、ROLLUP、GROUPING用法]]></title>
    <url>%2F2019%2F03%2F12%2FSQL-Cube-Rollup-GroupingSets%2F</url>
    <content type="text"><![CDATA[在SQL语法中，经常用到GROUP BY来做多维度的聚合。但是遇到多个维度并列聚合的方式，通常是将每个维度用GROUP BY统计后，再使用UNION语法将结果集汇总，但是这样的SQL执行计划会在数据INPUT端从存储引擎获取多次，导致重复获取数据，浪费机器资源。CUBE、ROLLUP、GROUPING的语法可以更高效的做到多维度的聚合 SoftWare Version Presto 0.149 Hive 1.1.0-cdh5.11.1 Java 1.8.0_121 此文会以Presto On Hive 的方式演示实例，Mysql不同版本支持不同，Oracle、Sql Server的语法方式略有差异 数据准备建测试表123456789101112131415CREATE TABLE `wxm_test_fun`( `id` int COMMENT 'null', `name` string COMMENT 'null', `area_id` int COMMENT 'null', `group_id` int COMMENT 'null')COMMENT 'wxm_test_fun'ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ( 'field.delim'='\t', 'serialization.format'='\t') STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'; 准备数据123456789101 wxm1 000 1111 wxm2 001 1122 wxm1 000 1112 wxm2 001 1123 wxm3 000 1113 wxm3 001 1124 wxm4 000 1111 wxm4 000 1124 wxm4 002 1115 wxm1 003 113 说明聚合维度 四个维度：id（0），name（1），area_id（2），group_id（3） CUBE cube会对所有聚合可能进行计算：CUBE（A,B,C），会计算group by A union group by B group by C union group by （AB） union group by （AC） union group by （BC） union group by （BC） union all （ABC） 分组的次数=2ⁿ-1；n为待分组的字段个数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657presto:temp&gt; select id,name,area_id,count(*) count from wxm_test_fun group by cube(id,name,area_id) order by id,name,area_id; id | name | area_id | count ------+------+---------+------- 1 | wxm1 | 0 | 1 1 | wxm1 | NULL | 1 1 | wxm2 | 1 | 1 1 | wxm2 | NULL | 1 1 | wxm4 | 0 | 1 1 | wxm4 | NULL | 1 1 | NULL | 0 | 2 1 | NULL | 1 | 1 1 | NULL | NULL | 3 2 | wxm1 | 0 | 1 2 | wxm1 | NULL | 1 2 | wxm2 | 1 | 1 2 | wxm2 | NULL | 1 2 | NULL | 0 | 1 2 | NULL | 1 | 1 2 | NULL | NULL | 2 3 | wxm3 | 0 | 1 3 | wxm3 | 1 | 1 3 | wxm3 | NULL | 2 3 | NULL | 0 | 1 3 | NULL | 1 | 1 3 | NULL | NULL | 2 4 | wxm4 | 0 | 1 4 | wxm4 | 2 | 1 4 | wxm4 | NULL | 2 4 | NULL | 0 | 1 4 | NULL | 2 | 1 4 | NULL | NULL | 2 5 | wxm1 | 3 | 1 5 | wxm1 | NULL | 1 5 | NULL | 3 | 1 5 | NULL | NULL | 1 NULL | wxm1 | 0 | 2 NULL | wxm1 | 3 | 1 NULL | wxm1 | NULL | 3 NULL | wxm2 | 1 | 2 NULL | wxm2 | NULL | 2 NULL | wxm3 | 0 | 1 NULL | wxm3 | 1 | 1 NULL | wxm3 | NULL | 2 NULL | wxm4 | 0 | 2 NULL | wxm4 | 2 | 1 NULL | wxm4 | NULL | 3 NULL | NULL | 0 | 5 NULL | NULL | 1 | 3 NULL | NULL | 2 | 1 NULL | NULL | 3 | 1 NULL | NULL | NULL | 10 (48 rows)Query 20190312_082310_01544_wwvpi, FINISHED, 2 nodesSplits: 4 total, 4 done (100.00%)0:00 [10 rows, 158B] [23 rows/s, 374B/s] 2的3次方减1 = 8个维度的整合 等价于8个union 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071presto:temp&gt; select * from ( -&gt; select id,null name,null area_id,count(*) count from wxm_test_fun group by id -&gt; union -&gt; select null,name,null,count(*) count from wxm_test_fun group by name -&gt; union -&gt; select null,null,area_id,count(*) count from wxm_test_fun group by area_id -&gt; union -&gt; select id,name,area_id,count(*) count from wxm_test_fun group by id,name,area_id -&gt; union -&gt; select id,name,null,count(*) count from wxm_test_fun group by id,name -&gt; union -&gt; select null,name,area_id,count(*) count from wxm_test_fun group by name,area_id -&gt; union -&gt; select id,null,area_id,count(*) count from wxm_test_fun group by id,area_id -&gt; union -&gt; select null,null,null,count(*) count from wxm_test_fun) order by id,name,area_id; id | name | area_id | count ------+------+---------+------- 1 | wxm1 | 0 | 1 1 | wxm1 | NULL | 1 1 | wxm2 | 1 | 1 1 | wxm2 | NULL | 1 1 | wxm4 | 0 | 1 1 | wxm4 | NULL | 1 1 | NULL | 0 | 2 1 | NULL | 1 | 1 1 | NULL | NULL | 3 2 | wxm1 | 0 | 1 2 | wxm1 | NULL | 1 2 | wxm2 | 1 | 1 2 | wxm2 | NULL | 1 2 | NULL | 0 | 1 2 | NULL | 1 | 1 2 | NULL | NULL | 2 3 | wxm3 | 0 | 1 3 | wxm3 | 1 | 1 3 | wxm3 | NULL | 2 3 | NULL | 0 | 1 3 | NULL | 1 | 1 3 | NULL | NULL | 2 4 | wxm4 | 0 | 1 4 | wxm4 | 2 | 1 4 | wxm4 | NULL | 2 4 | NULL | 0 | 1 4 | NULL | 2 | 1 4 | NULL | NULL | 2 5 | wxm1 | 3 | 1 5 | wxm1 | NULL | 1 5 | NULL | 3 | 1 5 | NULL | NULL | 1 NULL | wxm1 | 0 | 2 NULL | wxm1 | 3 | 1 NULL | wxm1 | NULL | 3 NULL | wxm2 | 1 | 2 NULL | wxm2 | NULL | 2 NULL | wxm3 | 0 | 1 NULL | wxm3 | 1 | 1 NULL | wxm3 | NULL | 2 NULL | wxm4 | 0 | 2 NULL | wxm4 | 2 | 1 NULL | wxm4 | NULL | 3 NULL | NULL | 0 | 5 NULL | NULL | 1 | 3 NULL | NULL | 2 | 1 NULL | NULL | 3 | 1 NULL | NULL | NULL | 10 (48 rows)Query 20190312_082943_01568_wwvpi, FINISHED, 2 nodesSplits: 42 total, 42 done (100.00%)0:01 [80 rows, 1.23KB] [101 rows/s, 1.57KB/s] ROLLUP 与CUBE不同，ROLLUP仅仅只展开第一层的维度聚合：分组的次数=待分组的字段数+1 cube会对所有聚合可能进行计算：CUBE（A,B,C），会计算group by y （ABC） union group by （AB） group by C union group by NULL 1234567891011121314151617181920212223242526272829presto:temp&gt; select name,area_id,group_id,count(*) count from wxm_test_fun group by rollup(name,area_id,group_id) order by name,area_id,group_id,count(*); name | area_id | group_id | count ------+---------+----------+------- wxm1 | 0 | 111 | 2 wxm1 | 0 | NULL | 2 wxm1 | 3 | 113 | 1 wxm1 | 3 | NULL | 1 wxm1 | NULL | NULL | 3 wxm2 | 1 | 112 | 2 wxm2 | 1 | NULL | 2 wxm2 | NULL | NULL | 2 wxm3 | 0 | 111 | 1 wxm3 | 0 | NULL | 1 wxm3 | 1 | 112 | 1 wxm3 | 1 | NULL | 1 wxm3 | NULL | NULL | 2 wxm4 | 0 | 111 | 1 wxm4 | 0 | 112 | 1 wxm4 | 0 | NULL | 2 wxm4 | 2 | 111 | 1 wxm4 | 2 | NULL | 1 wxm4 | NULL | NULL | 3 NULL | NULL | NULL | 10 (20 rows)Query 20190312_085457_01652_wwvpi, FINISHED, 2 nodesSplits: 4 total, 4 done (100.00%)0:00 [10 rows, 158B] [29 rows/s, 458B/s] 等价于3 + 1 = 4个union聚合 123456789101112131415161718192021222324252627282930313233343536presto:temp&gt; select * from ( -&gt; select name,null area_id,null group_id,count(*) count from wxm_test_fun group by name -&gt; union -&gt; select name,area_id,null group_id,count(*) count from wxm_test_fun group by name,area_id -&gt; union -&gt; select name,area_id,group_id,count(*) count from wxm_test_fun group by name,area_id,group_id -&gt; union -&gt; select null name,null area_id,null group_id,count(*) count from wxm_test_fun) order by name,area_id,group_id; name | area_id | group_id | count ------+---------+----------+------- wxm1 | 0 | 111 | 2 wxm1 | 0 | NULL | 2 wxm1 | 3 | 113 | 1 wxm1 | 3 | NULL | 1 wxm1 | NULL | NULL | 3 wxm2 | 1 | 112 | 2 wxm2 | 1 | NULL | 2 wxm2 | NULL | NULL | 2 wxm3 | 0 | 111 | 1 wxm3 | 0 | NULL | 1 wxm3 | 1 | 112 | 1 wxm3 | 1 | NULL | 1 wxm3 | NULL | NULL | 2 wxm4 | 0 | 111 | 1 wxm4 | 0 | 112 | 1 wxm4 | 0 | NULL | 2 wxm4 | 2 | 111 | 1 wxm4 | 2 | NULL | 1 wxm4 | NULL | NULL | 3 NULL | NULL | NULL | 10 (20 rows)Query 20190312_090034_01674_wwvpi, FINISHED, 2 nodesSplits: 22 total, 22 done (100.00%)0:01 [40 rows, 632B] [50 rows/s, 795B/s] GROUPING SETS 对任意指定分组进行聚合 GROUPING SETS（（AB）,（A）,（B），（）），仅仅group by （AB） union group by A union group by B union group by null 1234567891011121314151617181920212223242526presto:temp&gt; select id,name,area_id,count(*) count from wxm_test_fun group by grouping sets((id,name),name,area_id,()) order by id,name,area_id; id | name | area_id | count ------+------+---------+------- 1 | wxm1 | NULL | 1 1 | wxm2 | NULL | 1 1 | wxm4 | NULL | 1 2 | wxm1 | NULL | 1 2 | wxm2 | NULL | 1 3 | wxm3 | NULL | 2 4 | wxm4 | NULL | 2 5 | wxm1 | NULL | 1 NULL | wxm1 | NULL | 3 NULL | wxm2 | NULL | 2 NULL | wxm3 | NULL | 2 NULL | wxm4 | NULL | 3 NULL | NULL | 0 | 5 NULL | NULL | 1 | 3 NULL | NULL | 2 | 1 NULL | NULL | 3 | 1 NULL | NULL | NULL | 10 (17 rows)Query 20190312_091338_01714_wwvpi, FINISHED, 2 nodesSplits: 4 total, 4 done (100.00%)0:00 [10 rows, 158B] [30 rows/s, 485B/s] 等价于grouping sets中的字段union group by 123456789101112131415161718192021222324252627282930313233presto:temp&gt; select * from ( -&gt; select id,name,null area_id,count(*) count from wxm_test_fun group by id,name -&gt; union -&gt; select null id,name,null area_id,count(*) count from wxm_test_fun group by name -&gt; union -&gt; select null id,null name,area_id,count(*) count from wxm_test_fun group by area_id -&gt; union -&gt; select null id,null name,null area_id,count(*) count from wxm_test_fun) order by id,name,area_id; id | name | area_id | count ------+------+---------+------- 1 | wxm1 | NULL | 1 1 | wxm2 | NULL | 1 1 | wxm4 | NULL | 1 2 | wxm1 | NULL | 1 2 | wxm2 | NULL | 1 3 | wxm3 | NULL | 2 4 | wxm4 | NULL | 2 5 | wxm1 | NULL | 1 NULL | wxm1 | NULL | 3 NULL | wxm2 | NULL | 2 NULL | wxm3 | NULL | 2 NULL | wxm4 | NULL | 3 NULL | NULL | 0 | 5 NULL | NULL | 1 | 3 NULL | NULL | 2 | 1 NULL | NULL | 3 | 1 NULL | NULL | NULL | 10 (17 rows)Query 20190312_091542_01721_wwvpi, FINISHED, 2 nodesSplits: 22 total, 22 done (100.00%)0:00 [40 rows, 632B] [85 rows/s, 1.33KB/s] 维度信息获取 通常情况下，为了group by 的灵活性，通常会选用grouping sets，这就会出现一个维度标志问题。并不知道这个聚合数据哪个维度，通常可以用case when语句解决 以此句为例 1select id,name,area_id,count(*) count from wxm_test_fun group by grouping sets((id,name),name,area_id,()) order by id,name,area_id; (id,name)维度为维度01，name为维度02，area_id为维度03，()为全局维度 1234567891011121314151617181920212223242526272829303132presto:temp&gt; select -&gt; case when id is not null and name is not null and area_id is null then '维度01' -&gt; when id is null and name is not null and area_id is null then '维度02' -&gt; when id is null and name is null and area_id is not null then '维度03' -&gt; when id is null and name is null and area_id is null then '全局维度' -&gt; end -&gt; agg_code,id,name,area_id,count(*) count from wxm_test_fun group by grouping sets((id,name),name,area_id,()) order by id,name,area_id; agg_code | id | name | area_id | count ----------+------+------+---------+------- 维度01 | 1 | wxm1 | NULL | 1 维度01 | 1 | wxm2 | NULL | 1 维度01 | 1 | wxm4 | NULL | 1 维度01 | 2 | wxm1 | NULL | 1 维度01 | 2 | wxm2 | NULL | 1 维度01 | 3 | wxm3 | NULL | 2 维度01 | 4 | wxm4 | NULL | 2 维度01 | 5 | wxm1 | NULL | 1 维度02 | NULL | wxm1 | NULL | 3 维度02 | NULL | wxm2 | NULL | 2 维度02 | NULL | wxm3 | NULL | 2 维度02 | NULL | wxm4 | NULL | 3 维度03 | NULL | NULL | 0 | 5 维度03 | NULL | NULL | 1 | 3 维度03 | NULL | NULL | 2 | 1 维度03 | NULL | NULL | 3 | 1 全局维度 | NULL | NULL | NULL | 10 (17 rows)Query 20190312_092301_01746_wwvpi, FINISHED, 2 nodesSplits: 4 total, 4 done (100.00%)0:01 [10 rows, 158B] [15 rows/s, 242B/s] 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>BigData</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache HTTP Server 编译与安装]]></title>
    <url>%2F2019%2F03%2F01%2FApache-HTTP-Server-Compile-Install%2F</url>
    <content type="text"><![CDATA[在使用ApacheBench做压力测试的时候出现30srequest超时的情况，由于版本老旧，ab -h中发现并没有-s timeout的参数配置，Google上找了半天也没有可用的Binaries版本，于是开始自己编译httpd源码 ApacheBench Doc 文档 针对于ab timeout的问题(报错：The timeout specified has expired (70007))，加了-k 参数保证keepalived参数也无用，在2.4.4+版本中加入了-s参数控制timeout； 12-s timeoutMaximum number of seconds to wait before the socket times out. Default is 30 seconds. Available in 2.4.4 and later. SoftWare Version Ubuntu Server x64 14.04 pcre 8.38 apr-util 1.6.1 apr 1.6.5 httpd 2.4.38 gcc &amp; gcc-c++ 5.4.0 gcc &amp; gcc-c++ 系统自带；如果自己本机没有需要先install下 pcre 编译 pcre 下载 12345678unzip pcre-8.38.zip cd pcre-8.38# 指定编译目录./configure --prefix=/usr/local/pcre# 编译安装sudo make &amp; sudo make install 注意在make 失败后，一定要使用make clean命令清空环境 错误1 1xml/apr_xml.c:35:19: fatal error: expat.h: No such file or directory 解决 Centos安装：yum install expat-devel ubuntu安装：sudo apt-get install libexpat1-dev apr &amp; apr-util 编译 apr &amp; apr-util 下载 httpd 下载 准备工作1234567tar -zxvf apr-util-1.6.1.tar.gztar -zxvf apr-1.6.5.tar.gztar -zxvf httpd-2.4.38.tar.gz# 一定要复制到httpd的srclib目录下，且目录版本号去掉cp -r apr-1.6.5 httpd-2.4.38/srclib/aprcp -r apr-util-1.6.1 httpd-2.4.38/srclib/apr-util apr 编译12345678# apr 编译cd httpd-2.4.38/srclib/apr# 指定编译目录./configure --prefix=/usr/local/apr# 编译安装sudo make &amp; sudo make install apr-util 编译12345678# apr 编译cd httpd-2.4.38/srclib/apr-util# 指定编译目录 并 指定apr依赖./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/apr # 编译安装sudo make &amp; sudo make install httpd 编译1234567cd httpd-2.4.38# 指定编译目录 并 指定apr、apr-util、pcre依赖./configure --prefix=/usr/local/apache-httpd --with-apr=/usr/local/apr --with-apr-util=/usr/local/apr-util --with-pcre=/usr/local/pcre# 编译安装（这个过程比较长，耐心等待）sudo make &amp; sudo make install 错误2 1recipe for target 'htpasswd' failed 解决 是因为apr、apr-util源码没有在httpd-2.4.38/srclib/目录下，cp过去就可以（注意：只需目录名，无需版本号） 测试安装成功 ApacheBench 测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 发现已经有了 -s 参数/usr/local/apache-httpd/bin/ab -hUsage: ./ab [options] [http[s]://]hostname[:port]/pathOptions are: -n requests Number of requests to perform -c concurrency Number of multiple requests to make at a time -t timelimit Seconds to max. to spend on benchmarking This implies -n 50000 -s timeout Seconds to max. wait for each response Default is 30 seconds -b windowsize Size of TCP send/receive buffer, in bytes -B address Address to bind to when making outgoing connections -p postfile File containing data to POST. Remember also to set -T -u putfile File containing data to PUT. Remember also to set -T -T content-type Content-type header to use for POST/PUT data, eg. 'application/x-www-form-urlencoded' Default is 'text/plain' -v verbosity How much troubleshooting info to print -w Print out results in HTML tables -i Use HEAD instead of GET -x attributes String to insert as table attributes -y attributes String to insert as tr attributes -z attributes String to insert as td or th attributes -C attribute Add cookie, eg. 'Apache=1234'. (repeatable) -H attribute Add Arbitrary header line, eg. 'Accept-Encoding: gzip' Inserted after all normal header lines. (repeatable) -A attribute Add Basic WWW Authentication, the attributes are a colon separated username and password. -P attribute Add Basic Proxy Authentication, the attributes are a colon separated username and password. -X proxy:port Proxyserver and port number to use -V Print version number and exit -k Use HTTP KeepAlive feature -d Do not show percentiles served table. -S Do not show confidence estimators and warnings. -q Do not show progress when doing more than 150 requests -l Accept variable document length (use this for dynamic pages) -g filename Output collected data to gnuplot format file. -e filename Output CSV file with percentages served -r Don't exit on socket receive errors. -m method Method name -h Display usage information (this message) -I Disable TLS Server Name Indication (SNI) extension -Z ciphersuite Specify SSL/TLS cipher suite (See openssl ciphers) -f protocol Specify SSL/TLS protocol (TLS1, TLS1.1, TLS1.2 or ALL) -E certfile Specify optional client certificate chain and private key 服务测试 1234567891011sudovim /usr/local/apache-httpd/conf/httpd.conf# 添加ServerNameServerName localhost:80sudo /usr/local/apache-httpd/bin/apachectl startcurl localhost&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;sudo /usr/local/apache-httpd/bin/apachectl stop 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Apache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Phoenix RowTimestamp]]></title>
    <url>%2F2019%2F02%2F25%2FApache-Phoenix-Row-Timestamp%2F</url>
    <content type="text"><![CDATA[从4.6版本起，Phoenix支持一种时间戳映射到列的方法（即HBase中 column 的RowTimestamp），以便对Phoenix在存储时序数据时查询的优化 Row Timestamp介绍 在建表时对特定列指定ROW_TIMESTAMP关键字： 只有当parimary key为以下类型时才可以使用ROW_TIMESTAMP：TIME, DATE, TIMESTAMP, BIGINT, UNSIGNED_LONG 若primary key为组合主键的时候，只有一个column能指定为ROW_TIMESTAMP 此列的值不能为null 此列的值不能为负 当使用UPSERT VALUES or UPSERT SELECT语句更新数据时，可以指定ROW_TIMESTAMP的值，如果不指定则默认以服务器时间为此列的时间戳，同时这个时间戳也对应hbase 中row的timestamp 当查询过滤ROW_TIMESTAMP时，不仅可以做常规的查询优化，同时可以依靠timestamp对数据进行最大最小的范围优化，hbase服务器端可以直接跳过不在时间区间内的hfile文件从而使扫描效率提高，尤其是查询数据尾端的时候 Phenix RowTimestap Doc SoftWare Version Hbase 1.2.0-cdh5.11.1 Phoenix 4.13.0-cdh5.11.1 Java 1.8.0_121 部署环境：4 Region Servers 测试用例12345678910111213141516171819202122232425262728CREATE TABLE phoenix_dev.test_ywzx_wuhan_switch ( event_time TIMESTAMP NOT NULL /*event_time*/, "_KEY" BIGINT NOT NULL /*_key*/, switch_output_multicastpkts_delta BIGINT /*switch_output_multicastpkts_delta*/, switch_output_broadcastpkts_delta BIGINT /*switch_output_broadcastpkts_delta*/, switch_input_errors_delta BIGINT /*switch_input_errors_delta*/, switch_cpu_utilization BIGINT /*switch_cpu_utilization*/, switch_sysobjectid VARCHAR /*switch_sysobjectid*/, switch_memory_utilization FLOAT /*switch_memory_utilization*/, switch_input_multicastpkts_delta BIGINT /*switch_input_multicastpkts_delta*/, switch_input_bytes_delta BIGINT /*switch_input_bytes_delta*/, switch_output_errors_delta BIGINT /*switch_output_errors_delta*/, switch_output_bytes_delta BIGINT /*switch_output_bytes_delta*/, switch_port_status VARCHAR /*switch_port_status*/, switch_output_ucastpkts_delta BIGINT /*switch_output_ucastpkts_delta*/, switch_portname VARCHAR /*switch_portname*/, switch_fan_status VARCHAR /*switch_fan_status*/, switch_index VARCHAR /*switch_index*/, switch_host VARCHAR /*switch_host*/, switch_gims_measurement VARCHAR /*switch_gims_measurement*/, switch_power_status VARCHAR /*switch_power_status*/, switch_input_broadcastpkts_delta BIGINT /*switch_input_broadcastpkts_delta*/, switch_input_ucastpkts_delta BIGINT /*switch_input_ucastpkts_delta*/, switch_errorstatus INTEGER /*switch_errorstatus*/, CONSTRAINT pk PRIMARY KEY (event_time ASC, "_KEY" ASC)) COMPRESSION = SNAPPY, SALT_BUCKETS = 4, VERSIONS = 1 1234567891011121314151617181920212223242526272829UPSERT INTO PHOENIX_DEV.test_ywzx_wuhan_switch ( event_time, _KEY, switch_output_multicastpkts_delta, switch_output_broadcastpkts_delta, switch_input_errors_delta, switch_cpu_utilization, switch_sysobjectid, switch_memory_utilization, switch_input_multicastpkts_delta, switch_input_bytes_delta, switch_output_errors_delta, switch_output_bytes_delta, switch_port_status, switch_output_ucastpkts_delta, switch_portname, switch_fan_status, switch_index, switch_host, switch_gims_measurement, switch_power_status, switch_input_broadcastpkts_delta, switch_input_ucastpkts_delta, switch_errorstatus)VALUES ( ?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,? ) primary key 组成：CONSTRAINT pk PRIMARY KEY (event_time ASC, “_KEY” ASC) 建立一张表结构完全一样的表test_ywzx_wuhan_switch_rowtimestap，给event_time字段添加ROW_TIMESTAMP 即给时间主键设置ROW_TIMESTAMP 数据量及大小 2000w条数据压缩后size：3.5 G 10.4 G /hbase/data/PHOENIX_DEV/TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP event_time时间范围： 2018-10-21 14:03:34.503 —— 2018-12-06 21:10:14.303 自动split一次，由于salt = 4，所以写入始终向4 Region Servers，查询旧数据则扫描前4个RS，查询新数据扫描后4个RS，全表查询则使用8个RS select count(*) from PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH where event_time &gt;= to_timestamp(&#39;2018-10-21 00:00:00&#39;) AND event_time &lt;= to_timestamp(&#39;2018-12-05 00:00:00&#39;)size = 1900w+ 测试语句一 无ROW_TIMESTAMP 1234567select count(*) from PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH;+-----------+| COUNT(1) |+-----------+| 20000000 |+-----------+ 12345678// explain+-------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| PLAN | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |+-------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| CLIENT 72-CHUNK 18453404 ROWS 20132661420 BYTES PARALLEL 4-WAY FULL SCAN OVER PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH | 20132661420 | 18453404 | 1550646317452 || SERVER FILTER BY FIRST KEY ONLY | 20132661420 | 18453404 | 1550646317452 || SERVER AGGREGATE INTO SINGLE ROW | 20132661420 | 18453404 | 1550646317452 |+-------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+ 第N次执行 耗时 1 6.159 seconds 2 5.537 seconds 3 4.686 seconds 4 4.429 seconds 5 4.943 seconds 有ROW_TIMESTAMP 1234567select count(*) from PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP;+-----------+| COUNT(1) |+-----------+| 20000000 |+-----------+ 123456789// explain+--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| PLAN | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |+--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| CLIENT 52-CHUNK 12686716 ROWS 13841204812 BYTES PARALLEL 4-WAY FULL SCAN OVER PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP | 13841204812 | 12686716 | 1550647235912 || ROW TIMESTAMP FILTER [0, 9223372036854775807) | 13841204812 | 12686716 | 1550647235912 || SERVER FILTER BY FIRST KEY ONLY | 13841204812 | 12686716 | 1550647235912 || SERVER AGGREGATE INTO SINGLE ROW | 13841204812 | 12686716 | 1550647235912 |+--------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+ 第N次执行 耗时 1 21.726 seconds 2 20.906 seconds 3 19.494 seconds 4 19.514 seconds 5 19.478 seconds 测试语句二 无ROW_TIMESTAMP 1select count(SWITCH_HOST),SWITCH_HOST,count(SWITCH_POWER_STATUS),SWITCH_POWER_STATUS from PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH where event_time &gt;= to_timestamp('2018-10-21 00:00:00') AND event_time &lt;= to_timestamp('2018-12-05 00:00:00') group by SWITCH_HOST,SWITCH_POWER_STATUS limit 5000; 123456789// explain+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| PLAN | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| CLIENT 72-CHUNK 18453404 ROWS 20132661420 BYTES PARALLEL 72-WAY RANGE SCAN OVER PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH [0,'2018-10-20 16:00:00.000'] - [3,'2018-12-04 16:00:00.000'] | 20132661420 | 18453404 | 1550646317452 || SERVER AGGREGATE INTO DISTINCT ROWS BY [SWITCH_HOST, SWITCH_POWER_STATUS] | 20132661420 | 18453404 | 1550646317452 || CLIENT MERGE SORT | 20132661420 | 18453404 | 1550646317452 || CLIENT 5000 ROW LIMIT | 20132661420 | 18453404 | 1550646317452 |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+ 第N次执行 耗时 1 5.714 seconds 2 5.355 seconds 3 4.98 seconds 4 4.922 seconds 5 5.02 seconds 有ROW_TIMESTAMP 1select count(SWITCH_HOST),SWITCH_HOST,count(SWITCH_POWER_STATUS),SWITCH_POWER_STATUS from PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP where event_time &gt;= to_timestamp('2018-10-21 00:00:00') AND event_time &lt;= to_timestamp('2018-12-05 00:00:00') group by SWITCH_HOST,SWITCH_POWER_STATUS limit 5000; 12345678910// explain+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| PLAN | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| CLIENT 52-CHUNK 12686716 ROWS 13841204812 BYTES PARALLEL 52-WAY RANGE SCAN OVER PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP [0,'2018-10-20 16:00:00.000'] - [3,'2018-12-04 16:00:00.000'] | 13841204812 | 12686716 | 1550647235912 || ROW TIMESTAMP FILTER [1540051200000, 1543939200001) | 13841204812 | 12686716 | 1550647235912 || SERVER AGGREGATE INTO DISTINCT ROWS BY [SWITCH_HOST, SWITCH_POWER_STATUS] | 13841204812 | 12686716 | 1550647235912 || CLIENT MERGE SORT | 13841204812 | 12686716 | 1550647235912 || CLIENT 5000 ROW LIMIT | 13841204812 | 12686716 | 1550647235912 |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+ 第N次执行 耗时 1 19.391 seconds 2 18.249 seconds 3 18.195 seconds 4 18.567 seconds 5 18.247 seconds 测试语句三 无ROW_TIMESTAMP 1SELECT * FROM (SELECT CAST(floor(event_time, 'MINUTE', 1) AS timestamp) AS log_time, switch_host, MAX(switch_cpu_utilization) AS cpu_utilization FROM PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH AS ywzx_wuhan_switch WHERE switch_gims_measurement = 'ddd' AND switch_host = 'fff' AND ywzx_wuhan_switch.event_time &gt;= to_timestamp('2018-10-21 00:00:00') AND ywzx_wuhan_switch.event_time &lt;= to_timestamp('2018-12-05 00:00:00') GROUP BY log_time, switch_host ORDER BY switch_host,log_time desc) LIMIT 5000 offset 0; 12345678910// explain +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| PLAN | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| CLIENT 72-CHUNK 18453404 ROWS 20132661420 BYTES PARALLEL 72-WAY RANGE SCAN OVER PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH [0,'2018-10-20 16:00:00.000'] - [3,'2018-12-04 16:00:00.000'] | 20132661420 | 18453404 | 1550646317452 || SERVER FILTER BY (SWITCH_GIMS_MEASUREMENT = 'ddd' AND SWITCH_HOST = 'fff') | 20132661420 | 18453404 | 1550646317452 || SERVER AGGREGATE INTO DISTINCT ROWS BY [TO_TIMESTAMP(FLOOR(TO_DATE(EVENT_TIME))), SWITCH_HOST] | 20132661420 | 18453404 | 1550646317452 || CLIENT MERGE SORT | 20132661420 | 18453404 | 1550646317452 || CLIENT TOP 5000 ROWS SORTED BY [SWITCH_HOST, TO_TIMESTAMP(FLOOR(TO_DATE(EVENT_TIME))) DESC] | 20132661420 | 18453404 | 1550646317452 |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+ 第N次执行 耗时 1 6.917 seconds 2 5.914 seconds 3 5.929 seconds 4 6.091 seconds 5 6.135 seconds 有ROW_TIMESTAMP 1SELECT * FROM (SELECT CAST(floor(event_time, 'MINUTE', 1) AS timestamp) AS log_time, switch_host, MAX(switch_cpu_utilization) AS cpu_utilization FROM PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP AS ywzx_wuhan_switch WHERE switch_gims_measurement = 'ddd' AND switch_host = 'fff' AND ywzx_wuhan_switch.event_time &gt;= to_timestamp('2018-10-21 00:00:00') AND ywzx_wuhan_switch.event_time &lt;= to_timestamp('2018-12-05 00:00:00') GROUP BY log_time, switch_host ORDER BY switch_host,log_time desc) LIMIT 5000 offset 0; 1234567891011// explain +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| PLAN | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| CLIENT 52-CHUNK 12686716 ROWS 13841204812 BYTES PARALLEL 52-WAY RANGE SCAN OVER PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP [0,'2018-10-20 16:00:00.000'] - [3,'2018-12-04 16:00:00.000'] | 13841204812 | 12686716 | 1550647235912 || ROW TIMESTAMP FILTER [1540051200000, 1543939200001) | 13841204812 | 12686716 | 1550647235912 || SERVER FILTER BY (SWITCH_GIMS_MEASUREMENT = 'ddd' AND SWITCH_HOST = 'fff') | 13841204812 | 12686716 | 1550647235912 || SERVER AGGREGATE INTO DISTINCT ROWS BY [TO_TIMESTAMP(FLOOR(TO_DATE(EVENT_TIME))), SWITCH_HOST] | 13841204812 | 12686716 | 1550647235912 || CLIENT MERGE SORT | 13841204812 | 12686716 | 1550647235912 || CLIENT TOP 5000 ROWS SORTED BY [SWITCH_HOST, TO_TIMESTAMP(FLOOR(TO_DATE(EVENT_TIME))) DESC] | 13841204812 | 12686716 | 1550647235912 |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+ 第N次执行 耗时 1 19.875 seconds 2 20.301 seconds 3 18.88 seconds 4 19.706 seconds 5 19.302 seconds 测试语句四 无ROW_TIMESTAMP 1234567SELECT count(*) FROM (SELECT CAST(floor(event_time, 'MINUTE', 1) AS timestamp) AS log_time, switch_host, MAX(switch_cpu_utilization) AS cpu_utilization FROM PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH AS TEST_YWZX_WUHAN_SWITCH WHERE switch_gims_measurement = 'ddd' AND switch_host = 'fff' AND TEST_YWZX_WUHAN_SWITCH.event_time &gt;= to_timestamp('2018-10-21 00:00:00') AND TEST_YWZX_WUHAN_SWITCH.event_time &lt;= to_timestamp('2018-12-05 00:00:00') GROUP BY CAST(floor(event_time, 'MINUTE', 1) AS timestamp), switch_host);+-----------+| COUNT(1) |+-----------+| 60357 |+-----------+ 12345678910// explain +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| PLAN | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| CLIENT 72-CHUNK 18453404 ROWS 20132661420 BYTES PARALLEL 72-WAY RANGE SCAN OVER PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH [0,'2018-10-20 16:00:00.000'] - [3,'2018-12-04 16:00:00.000'] | 20132661420 | 18453404 | 1550646317452 || SERVER FILTER BY (SWITCH_GIMS_MEASUREMENT = 'ddd' AND SWITCH_HOST = 'fff') | 20132661420 | 18453404 | 1550646317452 || SERVER AGGREGATE INTO DISTINCT ROWS BY [TO_TIMESTAMP(FLOOR(TO_DATE(EVENT_TIME))), SWITCH_HOST] | 20132661420 | 18453404 | 1550646317452 || CLIENT MERGE SORT | 20132661420 | 18453404 | 1550646317452 || CLIENT AGGREGATE INTO SINGLE ROW | 20132661420 | 18453404 | 1550646317452 |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+ 第N次执行 耗时 1 6.888 seconds 2 5.896 seconds 3 5.263 seconds 4 5.576 seconds 5 5.665 seconds 有ROW_TIMESTAMP 1234567 SELECT count(*) FROM (SELECT CAST(floor(event_time, 'MINUTE', 1) AS timestamp) AS log_time, switch_host, MAX(switch_cpu_utilization) AS cpu_utilization FROM PHOENIX_DEV.TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP AS TEST_YWZX_WUHAN_SWITCH WHERE switch_gims_measurement = 'ddd' AND switch_host = 'fff' AND TEST_YWZX_WUHAN_SWITCH.event_time &gt;= to_timestamp('2018-10-21 00:00:00') AND TEST_YWZX_WUHAN_SWITCH.event_time &lt;= to_timestamp('2018-12-05 00:00:00') GROUP BY CAST(floor(event_time, 'MINUTE', 1) AS timestamp), switch_host); +-----------+| COUNT(1) |+-----------+| 60332 |+-----------+ 1234567891011// explain +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| PLAN | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+| CLIENT 52-CHUNK 12686716 ROWS 13841204812 BYTES PARALLEL 52-WAY RANGE SCAN OVER PHOENIX_DEV:TEST_YWZX_WUHAN_SWITCH_ROWTIMESTAMP [0,'2018-10-20 16:00:00.000'] - [3,'2018-12-04 16:00:00.000'] | 13841204812 | 12686716 | 1550647235912 || ROW TIMESTAMP FILTER [1540051200000, 1543939200001) | 13841204812 | 12686716 | 1550647235912 || SERVER FILTER BY (SWITCH_GIMS_MEASUREMENT = 'ddd' AND SWITCH_HOST = 'fff') | 13841204812 | 12686716 | 1550647235912 || SERVER AGGREGATE INTO DISTINCT ROWS BY [TO_TIMESTAMP(FLOOR(TO_DATE(EVENT_TIME))), SWITCH_HOST] | 13841204812 | 12686716 | 1550647235912 || CLIENT MERGE SORT | 13841204812 | 12686716 | 1550647235912 || CLIENT AGGREGATE INTO SINGLE ROW | 13841204812 | 12686716 | 1550647235912 |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+----------------+----------------+ 第N次执行 耗时 1 19.967 seconds 2 18.854 seconds 3 19.637 seconds 4 18.701 seconds 5 18.986 seconds [JIRA 问题追踪](https://issues.apache.org/jira/browse/PHOENIX-5157) 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>Phoenix</category>
        <category>HBase</category>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Phoenix</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 优雅的统计表（分区）Size and RowCount]]></title>
    <url>%2F2019%2F02%2F21%2FHive-Table-Statistics%2F</url>
    <content type="text"><![CDATA[在Hadoop平台运维监控中，Hive表的统计信息通常是作为集群数据质量的关键依据；通常以Hive表的大小、行数、分区数等信息来衡量集群数据量的增减趋势；本文以Hive表Size大小和数据行数作为重点，对于超大表统计时避免过多的资源消耗 两种常规方式统计： 方式一： size 统计：1long size = FileSystem.getContentSummary(new Path(tablePath)).getLength(); rowCount统计： 1select count(*) from tableName; 方式二： size统计： 1hadoop fs -du -s tablePath rowCount统计（非Orc、Parquet）： 1hadoop fs -text tablePath | wc -l 常规方式在获取rowCount时都有局限性（例如OrcFile无法用hadoop fs -text），并且效率不高 ANALYZE 方式统计 ANALYZE TABLE tablename [PARTITION(partcol1[=val1], partcol2[=val2], …)] COMPUTE STATISTICS [noscan]; 考虑到表的统计值通常是要通过计算后记录到某个地方，执行ANALYZE的时候才会快速显示；为此探究什么情况下导入Hive数据，ANALYZE的结果准确 下文会以TextFile、SequenceFile、OrcFile、ParquetFile，四种常见的Hive表文件格式来做测试 Hive数据导入途径： 1. Load file to hive table 2. Add partition and mv file to partition path 3. Insert into 测试数据文件 123456789101112131415161718192021// 数据文件 count = 201 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio21 wxmimperio12 wxmimperio2 TextFile123456// create table CREATE TABLE `analyze_text` (`id` string, `name` string) COMMENT 'test' PARTITIONED BY (`part_date` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ( 'field.delim' = '\t', 'serialization.format' = '\t') STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'; Load file to table 123456hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/wxm/test.txt' OVERWRITE INTO TABLE analyze_text partition(part_date='2019-02-21');Loading data to table dw.analyze_text partition (part_date=2019-02-21)Partition dw.analyze_text&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=0, totalSize=298, rawDataSize=0]OKTime taken: 1.035 seconds 12345hive&gt; ANALYZE TABLE analyze_text PARTITION(part_date='2019-02-21') COMPUTE STATISTICS noscan;Partition dw.analyze_text&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=0, totalSize=298, rawDataSize=0]OKTime taken: 0.463 seconds Load 操作实际上是执行了 mv 操作，将文件移动到表目录下面；ANALYZE 只能查看到numFiles（文件数）和totalSize（分区总大小） Add partition and mv file to partition path 12345// add partitionalter table analyze_text add partition(part_date='2019-02-20');// cp fileshadoop fs -cp hdfs://sdg/user/hive/warehouse/dw.db/analyze_text/part_date=2019-02-21/* hdfs://sdg/user/hive/warehouse/dw.db/analyze_text/part_date=2019-02-20/ 12345hive&gt; ANALYZE TABLE analyze_text PARTITION(part_date='2019-02-20') COMPUTE STATISTICS noscan;Partition dw.analyze_text&#123;part_date=2019-02-20&#125; stats: [numFiles=1, totalSize=298]OKTime taken: 0.202 seconds 和Load操作一样，ANALYZE 只能查看到numFiles（文件数）和totalSize（分区总大小） Insert into 123456789101112131415161718192021222324hive&gt; insert into table analyze_text partition(part_date='2019-02-19') select `id`,`name` from analyze_text_copy where part_date='2019-02-21';Query ID = hadoop_20190221143232_fbc19f00-d0af-4278-a644-924c92994a75Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_1535945194143_1278, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1278/Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job -kill job_1535945194143_1278Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02019-02-21 14:32:54,087 Stage-1 map = 0%, reduce = 0%2019-02-21 14:33:01,381 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.3 secMapReduce Total cumulative CPU time: 3 seconds 300 msecEnded Job = job_1535945194143_1278Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://sdg/user/hive/warehouse/dw.db/analyze_text/part_date=2019-02-19/.hive-staging_hive_2019-02-21_14-32-46_291_8677282162206970479-1/-ext-10000Loading data to table dw.analyze_text partition (part_date=2019-02-19)Partition dw.analyze_text&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=280, rawDataSize=260]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 3.3 sec HDFS Read: 3699 HDFS Write: 373 SUCCESSTotal MapReduce CPU Time Spent: 3 seconds 300 msecOKTime taken: 20.567 seconds 12345hive&gt; ANALYZE TABLE analyze_text PARTITION(part_date='2019-02-19') COMPUTE STATISTICS noscan;Partition dw.analyze_text&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=280, rawDataSize=260]OKTime taken: 0.197 seconds 由于insert into 用了 MapReduce，在计算的过程中就已经将表的统计信息记录了下来，所以numRows、rawDataSize都有 SequenceFile123456// create table CREATE TABLE `analyze_sequence_test` (`id` string, `name` string) PARTITIONED BY (`part_date` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ( 'field.delim' = '\t', 'serialization.format' = '\t') STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'; Load file to table 123456hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/wxm/analyze_sequence_test_file' OVERWRITE INTO TABLE analyze_sequence_test partition(part_date='2019-02-21');Loading data to table dw.analyze_sequence_test partition (part_date=2019-02-21)Partition dw.analyze_sequence_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=0, totalSize=607, rawDataSize=0]OKTime taken: 1.525 seconds 12345hive&gt; ANALYZE TABLE analyze_sequence_test PARTITION(part_date='2019-02-21') COMPUTE STATISTICS noscan;Partition dw.analyze_sequence_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=0, totalSize=607, rawDataSize=0]OKTime taken: 0.547 seconds stats: [numFiles=1, numRows=0, totalSize=607, rawDataSize=0] Add partition and mv file to partition path 12345// add partitionalter table analyze_sequence_test add partition(part_date='2019-02-20');// cp fileshadoop fs -cp hdfs://sdg/user/hive/warehouse/dw.db/analyze_sequence_test/part_date=2019-02-19/* hdfs://sdg/user/hive/warehouse/dw.db/analyze_sequence_test/part_date=2019-02-20/ 12345ANALYZE TABLE analyze_sequence_test PARTITION(part_date='2019-02-20') COMPUTE STATISTICS noscan;Partition dw.analyze_sequence_test&#123;part_date=2019-02-20&#125; stats: [numFiles=1, totalSize=607]OKTime taken: 0.875 seconds 由于sequenceFile文件的head中没有存储row的相关信息，所以获取不到 Insert into 123456789101112131415161718192021222324hive&gt; insert into table analyze_sequence_test partition(part_date='2019-02-19') select `id`,`name` from analyze_text_copy where part_date='2019-02-21';Query ID = hadoop_20190221144141_f9ffbdc3-9e68-43d4-af7c-b8f883e84d3aTotal jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_1535945194143_1279, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1279/Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job -kill job_1535945194143_1279Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02019-02-21 14:41:12,146 Stage-1 map = 0%, reduce = 0%2019-02-21 14:41:18,387 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.94 secMapReduce Total cumulative CPU time: 2 seconds 940 msecEnded Job = job_1535945194143_1279Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://sdg/user/hive/warehouse/dw.db/analyze_sequence_test/part_date=2019-02-19/.hive-staging_hive_2019-02-21_14-41-04_329_3570329183914133571-1/-ext-10000Loading data to table dw.analyze_sequence_test partition (part_date=2019-02-19)Partition dw.analyze_sequence_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=607, rawDataSize=260]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 2.94 sec HDFS Read: 3954 HDFS Write: 709 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 940 msecOKTime taken: 20.673 seconds 12345hive&gt; ANALYZE TABLE analyze_sequence_test PARTITION(part_date='2019-02-19') COMPUTE STATISTICS noscan;Partition dw.analyze_sequence_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=607, rawDataSize=260]OKTime taken: 0.191 seconds stats: [numFiles=1, numRows=20, totalSize=607, rawDataSize=260] OrcFile1234// create tableCREATE TABLE `analyze_orc_test` (`id` string, `name` string) PARTITIONED BY (`part_date` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'; Load file to table 123456hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/wxm/analyze_orc_test_file' OVERWRITE INTO TABLE analyze_orc_test partition(part_date='2019-02-21');Loading data to table dw.analyze_orc_test partition (part_date=2019-02-21)Partition dw.analyze_orc_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=0, totalSize=365, rawDataSize=0]OKTime taken: 0.991 seconds 12345hive&gt; ANALYZE TABLE analyze_orc_test PARTITION(part_date='2019-02-21') COMPUTE STATISTICS noscan;Partition dw.analyze_orc_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600]OKTime taken: 0.661 seconds stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600] Add partition and mv file to partition path 12345// add partitionalter table analyze_orc_test add partition(part_date='2019-02-20');// cp fileshadoop fs -cp hdfs://sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/* hdfs://sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-20/ 12345hive&gt; ANALYZE TABLE analyze_orc_test PARTITION(part_date='2019-02-20') COMPUTE STATISTICS noscan;Partition dw.analyze_orc_test&#123;part_date=2019-02-20&#125; stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600]OKTime taken: 0.469 seconds stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600] Insert into 123456789101112131415161718192021222324hive&gt; insert into table analyze_orc_test partition(part_date='2019-02-19') select `id`,`name` from analyze_text_copy where part_date='2019-02-21';Query ID = hadoop_20190221145353_70e75ca0-210f-48d1-bbd0-b4b0ed01c0cfTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_1535945194143_1281, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1281/Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job -kill job_1535945194143_1281Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02019-02-21 14:53:53,124 Stage-1 map = 0%, reduce = 0%2019-02-21 14:54:00,420 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.6 secMapReduce Total cumulative CPU time: 3 seconds 600 msecEnded Job = job_1535945194143_1281Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/.hive-staging_hive_2019-02-21_14-53-45_113_8780230061527813380-1/-ext-10000Loading data to table dw.analyze_orc_test partition (part_date=2019-02-19)Partition dw.analyze_orc_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 3.6 sec HDFS Read: 3947 HDFS Write: 463 SUCCESSTotal MapReduce CPU Time Spent: 3 seconds 600 msecOKTime taken: 18.592 seconds 12345hive&gt; ANALYZE TABLE analyze_orc_test PARTITION(part_date='2019-02-19') COMPUTE STATISTICS noscan;Partition dw.analyze_orc_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600]OKTime taken: 0.274 seconds stats: [numFiles=1, numRows=20, totalSize=365, rawDataSize=3600] Parquet1234// create tableCREATE TABLE `analyze_parquet_test` (`id` string, `name` string) PARTITIONED BY (`part_date` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'; Load file to table 123456hive&gt; LOAD DATA LOCAL INPATH '/home/hadoop/wxm/analyze_parquet_test_file' OVERWRITE INTO TABLE analyze_parquet_test partition(part_date='2019-02-21');Loading data to table dw.analyze_parquet_test partition (part_date=2019-02-21)Partition dw.analyze_parquet_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=0, totalSize=390, rawDataSize=0]OKTime taken: 0.972 seconds 12345hive&gt; ANALYZE TABLE analyze_parquet_test PARTITION(part_date='2019-02-21') COMPUTE STATISTICS noscan;Partition dw.analyze_parquet_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=0, totalSize=390, rawDataSize=0]OKTime taken: 0.576 seconds stats: [numFiles=1, numRows=0, totalSize=390, rawDataSize=0] Add partition and mv file to partition path 12345// add partitionalter table analyze_parquet_test add partition(part_date='2019-02-20');// cp fileshadoop fs -cp hdfs://sdg/user/hive/warehouse/dw.db/analyze_parquet_test/part_date=2019-02-19/* hdfs://sdg/user/hive/warehouse/dw.db/analyze_parquet_test/part_date=2019-02-20/ 12345hive&gt; ANALYZE TABLE analyze_parquet_test PARTITION(part_date='2019-02-20') COMPUTE STATISTICS noscan;Partition dw.analyze_parquet_test&#123;part_date=2019-02-20&#125; stats: [numFiles=1, totalSize=390]OKTime taken: 0.799 seconds stats: [numFiles=1, totalSize=390] Insert into 123456789101112131415161718192021222324hive&gt; insert into table analyze_parquet_test partition(part_date='2019-02-19') select `id`,`name` from analyze_text_copy where part_date='2019-02-21';Query ID = hadoop_20190221150303_86874b16-b4c6-4e6d-a2e6-c1a02aaae3b8Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_1535945194143_1282, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1282/Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job -kill job_1535945194143_1282Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02019-02-21 15:03:27,069 Stage-1 map = 0%, reduce = 0%2019-02-21 15:03:35,527 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 4.98 secMapReduce Total cumulative CPU time: 4 seconds 980 msecEnded Job = job_1535945194143_1282Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://sdg/user/hive/warehouse/dw.db/analyze_parquet_test/part_date=2019-02-19/.hive-staging_hive_2019-02-21_15-03-17_975_7489031492557604858-1/-ext-10000Loading data to table dw.analyze_parquet_test partition (part_date=2019-02-19)Partition dw.analyze_parquet_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=390, rawDataSize=40]MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Cumulative CPU: 4.98 sec HDFS Read: 4039 HDFS Write: 490 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 980 msecOKTime taken: 22.794 seconds 12345hive&gt; ANALYZE TABLE analyze_parquet_test PARTITION(part_date='2019-02-19') COMPUTE STATISTICS noscan;Partition dw.analyze_parquet_test&#123;part_date=2019-02-19&#125; stats: [numFiles=1, numRows=20, totalSize=390, rawDataSize=40]OKTime taken: 0.226 seconds stats: [numFiles=1, numRows=20, totalSize=390, rawDataSize=40] Result通过上述测试可以看出,OrcFile文件的三种数据导入方式可以直接通过ANALYZE获取完整信息，其他文件格式默认只能获取到Size，除非insert into触发了MapReduce时才能获取RowCount ANALYZE耗时非常短，文件的统计数据一定是存储在某个地方不需要RunTime去计算（查询了Hive MetaStore，并没有发现表Row的统计存储），猜想原因在于OrcFile的自描述Header里存储了对Row的统计信息，ANALYZE会直接获取自描述文件中的统计信息，可以看下文件的Dump Hive Orc File DumpHive Orc Doc 12345678910111213141516171819202122232425262728293031323334353637383940414243// Orc Dumphive --orcfiledump hdfs://sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/000000_0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Structure for hdfs://sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/000000_0File Version: 0.12 with HIVE_873219/02/21 15:54:02 INFO orc.ReaderImpl: Reading ORC rows from hdfs://sdg/user/hive/warehouse/dw.db/analyze_orc_test/part_date=2019-02-19/000000_0 with &#123;include: null, offset: 0, length: 9223372036854775807&#125;Rows: 20Compression: ZLIBCompression size: 262144Type: struct&lt;_col0:string,_col1:string&gt;Stripe Statistics: Stripe 1: Column 0: count: 20 hasNull: false Column 1: count: 20 hasNull: false min: 1 max: 2 sum: 20 Column 2: count: 20 hasNull: false min: wxmimperio1 max: wxmimperio2 sum: 220File Statistics: Column 0: count: 20 hasNull: false Column 1: count: 20 hasNull: false min: 1 max: 2 sum: 20 Column 2: count: 20 hasNull: false min: wxmimperio1 max: wxmimperio2 sum: 220Stripes: Stripe: offset: 3 data: 52 rows: 20 tail: 57 index: 77 Stream: column 0 section ROW_INDEX start: 3 length 11 Stream: column 1 section ROW_INDEX start: 14 length 26 Stream: column 2 section ROW_INDEX start: 40 length 40 Stream: column 1 section DATA start: 80 length 8 Stream: column 1 section LENGTH start: 88 length 6 Stream: column 1 section DICTIONARY_DATA start: 94 length 5 Stream: column 2 section DATA start: 99 length 8 Stream: column 2 section LENGTH start: 107 length 6 Stream: column 2 section DICTIONARY_DATA start: 113 length 19 Encoding column 0: DIRECT Encoding column 1: DICTIONARY_V2[2] Encoding column 2: DICTIONARY_V2[2]File length: 365 bytesPadding length: 0 bytesPadding ratio: 0% 可以看到Dump信息里面有Rows: 20的信息，所以可以确定ANALYZE 命令是分析了file的meta Parquet File DumpParquet Tools 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162hadoop jar parquet-tools-1.5.0.jar dump hdfs://sdg/user/hive/warehouse/dw.db/analyze_parquet_test/part_date=2019-02-19/000000_0row group 0 --------------------------------------------------------------------------------id: BINARY UNCOMPRESSED DO:0 FPO:4 SZ:61/61/1.00 VC:20 ENC:BIT_PACK [more]...name: BINARY UNCOMPRESSED DO:0 FPO:65 SZ:101/101/1.00 VC:20 ENC:BIT_P [more]... id TV=20 RL=0 DL=1 DS: 2 DE:PLAIN_DICTIONARY ---------------------------------------------------------------------------- page 0: DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:11 [more]... name TV=20 RL=0 DL=1 DS: 2 DE:PLAIN_DICTIONARY ---------------------------------------------------------------------------- page 0: DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:11 [more]...BINARY id --------------------------------------------------------------------------------*** row group 1 of 1, values 1 to 20 *** value 1: R:0 D:1 V:1value 2: R:0 D:1 V:2value 3: R:0 D:1 V:1value 4: R:0 D:1 V:2value 5: R:0 D:1 V:1value 6: R:0 D:1 V:2value 7: R:0 D:1 V:1value 8: R:0 D:1 V:2value 9: R:0 D:1 V:1value 10: R:0 D:1 V:2value 11: R:0 D:1 V:1value 12: R:0 D:1 V:2value 13: R:0 D:1 V:1value 14: R:0 D:1 V:2value 15: R:0 D:1 V:1value 16: R:0 D:1 V:2value 17: R:0 D:1 V:1value 18: R:0 D:1 V:2value 19: R:0 D:1 V:1value 20: R:0 D:1 V:2BINARY name --------------------------------------------------------------------------------*** row group 1 of 1, values 1 to 20 *** value 1: R:0 D:1 V:wxmimperio1value 2: R:0 D:1 V:wxmimperio2value 3: R:0 D:1 V:wxmimperio1value 4: R:0 D:1 V:wxmimperio2value 5: R:0 D:1 V:wxmimperio1value 6: R:0 D:1 V:wxmimperio2value 7: R:0 D:1 V:wxmimperio1value 8: R:0 D:1 V:wxmimperio2value 9: R:0 D:1 V:wxmimperio1value 10: R:0 D:1 V:wxmimperio2value 11: R:0 D:1 V:wxmimperio1value 12: R:0 D:1 V:wxmimperio2value 13: R:0 D:1 V:wxmimperio1value 14: R:0 D:1 V:wxmimperio2value 15: R:0 D:1 V:wxmimperio1value 16: R:0 D:1 V:wxmimperio2value 17: R:0 D:1 V:wxmimperio1value 18: R:0 D:1 V:wxmimperio2value 19: R:0 D:1 V:wxmimperio1value 20: R:0 D:1 V:wxmimperio2 虽然看到Dump文件中有TV=20，但是不像ORC File 有完整的Row信息，meta还是基于列的，所以读取不到 MapReduce 统计 我们想要获取完整的表统计信息，可以将ANALYZE命令的noscan去掉执行，则会触发一个MapReduce，这个MR会对表文件做一个统计，并将结果存储到Hive MetaStore中，后续在用ANALYZE分析就会直接得到结果 12345678910111213141516171819hive&gt; ANALYZE TABLE analyze_parquet_test PARTITION(part_date='2019-02-21') COMPUTE STATISTICS;Query ID = hadoop_20190221173030_d9cc4e9c-73f6-403e-bfbc-c3cf381b373dTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_1535945194143_1284, Tracking URL = http://wh-8-211:8088/proxy/application_1535945194143_1284/Kill Command = /app/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/bin/hadoop job -kill job_1535945194143_1284Hadoop job information for Stage-0: number of mappers: 1; number of reducers: 02019-02-21 17:30:54,865 Stage-0 map = 0%, reduce = 0%2019-02-21 17:31:02,248 Stage-0 map = 100%, reduce = 0%, Cumulative CPU 4.21 secMapReduce Total cumulative CPU time: 4 seconds 210 msecEnded Job = job_1535945194143_1284Partition dw.analyze_parquet_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=20, totalSize=390, rawDataSize=60]MapReduce Jobs Launched: Stage-Stage-0: Map: 1 Cumulative CPU: 4.21 sec HDFS Read: 2931 HDFS Write: 100 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 210 msecOKTime taken: 21.214 seconds 12345hive&gt; ANALYZE TABLE analyze_parquet_test PARTITION(part_date='2019-02-21') COMPUTE STATISTICS noscan;Partition dw.analyze_parquet_test&#123;part_date=2019-02-21&#125; stats: [numFiles=1, numRows=20, totalSize=390, rawDataSize=60]OKTime taken: 0.219 seconds 总结我们可以看到利用ANALYZE命令可以快速帮助我们分析一张表的基本统计信息，但也有缺点：如果是分区表，写分区则会列出这个分区的统计，不写分区会列出所有分区的数据列表；并没有一个队整张表进行的汇总统计，所以通常需要配合最开始提到的两种常用方法 对于OrcFile则直接可以用ANALYZE命令分析，如果是TextFile、SequenceFile、ParquetFile，则需要确保Hive 表的数据加载方式是通过MapReduce，其他方式Load数据的则需要执行noscan 的ANALYZE（可以准备离线任务，每天对非MR Load的数据表执行一次）；也可以在执行ETL的时候，将统计作为一个回调自动完成 Size 1long size = FileSystem.getContentSummary(new Path(tablePath)).getLength(); 此方法可以获取一个表路径、分区路径以及具体文件的Size大小；Size是压缩后的值 全表的统计可以将tablePath截止到tableName就可以，获取某个分区的统计，则tablePath精确到分区目录 RowCount 1ANALYZE TABLE tablName [PARTITION(part)] COMPUTE STATISTICS noscan; 此方法会列出所有表的分区统计，解析这些统计并将numRows加总就是全表的RowCount；如果是非分区表或只想获取当前分区的统计，则只解析一条记录 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>BigData</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot-Java8Date]]></title>
    <url>%2F2019%2F02%2F19%2FSpringBoot-Java8Date%2F</url>
    <content type="text"><![CDATA[在SpringBoot和Feign使用Java8的Date类型，如LocalDate、LocalDateTime，会出现Format不符合常规的情况 Software Version SpringBoot 1.5.13.RELEASE Java 1.8.0_201 默认时间格式显示123456789101112// DefaultTimeBeanprivate LocalDate localDate;private LocalDateTime localDateTime;private LocalTime localTime;public DefaultTimeBean(LocalDate localDate, LocalDateTime localDateTime, LocalTime localTime) &#123; this.localDate = localDate; this.localDateTime = localDateTime; this.localTime = localTime;&#125;// getter and setter 通过Rest获取 1234@GetMapping("/getDefaultJava8Time")public DefaultTimeBean getDefaultJava8Time() &#123; return new DefaultTimeBean(LocalDate.now(), LocalDateTime.now(), LocalTime.now());&#125; 结果 123456// 全部是以数组的形式返回，这不符合我们的开发规范，要在序列化/反序列化的时候就进行formart&#123; localDate: [2019, 2, 19], localDateTime: [2019, 2, 19, 15, 49, 10, 964000000], localTime: [15, 49, 10, 964000000]&#125; 格式化时间显示 Maven 12345// 不需要写version&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt; &lt;artifactId&gt;jackson-datatype-jsr310&lt;/artifactId&gt;&lt;/dependency&gt; application.yml 1234spring: jackson: serialization: WRITE_DATES_AS_TIMESTAMPS: false 注解配置format规则 1234567891011// TimeBean@DateTimeFormat(iso = DateTimeFormat.ISO.DATE)private LocalDate localDate;@DateTimeFormat(pattern = "yyyy-MM-dd HH:mm:ss")@JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss")private LocalDateTime localDateTime;@JsonFormat(pattern = "HH:mm:ss")@DateTimeFormat(iso = DateTimeFormat.ISO.TIME)private LocalTime localTime; 通过Rest获取 Get 1234@GetMapping("/getFormatJava8Time")public TimeBean getFormatJava8Time() &#123; return new TimeBean(LocalDate.now(), LocalDateTime.now(), LocalTime.now());&#125; 结果 12345&#123; localDate: "2019-02-19", localDateTime: "2019-02-19 15:57:08", localTime: "15:57:08"&#125; Post 1234567@PostMapping("/formatJava8Time")public TimeBean postDateTime(@RequestBody TimeBean timeBean) &#123; System.out.println(timeBean.getLocalDate()); System.out.println(timeBean.getLocalDateTime()); System.out.println(timeBean.getLocalTime()); return timeBean;&#125; 结果 12345&#123; "localDate": "2019-05-02", "localDateTime": "2019-06-15 12:23:59", "localTime": "15:12:23"&#125; 转载请注明出处]]></content>
      <categories>
        <category>SpringBoot</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cassandra集群运维[Add & Remove Nodes]]]></title>
    <url>%2F2018%2F10%2F20%2FCassandra-Cluster-Operation%2F</url>
    <content type="text"><![CDATA[在对Cassandra进行维护的时候，通常需要扩集群或者迁移数据，涉及到添加、移除节点。 Cassandra Version: Apache Cassandra 3.0.6 Add NodesVirtual nodes (vnodes) greatly simplify adding nodes to an existing cluster:Calculating tokens and assigning them to each node is no longer required.Rebalancing a cluster is no longer necessary because a node joining the cluster assumes responsibility for an even portion of the data. 确保新加节点和现有集群的Cassandra 版本一致 【操作步骤】在新的机器上部署cassandra，但不要启动通常都是从现有集群的一台机器上scp cassandra目录到新机器 基于现有集群所用的snitch算法修改配置文件 cassandra-topology.properties or the cassandra-rackdc.properties 使用PropertyFileSnitch算法配置：cassandra-topology.properties 使用GossipingPropertyFileSnitch, Ec2Snitch, Ec2MultiRegionSnitch, and GoogleCloudSnitch算法配置：cassandra-rackdc.properties ps: 这两个配置与机架和多数据中心有关，如果是同机架单数据中心则不用配置 修改配置cassandra.yaml文件 name desc auto_bootstrap 默认文件中是没有这个参数的，如果没有默认为true；如果有且为false修改为true cluster_name 需要加入的集群名称 listen_address/broadcast_address 用来与集群内其他节点通信的ip，通常为本机真实ip，不要填写127.0.0.1或localhost endpoint_snitch 用于定位节点和路由请求的算法，与现有集群保持一致 num_tokens 节点中vnodes的数量，与现有集群配置保持一致，如果当前机器配置更高可以按比例增加这个值，可以有更好的性能 seed_provider 种子节点，至少保证有一个现有集群的节点，-seeds列表表示了新节点与现有集群通过哪些节点通信（种子节点无法引导，所以不要仅仅把要加入的新节点配置进去，也不要将集群所有节点配置成种子节点） 启动新节点Cassandra1./bin/cassandra 初始化system相关信息 1234567891011121314151617......INFO 06:14:41 Initializing system.IndexInfoINFO 06:14:42 Initializing system.batchesINFO 06:14:42 Initializing system.paxosINFO 06:14:42 Initializing system.localINFO 06:14:42 Initializing system.peersINFO 06:14:42 Initializing system.peer_eventsINFO 06:14:42 Initializing system.range_xfersINFO 06:14:42 Initializing system.compaction_historyINFO 06:14:42 Initializing system.sstable_activityINFO 06:14:42 Initializing system.size_estimatesINFO 06:14:42 Initializing system.available_rangesINFO 06:14:42 Initializing system.views_builds_in_progressINFO 06:14:42 Initializing system.built_viewsINFO 06:14:42 Initializing system.hintsINFO 06:14:42 Initializing system.batchlog...... 寻找现有集群节点 12345678INFO 06:14:44 Node /xx.xxx.xx.xx is now part of the clusterINFO 06:14:44 Node /xx.xxx.xx.xx is now part of the clusterINFO 06:14:44 Node /xx.xxx.xx.xx is now part of the clusterINFO 06:14:44 Handshaking version with /xx.xxx.xx.xxINFO 06:14:44 Handshaking version with /xx.xxx.xx.xxINFO 06:14:44 InetAddress /xx.xxx.xx.xx is now UPINFO 06:14:44 InetAddress /xx.xxx.xx.xx is now UPINFO 06:14:44 InetAddress /xx.xxx.xx.xx is now UP 新节点加入集群 12INFO 06:14:45 JOINING: waiting for ring informationINFO 06:14:45 Updating topology for all endpoints that have changed 同步schema 123456789INFO 06:14:49 Initializing system_traces.eventsINFO 06:14:49 Initializing system_traces.sessionsINFO 06:14:49 Initializing system_distributed.parent_repair_historyINFO 06:14:49 Initializing system_distributed.repair_historyINFO 06:14:49 Initializing system_auth.resource_role_permissons_indexINFO 06:14:49 Initializing system_auth.role_membersINFO 06:14:49 Initializing system_auth.role_permissionsINFO 06:14:49 Initializing system_auth.rolesINFO 06:14:49 JOINING: waiting for schema information to complete Copy Schema数据 1234567891011121314INFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Executing streaming plan for BootstrapINFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Starting streaming to /xx.xxx.xx.xxINFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Starting streaming to /xx.xxx.xx.xxINFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Starting streaming to /xx.xxx.xx.xxINFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4, ID#0] Beginning stream session with /xx.xxx.xx.xxINFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4, ID#0] Beginning stream session with /xx.xxx.xx.xxINFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4, ID#0] Beginning stream session with /xx.xxx.xx.xxINFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4 ID#0] Prepare completed. Receiving 48 files(358160851 bytes), sending 0 files(0 bytes)INFO 06:15:22 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4 ID#0] Prepare completed. Receiving 35 files(132483825 bytes), sending 0 files(0 bytes)INFO 06:15:23 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4 ID#0] Prepare completed. Receiving 46 files(174538642 bytes), sending 0 files(0 bytes)INFO 06:16:54 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Session with /xx.xxx.xx.xx is completeINFO 06:17:38 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Session with /xx.xxx.xx.xx is completeINFO 06:20:28 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] Session with /xx.xxx.xx.xx is completeINFO 06:20:28 [Stream #317a30b0-d29d-11e8-aa92-e9ebc9b827d4] All sessions completed 节点切换成NORMAL 12INFO 06:20:29 Node /xx.xxx.xx.xx state jump to NORMALINFO 06:20:29 Waiting for gossip to settle before accepting client requests... 查看节点同步状态 ./bin/nodetool status 数据同步期间节点的状态： 123456789Datacenter: dc1===============Status=Up/Down|/ State=Normal/Leaving/Joining/Moving-- Address Load Tokens Owns (effective) Host ID RackUN xx.xxx.xx.xx 1.53 GB 256 100.0% 30ed942d-6827-469b-aab9-7fb649c6c3d7 rack1UN xx.xxx.xx.xx 1.38 GB 256 100.0% 96736106-e95d-4c54-aabf-41666071bc59 rack1UN xx.xxx.xx.xx 1.07 GB 256 100.0% 4351af17-2e68-4b46-a78f-fad900e44d13 rack1UJ 新加节点 57.87 MB 256 ? f3f590ac-9835-47bb-b4d8-6e17ea2916ac rack1 数据同步结束后的状态： 123456789Datacenter: dc1===============Status=Up/Down|/ State=Normal/Leaving/Joining/Moving-- Address Load Tokens Owns (effective) Host ID RackUN xx.xxx.xx.xx 1.53 GB 256 69.2% 30ed942d-6827-469b-aab9-7fb649c6c3d7 rack1UN xx.xxx.xx.xx 1.38 GB 256 79.3% 96736106-e95d-4c54-aabf-41666071bc59 rack1UN xx.xxx.xx.xx 1.07 GB 256 78.0% 4351af17-2e68-4b46-a78f-fad900e44d13 rack1UN xx.xxx.xx.xx 581.43 MB 256 73.5% f3f590ac-9835-47bb-b4d8-6e17ea2916ac rack1 运行nodetool cleanup nodetool options cleanup [keyspace_name [table_name] […] ] 在所有新节点都加入集群并且数据同步完成后，在之前旧的每一个节点上运行nodetool cleanup操作删除keys。在做操作时保证一个节点结束后再运行下一个节点，不要并发执行，这样可以安全地推迟清理 Reomve NodesUN状态的节点下线在要下线的节点运行nodetool decommission命令 1nodetool &lt;options&gt; decommission 该命令会将当前节点的range和请求交给其他节点管理，并且将数据同步给其他节点 DN状态的节点下线在任何存活的节点运行nodetool removenode命令 该命令会将当前集群下线的节点移除，并且将数据同步给其他节点 查看节点状态： 12345678Datacenter: DC1===============Status=Up/Down|/ State=Normal/Leaving/Joining/Moving-- Address Load Tokens Owns (effective) Host ID RackUN 192.168.2.101 112.82 KB 256 31.7% 420129fc-0d84-42b0-be41-ef7dd3a8ad06 RAC1DN 192.168.2.103 91.11 KB 256 33.9% d0844a21-3698-4883-ab66-9e2fd5150edd RAC1UN 192.168.2.102 124.42 KB 256 32.6% 8d5ed9f4-7764-4dbd-bad8-43fddce94b7c RAC1 1&gt; nodetool &lt;options&gt; removenode -- &lt;status&gt; | &lt;force&gt; | &lt;ID&gt; 1&gt; nodetool removenode d0844a21-3698-4883-ab66-9e2fd5150edd 节点下线失败 nodetool assassinate 1nodetool [options] assassinate &lt;ip_address&gt; 1nodetool -u cassandra -pw cassandra assassinate 192.168.100.2 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Cassandra</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Cassandra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次HBase Drop的错误]]></title>
    <url>%2F2018%2F09%2F24%2FHBase-DropTable-Error%2F</url>
    <content type="text"><![CDATA[hbsae表进行数据清洗，在清空数据的时候发生错误，引发后续问题 HBase version: 1.2.0-cdh5.11.1 HBase 清空数据 truncate ‘TableName’（清除数据，并且清除了分区） truncate_preserve ‘TableName’（清除数据，不清除分区) 由于我想保留分区，所以选择了 truncate_preserve 问题发生：用hbase shell 执行truncate_preserve ‘TableName’，中途网络问题ssh突然断开连接 shell显示： 123Truncating 'TableName' table (it may take a while): - Disabling table... - Truncating table... 后重新连接ssh hbase shell，list 发现表名存在，但是scan、disable、drop命令都报Table not found 12345ERROR: Table TableName does not exist.Start disable of named table: hbase&gt; disable 't1' hbase&gt; disable 'ns1:t1' HBase Web UI 上也存在这张表，但是点进去有报错信息： 1234567891011121314151617181920212223242526272829303132org.apache.hadoop.hbase.client.HBaseAdmin.checkTableExistence(HBaseAdmin.java:1499)org.apache.hadoop.hbase.client.HBaseAdmin.isTableEnabled(HBaseAdmin.java:1510)org.apache.hadoop.hbase.generated.master.table_jsp._jspService(table_jsp.java:192)org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)javax.servlet.http.HttpServlet.service(HttpServlet.java:820)org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:113)org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)org.apache.hadoop.hbase.http.ClickjackingPreventionFilter.doFilter(ClickjackingPreventionFilter.java:48)org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1354)org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49)org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49)org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)org.mortbay.jetty.Server.handle(Server.java:326)org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) 问题推断第一反应是这张表到底存不存在？ 到HDFS上查看文件： 1hadoop fs -ls /hbase/data/default/TableName 发现文件目录存在，但是里面没有任何文件，是空目录。推断可能是文件已经删掉了，但是缓存中或者zk中还是有这张表的信息，因为突然中止导致table meta没有生成 尝试恢复 snapshot 恢复（失败） 因为这张表之前做过snapshot备份，想从snapshot恢复 12clone_snapshot 'TableName_Bak', 'TableName'restore_snapshot 'TableName_Bak' 发现一只会卡在restore_snapshot &#39;TableName_Bak&#39;，应该是找不到这张表的meta hbck修复（失败） 想着通过meta修复，可以自动生成desc文件 12345#修复 metahbase hbck -fixMeta#重新分配rshbase hbck -fixAssignments 执行这两条语句后发现日志中均没有该表名，也没有任何异常，问题依旧 zookeeper删除信息（成功） 登录hbase zk： 1234567zkCli.shls /hbase/tablermr /hbase/table/TableName 相关信息ls /hbase/table-lockrmr /hbase/table-lock/TableName 相关信息 重启hbase 集群后这张表已经不存在，重新建表后正常 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>HBase</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Struts2-Upgrade]]></title>
    <url>%2F2018%2F08%2F27%2FStruts2-Upgrade%2F</url>
    <content type="text"><![CDATA[遇到总结一下项目中有关Struts2升级中遇到的坑。项目大概开始于09年左右，维护近10年，由于Struts2安全漏洞问题决定升级版本，由于版本跨度比较大，一些方法已经弃用或配置变更等 Main Maven Dependency12345678910111213// old&lt;dependency&gt; &lt;groupId&gt;org.apache.struts&lt;/groupId&gt; &lt;artifactId&gt;struts2-core&lt;/artifactId&gt; &lt;version&gt;2.1.8.1&lt;/version&gt;&lt;/dependency&gt;// new&lt;dependency&gt; &lt;groupId&gt;org.apache.struts&lt;/groupId&gt; &lt;artifactId&gt;struts2-core&lt;/artifactId&gt; &lt;version&gt;2.5.17&lt;/version&gt;&lt;/dependency&gt; Struts2.xml old 1234567891011121314151617181920212223&lt;filter&gt; &lt;filter-name&gt;ActionContextCleanUp&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ActionContextCleanUp&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;ActionContextCleanUp&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter&gt; &lt;filter-name&gt;struts&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.FilterDispatcher&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;struts.i18n.encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts&lt;/filter-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt; &lt;/filter-mapping&gt; new 123456789101112131415161718&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;struts.i18n.encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/project/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 在Struts2 2.5.17中org.apache.struts2.dispatcher.FilterDispatcher 和 org.apache.struts2.dispatcher.ActionContextCleanUp 被废除，用 org.apache.struts2.dispatcher.filter.StrutsPrepareAndExecuteFilter 替换 升级过程中遇到的报错： java.lang.NoSuchMethodError: ognl.SimpleNode.isEvalChain(Lognl/OgnlContext;)Z 问题：ognl 的jar包冲突 需要查询项目中Jar依赖关系，排除无用Jar包。Ognl版本至少在3.0.6以上 12345// 我遇到的是xwork中有低版本的ognl包，故排除&lt;exclusion&gt; &lt;artifactId&gt;ognl&lt;/artifactId&gt; &lt;groupId&gt;opensymphony&lt;/groupId&gt;&lt;/exclusion&gt; There is no Action mapped for namespace [/] and action name [user!add] associated with context path 问题：由于2.5.17安全机制，过滤器必须指定mapped规则 粗粒度—动态方法调用 12345678910// struts.xml配置添加 &lt;struts&gt; &lt;constant name="struts.enable.DynamicMethodInvocation" value="true"/&gt; ...... &lt;package name="default" extends="struts-default"&gt; ...... &lt;global-allowed-methods&gt;regex:.*&lt;/global-allowed-methods&gt; ...... &lt;/package&gt;&lt;/struts&gt; 细粒度-动态方法调用 12345678910// action配置 &lt;action name="login_*" method="&#123;2&#125;" class="com.wxmimperio.struts.&#123;1&#125;Action"&gt; ....... &lt;result name="success"&gt;/pages/success.jsp&lt;/result&gt; &lt;result name="error"&gt;/pages/error.jsp&lt;/result&gt; ...... &lt;allowed-methods&gt;regex:.*&lt;/allowed-methods&gt; ......&lt;/action&gt; 这里在action的name中通配了一个login_*，它对应映射的是method属性。如果在客户端发生一个这样的请 求：login_init.action、login_show.action等，这时服务器就会自动调用这个action中的init()方法或 show()方法。这里的method=”{1}”代表是第一个星号，如果有多个星号，就要根据顺序来通配{1},{2},{3}…. allowed-methods中可以用逗号分隔指定方法名，也可以用正则匹配。 错误： Struts2 与 Servlet 冲突 表现在当struts.xml如下配置： 1234&lt;filter-mapping&gt; &lt;filter-name&gt;struts&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; struts拦截器会拦截/*下所有路径，所以自定义的servlet无法被mapped到，导致请求根本无法响应 解决方案如下： 修改servlet的相关配置，统一在servlet后面加上.servlet 123456789&lt;servlet&gt; &lt;servlet-name&gt;jqueryAjaxServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.clzhang.sample.struts2.servlet.jQueryAjaxServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;jqueryAjaxServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/jqueryAjax.servlet&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 修改拦截页面配置，就是将struts的相关拦截配置一下 12345678910111213141516&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;*.jsp&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/user/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 修改struts.xml文件中的后缀映射 1&lt;constant name="struts.action.extension" value="action"&gt;&lt;/constant&gt; 转载请注明出处]]></content>
      <categories>
        <category>Java</category>
        <category>Struts2</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Web</tag>
        <tag>Struts2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PinPoint-Deploy[部署]]]></title>
    <url>%2F2018%2F06%2F30%2Fpinpoint-deploy%2F</url>
    <content type="text"><![CDATA[Pinpoint is an APM (Application Performance Management) tool for large-scale distributed systems written in Java. 系统架构 架构图 组件： pinpoint-collector-1.7.3.war （数据收集） pinpoint-web-1.7.3.war （页面展示） pinpoint-agent-1.7.3.tar.gz （数据采集） 所需环境 版本适配信息 Tomcat-8.5.32（web container） Hbase-1.2.6 （for storage） 部署步骤 安装HBase 创建监控所需的HBase 表 下载最新PinPoint执行文件（或自行git clone —&gt; maven build） The current stable version is v1.7.3. DownLoad 部署 Pinpoint Collector 解压 pinpoint-collector-$VERSION.war 到 Tomcat 容器 修改 pinpoint-collector.properties, hbase.properties 文件 部署 Pinpoint Web 解压 pinpoint-web-$VERSION.war 到 Tomcat 容器 修改 pinpoint-web, hbase.properties 文件 启动Tomcat 部署 Pinpoint Agent 解压 pinpoint-agent 压缩包 设置 -javaagent:$AGENT_PATH/pinpoint-bootstrap-$VERSION.jar JVM参数到 App Jar 启动参数 设置 -Dpinpoint.agentId and -Dpinpoint.applicationName 到 App Jar 启动参数 启动Java App 部署 Tomcat 两个webapp实例 12345678910111213141516171819202122232425......&lt;Service name="Catalina1"&gt; &lt;Connector port="8081" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; &lt;Engine name="Catalina1" defaultHost="localhost"&gt; &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt; &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm" resourceName="UserDatabase"/&gt; &lt;/Realm&gt; &lt;Host name="localhost" appBase="/home/wxmimperio/software/apache-tomcat-8.5.32/pinpoint-web" unpackWARs="true" autoDeploy="true"&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt; &lt;Service name="Catalina2"&gt; &lt;Connector port="8082" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; &lt;Engine name="Catalina2" defaultHost="localhost"&gt; &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt; &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm" resourceName="UserDatabase"/&gt; &lt;/Realm&gt; &lt;Host name="localhost" appBase="/home/wxmimperio/software/apache-tomcat-8.5.32/pinpoint-collector" unpackWARs="true" autoDeploy="true"&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;...... 初始化Hbase 监控表 12345# https://github.com/naver/pinpoint/blob/master/hbase/scripts/hbase-create.hbasevim hbase-create.hbasehbase shell hbase-create.hbase pinpoint-web 解压war，修改配置文件： 123456789101112# unzip pinpoint-web-1.7.3.war -d pinpoint-web-1.7.3# /apache-tomcat-8.5.32/pinpoint-web/pinpoint-web-1.7.3/WEB-INF/classesvim hbase.properties# 设置hbase地址hbase.client.host=192.168.1.110hbase.client.port=2181vim pinpoint-web.properties# 关闭集群模式cluster.enable=false pinpoint-collector 123456789101112# unzip pinpoint-collector-1.7.3.war -d pinpoint-collector-1.7.3# /apache-tomcat-8.5.32/pinpoint-collector/pinpoint-collector-1.7.3/WEB-INF/classesvim hbase.properties# 设置hbase地址hbase.client.host=192.168.1.110hbase.client.port=2181vim pinpoint-collector.properties# 关闭集群模式cluster.enable=false 重启Tomcat 123./startup.sh# http://192.168.1.110:8081/pinpoint-web-1.7.3/#/main 部署应用 1java -javaagent:/home/wxmimperio/software/pinpoint/pinpoint-agent-1.7.3/pinpoint-bootstrap-1.7.3.jar -Dpinpoint.agentId=wxm-consumer -Dpinpoint.applicationName=wxm-consumer -jar spring-boot-test-1.0-SNAPSHOT.jar]]></content>
      <categories>
        <category>BigData</category>
        <category>MicroService</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>监控</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Avro-介绍]]></title>
    <url>%2F2018%2F05%2F27%2Fagamotto-avro-overview%2F</url>
    <content type="text"><![CDATA[Apache Avro 是一个与语言无关的序列化工具，由Hadoop之父Doug Cutting开发。通过Avro读写数据文件无需生成代码，也不需要使用或实现RPC协议,使用JSON格式来自描述数据结构。支持Java、C、C++、C＃、Python和Ruby等语言。 Avro Schema Avro依赖于Schema，使得序列化速度很快，并且生成的序列化数据较小。当写入数据的时候，Schema结构与Avro数据一起存储在数据文件中以供进一步处理。 在RPC中使用Avro，client和server连接时进行Schema交换，无论是client端还是server端都有完整的schema定义，这样使得命名相同的字段、缺失的字段、额外的字段等之间的对应关系可以进行同步及处理。 Avro模式使用Json定义，这有助于在已经有Json库的语言中实现。与Avro一样，Hadoop中还有其他序列化机制，例如Sequence Files、Protocol Buffers和Thrift. Avro是一种可压缩和可拆分的二进制结构化格式，用来支持数据密集型应用，适合于远程或本地大规模数据的存储和交换，可以进行纯数据的发送和接收 1234567891011// Schema样例&#123; "type": "record", "name": "Employee", "fields": [ &#123;"name": "name", "type": "string"&#125;, &#123;"name": "age", "type": "int"&#125;, &#123;"name": "emails", "type": &#123;"type": "array", "items": "string"&#125;&#125;, &#123;"name": "boss", "type": ["Employee","null"]&#125; ]&#125; Thrift &amp; Protocol Buffers 对比 Avro Avro同时支持动态和静态类型，而Protocol Buffers和Thrift使用接口定义语言（IDL）来指定数据结构及其类型，同时还需要使用定义好的数据机构和类型来序列化和反序列化代码。 Avro以Hadoop生态环境为基础，建立在Hadoop中，可以很好的与MapReduce Job进行结合；而Protocol Buffers和Thrift脱离Hadoop生态。 Avro使用步骤 创建Schema——根据数据格式要求创建符合Json规范的Schema文件 读取Schema文件——在程序中读取Schema文件，进行初始化（两种方式） Avro编译模式，使用Avro-Tools生成带有Schema的实体类 Avro解析器模式，可以引入Avro的相关以来，通过代码解析的方式解析Schema文件 使用Avro提供的序列化API对数据进行序列化 使用Avro提供的反序列化API对数据进行反序列化 转载请注明出处：https://github.com/imperio-wxm]]></content>
      <categories>
        <category>BigData</category>
        <category>Avro</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Avro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java自定义线程池ThreadPoolExecutor]]></title>
    <url>%2F2018%2F01%2F28%2FCustom-ThreadPool%2F</url>
    <content type="text"><![CDATA[为了提高性能和充分利用系统资源，通常会选择使用多线程技术。然而线程的启动、销毁成本是比较高的，线程之间的切换也会消耗大量的JVM资源，所以线程池的出现是为了更好管理和调度线程的一种方式，和连接池和对象池的初衷一样，在有空闲资源的前提下，让现有资源充分重复利用，避免不必要的开销。 线程池的好处： 降低资源消耗 提高响应速度 增强线程的管理 自带线程池概述 JDK已经实现了4个经典线程池 Executors.newCacheThreadPool() 可缓存线程池——先检查线程池中是否有已经创建且空闲的线程，如果有直接使用，否则创建一个新的线程加入线程池并启动 Worker线程的数据量没有限制——Interger.MAX_VALUE Worker默认的生命周期为1min，即当线程池中线程空闲时间超过1min，此线程会被自动销毁，当有新的任务进来时重新建立线程 12345678910111213141516171819202122232425/** * Creates a thread pool that creates new threads as needed, but * will reuse previously constructed threads when they are * available. These pools will typically improve the performance * of programs that execute many short-lived asynchronous tasks. * Calls to &#123;@code execute&#125; will reuse previously constructed * threads if available. If no existing thread is available, a new * thread will be created and added to the pool. Threads that have * not been used for sixty seconds are terminated and removed from * the cache. Thus, a pool that remains idle for long enough will * not consume any resources. Note that pools with similar * properties but different details (for example, timeout parameters) * may be created using &#123;@link ThreadPoolExecutor&#125; constructors. * * @return the newly created thread pool */public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor( 0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;() );&#125; Executors.newFixedThreadPool() 可控Worker数量的线程池，此线程池中有一个无界队列，当提交的Worker数量大于设定值n时，提交的线程不会立刻启动，而是放到队列中；当n中某些线程执行结束，队列中的线程才得以执行 从源码中得知，newFixedThreadPool用的是LinkedBlockingQueue，此队列是一个基于链表结构的阻塞队列，此队列按FIFO原则存储元素 在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源，即始终会保留设定值n个线程在线程池中 123456789101112131415161718192021222324/** * Creates a thread pool that reuses a fixed number of threads * operating off a shared unbounded queue. At any point, at most * &#123;@code nThreads&#125; threads will be active processing tasks. * If additional tasks are submitted when all threads are active, * they will wait in the queue until a thread is available. * If any thread terminates due to a failure during execution * prior to shutdown, a new one will take its place if needed to * execute subsequent tasks. The threads in the pool will exist * until it is explicitly &#123;@link ExecutorService#shutdown shutdown&#125;. * * @param nThreads the number of threads in the pool * @return the newly created thread pool * @throws IllegalArgumentException if &#123;@code nThreads &lt;= 0&#125; */public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor( nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;() );&#125; Executors.newScheduledThreadPool() 创建一个定长的线程池，而且支持定时的以及周期性的任务执行，支持定时及周期性任务执行 看其源码构造方法中，有一个延迟队列DelayedWorkQueue JDK中定义：Delayed元素的一个无界阻塞队列，只有在延迟期满时才能从中提取元素。该队列的头部是延迟期满后保存时间最长的Delayed元素。如果延迟都还没有期满，则队列没有头部，并且poll将返回null。当一个元素的 getDelay(TimeUnit.NANOSECONDS)方法返回一个小于等于0的值时，将发生到期。即使无法使用take或poll移除未到期的元素，也不会将这些元素作为正常元素对待。例如，size方法同时返回到期和未到期元素的计数。此队列不允许使用null元素。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Creates a new &#123;@code ScheduledThreadPoolExecutor&#125; with the * given core pool size. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @throws IllegalArgumentException if &#123;@code corePoolSize &lt; 0&#125; */public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,new DelayedWorkQueue());&#125;// 这里的父类是/** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters and default rejected execution handler. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @param threadFactory the factory to use when the executor * creates a new thread * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; * or &#123;@code threadFactory&#125; is null */public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime,unit, workQueue,threadFactory, defaultHandler);&#125; Executors.newSingleThreadExecutor() 单线程池，即提交到此线程池的线程只能用一个Worker线程去执行；保证所有任务按照指定顺序(FIFO, LIFO)执行；多应用于并发操作某类只能由一个线程处理的任务，例如多线程写多个不同的文件，每个线程对应于一个文件，但对于一个文件来说同一时刻只能由一个线程顺序去写。 12345678910111213141516171819/** * Creates an Executor that uses a single worker thread operating * off an unbounded queue. (Note however that if this single * thread terminates due to a failure during execution prior to * shutdown, a new one will take its place if needed to execute * subsequent tasks.) Tasks are guaranteed to execute * sequentially, and no more than one task will be active at any * given time. Unlike the otherwise equivalent * &#123;@code newFixedThreadPool(1)&#125; the returned executor is * guaranteed not to be reconfigurable to use additional threads. * * @return the newly created single-threaded Executor */public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService( new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;() ) );&#125; 自定义线程池ThreadPoolExecutor1ThreadPoolExecutor(corePoolSize,maximumPoolSize,keepAliveTime,milliseconds,runnableTaskQueue,handler) 【Params】: corePoolSize（线程池的基本大小-可运行Worker线程数量）:当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads方法，线程池会提前创建并启动所有基本线程 runnableTaskQueue（任务队列）：当线程超过corePoolSize设定的个数，会将线程先放到阻塞队列中不立刻执行可以选择以下几个阻塞队列： ArrayBlockingQueue：一个基于数组的有界阻塞队列，此队列按FIFO原则 LinkedBlockingQueue：一个基于链表的阻塞队列，此队列按FIFO原则，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列 SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于 LinkedBlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列 PriorityBlockingQueue：一个具有优先级的无限阻塞队列 maximumPoolSize（线程池最大大小）：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。值得注意的是如果使用了无界的任务队列这个参数就没什么效果 ThreadFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字 RejectedExecutionHandler（饱和策略）：当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。以下是JDK1.5提供的四种策略: AbortPolicy：直接抛出异常CallerRunsPolicy：只用调用者所在线程来运行任务DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务DiscardPolicy：不处理，丢弃掉 当然也可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略。如记录日志或持久化不能处理的任务。``` keepAliveTime（线程活动保持时间）：线程池的工作线程空闲后，保持存活的时间。所以如果任务很多，并且每个任务执行的时间比较短，可以调大这个时间，提高线程的利用率 TimeUnit（线程活动保持时间的单位）：可选的单位有天（DAYS），小时（HOURS），分钟（MINUTES），毫秒(MILLISECONDS)，微秒(MICROSECONDS, 千分之一毫秒)和毫微秒(NANOSECONDS, 千分之一微秒)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cassandra Install & Deploy [安装部署]]]></title>
    <url>%2F2016%2F10%2F15%2FCassandra-Install%2F</url>
    <content type="text"><![CDATA[Cassandra是一套开源分布式NoSQL数据库系统。它最初由Facebook开发，用于储存收件箱等简单格式数据，集GoogleBigTable的数据模型与Amazon Dynamo的完全分布式的架构于一身Facebook于2008将 Cassandra 开源，此后，由于Cassandra良好的可扩展性，被Digg、Twitter等知名Web 2.0网站所采纳，成为了一种流行的分布式结构化数据存储方案。 官网 Documentation Datastax Datastax Github 测试环境12345678Ubuntu Server 14.04 x64JDK 1.8+python 2.7.11+Cassandra 3.0.9server1: 192.168.1.110 seedsserver2: 192.168.1.111 seedsserver3: 192.168.1.112 cassandra安装 下载地址 修改cassandra.yaml配置文件12# 每一个节点上新建数据目录mdkir cassandra-data 12345678910111213141516171819202122232425262728293031# 修改参数vim conf/cassandra.yamlcluster_name: 'Cassandra Cluster'hints_directory: /home/wxmimperio/data/cassandra-data/hintsdata_file_directories: - /home/wxmimperio/data/cassandra-data/datacommitlog_directory: /home/wxmimperio/data/cassandra-data/commitlogsaved_caches_directory: /home/wxmimperio/data/cassandra-data/saved_caches- seeds: "192.168.1.110,192.168.1.111"listen_address: 192.168.1.110# 开启thriftstart_rpc: truerpc_address: 192.168.1.110# batch 增加批量插入的一次插入量batch_size_warn_threshold_in_kb: 150batch_size_fail_threshold_in_kb: 1500# 节点感知策略endpoint_snitch: GossipingPropertyFileSnitch 其他节点同步目录123scp -r apache-cassandra-3.0.9 wxmimperio@192.168.1.111:/home/wxmimperio/softwarescp -r apache-cassandra-3.0.9 wxmimperio@192.168.1.112:/home/wxmimperio/software 修改其他节点cassandra.yaml123456789# 192.168.1.111- seeds: "192.168.1.111,192.168.1.110"listen_address: 192.168.1.111rpc_address: 192.168.1.111# 192.168.1.112- seeds: "192.168.1.111,192.168.1.110"listen_address: 192.168.1.112rpc_address: 192.168.1.112 启动cassandra1234567# 每一个节点# 前台启动./bin/cassandra -f# 后台启动./bin/cassandra 检查状态1234567# 在任意节点运行./apache-cassandra-3.0.9/bin/nodetool status-- Address Load Tokens Owns (effective) Host ID RackUN 192.168.1.110 91.9 KB 256 65.5% 7ea5d945-fb2f-410e-b03c-da8a1596d150 rack1UN 192.168.1.111 112.05 KB 256 67.2% da86d35a-9819-4d33-a6df-c378c3872936 rack1UN 192.168.1.112 150.97 KB 256 67.3% a851446d-bb15-44b6-9b2e-33fcf7076279 rack1 CQL操作1./apache-cassandra-3.0.9/bin/cqlsh --request-timeout=500 192.168.1.110 创建Keyspace 123# 多数据中心策略NetworkTopologyStrategy# 副本因子2CREATE KEYSPACE IF NOT EXISTS cassandra_test WITH replication = &#123;'class' : 'NetworkTopologyStrategy', 'dc1' : 2&#125;; 创建表 1234# id、name 为组合primary key# id 为 partition key# name、password 为cluster keyCREATE TABLE IF NOT EXISTS cassandra_test.user(id text, name text, password text,PRIMARY KEY (id,name)) WITH comment='UserTable'; 查看表结构 12345678910111213141516171819202122DESCRIBE TABLE cassandra_test.user;CREATE TABLE cassandra_test.user ( id text, name text, password text, PRIMARY KEY (id, name)) WITH CLUSTERING ORDER BY (name ASC) AND bloom_filter_fp_chance = 0.01 AND caching = &#123;'keys': 'ALL', 'rows_per_partition': 'NONE'&#125; AND comment = 'UserTable' AND compaction = &#123;'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'&#125; AND compression = &#123;'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'&#125; AND crc_check_chance = 1.0 AND dclocal_read_repair_chance = 0.1 AND default_time_to_live = 0 AND gc_grace_seconds = 864000 AND max_index_interval = 2048 AND memtable_flush_period_in_ms = 0 AND min_index_interval = 128 AND read_repair_chance = 0.0 AND speculative_retry = '99PERCENTILE'; 插入数据 1INSERT INTO cassandra_test.user(id,name,password) VALUES('1','wxmimperio','123456'); 查询数据 12345SELECT * FROM cassandra_test.user WHERE id = '1';id | name | password----+------------+---------- 1 | wxmimperio | 123456 更新数据 1234567UPDATE cassandra_test.user SET password = 'abcde' WHERE id = '1' AND name = 'wxmimperio';SELECT * FROM cassandra_test.user WHERE id = '1';id | name | password----+------------+---------- 1 | wxmimperio | abcde 删除数据 1DELETE FROM cassandra_test.user WHERE id = '1' AND name = 'wxmimperio'; 检查数据一致性123456789101112131415# 用192.168.1.111和192.168.1.112启动CQL进行数据查询# 192.168.1.111SELECT * FROM cassandra_test.user WHERE id = '1';id | name | password----+------------+---------- 1 | wxmimperio | 123456# 192.168.1.112SELECT * FROM cassandra_test.user WHERE id = '1';id | name | password----+------------+---------- 1 | wxmimperio | 123456 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Cassandra</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Cassandra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto Install & Deploy [安装部署]]]></title>
    <url>%2F2016%2F08%2F20%2FPresto-Install%2F</url>
    <content type="text"><![CDATA[Presto是什么？Presto是一个开源的分布式SQL查询引擎，适用于交互式分析查询，数据量支持GB到PB字节Presto的设计和编写完全是为了解决像Facebook这样规模的商业数据仓库的交互式分析和处理速度的问题 Presto可以做什么？Presto支持在线数据查询，包括Hive, Cassandra, 关系数据库以及专有数据存储。 一条Presto查询可以将多个数据源的数据进行合并，可以跨越整个组织进行分析Presto以分析师的需求作为目标，他们期望响应时间小于1秒到几分钟。 Presto终结了数据分析的两难选择，要么使用速度快的昂贵的商业方案，要么使用消耗大量硬件的慢速的“免费”方案。 (摘自Presto中文网) 官网 中国官网 项目Github 测试环境123Ubuntu Server 14.04 x64JDK 1.8+Presto 0.151 Presto安装下载Presto 下载地址 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158# 下载wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.151/presto-server-0.151.tar.gz# 解压tar -zxvf presto-server-0.151.tar.gz# 目录结构（apt-get install tree）tree -L 2.├── bin│ ├── launcher│ ├── launcher.properties│ ├── launcher.py│ └── procname├── lib│ ├── aether-api-1.13.1.jar│ ├── aether-connector-asynchttpclient-1.13.1.jar│ ├── aether-connector-file-1.13.1.jar│ ├── aether-impl-1.13.1.jar│ ├── aether-spi-1.13.1.jar│ ├── aether-util-1.13.1.jar│ ├── annotations-2.0.3.jar│ ├── antlr4-runtime-4.5.1.jar│ ├── aopalliance-1.0.jar│ ├── asm-all-5.0.4.jar│ ├── async-http-client-1.6.5.jar│ ├── bootstrap-0.131.jar│ ├── bval-core-0.5.jar│ ├── bval-jsr303-0.5.jar│ ├── cglib-nodep-2.2.2.jar│ ├── commons-beanutils-core-1.8.3.jar│ ├── commons-lang3-3.1.jar│ ├── commons-math3-3.2.jar│ ├── concurrent-0.131.jar│ ├── configuration-0.131.jar│ ├── discovery-0.131.jar│ ├── discovery-server-1.25.jar│ ├── event-0.131.jar│ ├── fastutil-6.5.9.jar│ ├── guava-18.0.jar│ ├── guice-4.0.jar│ ├── guice-multibindings-4.0.jar│ ├── hk2-api-2.4.0-b34.jar│ ├── hk2-locator-2.4.0-b34.jar│ ├── hk2-utils-2.4.0-b34.jar│ ├── http2-client-9.3.11.M0.jar│ ├── http2-common-9.3.11.M0.jar│ ├── http2-hpack-9.3.11.M0.jar│ ├── http2-http-client-transport-9.3.11.M0.jar│ ├── http2-server-9.3.11.M0.jar│ ├── http-client-0.131.jar│ ├── http-server-0.131.jar│ ├── jackson-annotations-2.4.4.jar│ ├── jackson-core-2.4.4.jar│ ├── jackson-databind-2.4.4.jar│ ├── jackson-dataformat-smile-2.4.4.jar│ ├── jackson-datatype-guava-2.4.4.jar│ ├── jackson-datatype-jdk7-2.4.4.jar│ ├── jackson-datatype-jdk8-2.4.4.jar│ ├── jackson-datatype-joda-2.4.4.jar│ ├── jackson-datatype-jsr310-2.4.4.jar│ ├── javassist-3.18.1-GA.jar│ ├── javax.annotation-api-1.2.jar│ ├── javax.inject-1.jar│ ├── javax.servlet-api-3.1.0.jar│ ├── javax.ws.rs-api-2.0.1.jar│ ├── jaxrs-0.131.jar│ ├── jcl-over-slf4j-1.7.12.jar│ ├── jersey-client-2.22.2.jar│ ├── jersey-common-2.22.2.jar│ ├── jersey-container-servlet-2.22.2.jar│ ├── jersey-container-servlet-core-2.22.2.jar│ ├── jersey-guava-2.22.2.jar│ ├── jersey-media-jaxb-2.22.2.jar│ ├── jersey-server-2.22.2.jar│ ├── jetty-alpn-client-9.3.11.M0.jar│ ├── jetty-client-9.3.11.M0.jar│ ├── jetty-http-9.3.11.M0.jar│ ├── jetty-io-9.3.11.M0.jar│ ├── jetty-jmx-9.3.11.M0.jar│ ├── jetty-security-9.3.11.M0.jar│ ├── jetty-server-9.3.11.M0.jar│ ├── jetty-servlet-9.3.11.M0.jar│ ├── jetty-util-9.3.11.M0.jar│ ├── jgrapht-core-0.9.0.jar│ ├── jmx-0.131.jar│ ├── jmx-http-0.131.jar│ ├── jmx-http-rpc-0.118.jar│ ├── jmxutils-1.19.jar│ ├── joda-time-2.8.2.jar│ ├── jol-core-0.2.jar│ ├── joni-2.1.5.1.jar│ ├── json-0.131.jar│ ├── leveldb-0.7.jar│ ├── leveldb-api-0.7.jar│ ├── log-0.131.jar│ ├── log4j-over-slf4j-1.7.12.jar│ ├── logback-core-1.0.13.jar│ ├── log-manager-0.131.jar│ ├── maven-aether-provider-3.0.4.jar│ ├── maven-artifact-3.0.4.jar│ ├── maven-compat-3.0.4.jar│ ├── maven-core-3.0.4.jar│ ├── maven-embedder-3.0.4.jar│ ├── maven-model-3.0.4.jar│ ├── maven-model-builder-3.0.4.jar│ ├── maven-plugin-api-3.0.4.jar│ ├── maven-repository-metadata-3.0.4.jar│ ├── maven-settings-3.0.4.jar│ ├── maven-settings-builder-3.0.4.jar│ ├── netty-3.7.0.Final.jar│ ├── node-0.131.jar│ ├── osgi-resource-locator-1.0.1.jar│ ├── plexus-cipher-1.7.jar│ ├── plexus-classworlds-2.4.jar│ ├── plexus-component-annotations-1.5.5.jar│ ├── plexus-container-default-1.5.5.jar│ ├── plexus-interpolation-1.14.jar│ ├── plexus-sec-dispatcher-1.3.jar│ ├── plexus-utils-2.0.6.jar│ ├── presto-bytecode-0.151.jar│ ├── presto-client-0.151.jar│ ├── presto-main-0.151.jar│ ├── presto-parser-0.151.jar│ ├── presto-spi-0.151.jar│ ├── re2j-td-1.4.jar│ ├── resolver-1.3.jar│ ├── slf4j-api-1.7.12.jar│ ├── slf4j-jdk14-1.7.12.jar│ ├── slice-0.22.jar│ ├── stats-0.131.jar│ ├── trace-token-0.131.jar│ ├── units-0.131.jar│ ├── validation-api-1.1.0.Final.jar│ ├── wagon-provider-api-2.2.jar│ └── xbean-reflect-3.4.jar├── NOTICE├── plugin│ ├── atop│ ├── blackhole│ ├── cassandra│ ├── example-http│ ├── hive-cdh4│ ├── hive-cdh5│ ├── hive-hadoop1│ ├── hive-hadoop2│ ├── jmx│ ├── kafka│ ├── localfile│ ├── ml│ ├── mongodb│ ├── mysql│ ├── postgresql│ ├── raptor│ ├── redis│ ├── teradata-functions│ └── tpch└── README.txt Presto配置 在安装目录中创建一个etc目录。 在这个etc目录中放入以下配置信息： 1mkdir etc node.properties 节点配置：每个节点的环境信息 jvm.config JVM 配置：JVM的命令行选项 config.properties 参数配置：Presto server的参数信息 log.properties 日志信息：配置输出日志级别 catalog目录： configuration forConnectors（数据源）的配置信息 node.properties包含针对于每个节点的特定的配置信息，一个节点就是在一台机器上安装的Presto实例 12345vim etc/node.propertiesnode.environment=productionnode.id=ffffffff-ffff-ffff-ffff-ffffffffffffnode.data-dir=/var/presto/data node.environment：集群名称，所有在同一个集群中的Presto节点必须拥有相同的集群名称 node.id：每个Presto节点的唯一标示。每个节点的node.id都必须是唯一的。在Presto进行重启或者升级过程中每个节点的node.id必须保持不变。如果在一个节点上安装多个Presto实例（例如：在同一台机器上安装多个Presto节点），那么每个Presto节点必须拥有唯一的node.id node.data-dir：数据存储目录的位置（操作系统上的路径），Presto将会把日期和数据存储在这个目录下 jvm.config包含一系列在启动JVM的时候需要使用的命令行选项。这份配置文件的格式是：一系列的选项，每行配置一个单独的选项。由于这些选项不在shell命令中使用。 因此即使将每个选项通过空格或者其他的分隔符分开，java程序也不会将这些选项分开，而是作为一个命令行选项处理 Presto会将查询编译成字节码文件，因此Presto会生成很多class，因此我们我们应该增大Perm区的大小（在Perm中主要存储class）并且要允许Jvm class unloading 1234567891011vim etc/jvm.config-server-Xmx4G-XX:+UseConcMarkSweepGC-XX:+ExplicitGCInvokesConcurrent-XX:+CMSClassUnloadingEnabled-XX:+AggressiveOpts-XX:+HeapDumpOnOutOfMemoryError-XX:OnOutOfMemoryError=kill -9 %p-XX:ReservedCodeCacheSize=150M config.properties包含了Presto server的所有配置信息。 每个Presto server既是一个coordinator也是一个worker。但是在大型集群中，处于性能考虑，建议单独用一台机器作为 coordinator 新版本中已经不使用一下配置: task.max-memory=1GB 改为:query.max-memory=1GB和query.max-memory-per-node=1GB 1234567891011vim etc/config.properties# coordinator最小配置coordinator=truenode-scheduler.include-coordinator=falsehttp-server.http.port=8080query.max-memory=2GBquery.max-memory-per-node=1GBdiscovery-server.enabled=truediscovery.uri=http://example.net:8080 123456789vim etc/config.properties# worker最小配置coordinator=falsehttp-server.http.port=8080query.max-memory=2GBquery.max-memory-per-node=1GBdiscovery.uri=http://example.net:8080 1234567891011vim etc/config.properties# 最小伪分布式，coordinator和worker在一台机coordinator=truenode-scheduler.include-coordinator=truehttp-server.http.port=8080query.max-memory=2GBquery.max-memory-per-node=1GBdiscovery-server.enabled=truediscovery.uri=http://example.net:8080 coordinator：指定是否运维Presto实例作为一个coordinator(接收来自客户端的查询情切管理每个查询的执行过程) node-scheduler.include-coordinator：是否允许在coordinator服务中进行调度工作。对于大型的集群，在一个节点上的Presto server即作为coordinator又作为worke将会降低查询性能。因为如果一个服务器作为worker使用，那么大部分的资源都不会被worker占用，那么就不会有足够的资源进行关键任务调度、管理和监控查询执行 http-server.http.port：指定HTTP server的端口。Presto 使用 HTTP进行内部和外部的所有通讯 query.max-memory：一个提交计划在所有worker节点上占用的内存上限 query.max-memory-per-node：一个查询在一个机器上可使用的内存上限 discovery-server.enabled：Presto 通过Discovery 服务来找到集群中所有的节点。为了能够找到集群中所有的节点，每一个Presto实例都会在启动的时候将自己注册到discovery服务。Presto为了简化部署，并且也不想再增加一个新的服务进程，Presto coordinator 可以运行一个内嵌在coordinator 里面的Discovery 服务。这个内嵌的Discovery 服务和Presto共享HTTP server并且使用同样的端口 discovery.uri：Discovery server的URI。由于启用了Presto coordinator内嵌的Discovery 服务，因此这个uri就是Presto coordinator的uri。修改example.net:8080，根据你的实际环境设置该URI。注意：这个URI一定不能以“/“结尾 log.properties在这个配置文件中允许你根据不同的日志结构设置不同的日志级别。每个logger都有一个名字（通常是使用logger的类的全标示类名）. Loggers通过名字中的“.“来表示层级和集成关系 默认就是INFO,四个级别 DEBUG, INFO, WARN and ERROR 123vim etc/log.propertiescom.facebook.presto=INFO 数据源Catalog PropertiesPresto通过connectors访问数据。这些connectors挂载在catalogs上。 connector 可以提供一个catalog中所有的schema和表 创建catalog目录 1mkdir etc/catalog 创建jmx连接器 123vim etc/catalog/jmx.propertiesconnector.name=jmx 运行Presto 后台进程启动 1bin/launcher start 前台运行， 日志和相关输出将会写入stdout/stderr 1bin/launcher run 启动成功123456789101112131415..................2016-08-21T21:56:25.396+0800 INFO main com.facebook.presto.metadata.CatalogManager -- Loading catalog etc/catalog/jmx.properties --2016-08-21T21:56:25.675+0800 INFO main Bootstrap PROPERTY DEFAULT RUNTIME DESCRIPTION2016-08-21T21:56:25.675+0800 INFO main Bootstrap jmx.dump-period 10.00s 10.00s2016-08-21T21:56:25.675+0800 INFO main Bootstrap jmx.dump-tables [] []2016-08-21T21:56:25.675+0800 INFO main Bootstrap jmx.max-entries 86400 864002016-08-21T21:56:25.675+0800 INFO main Bootstrap2016-08-21T21:56:25.894+0800 INFO main io.airlift.bootstrap.LifeCycleManager Life cycle starting...2016-08-21T21:56:25.895+0800 INFO main io.airlift.bootstrap.LifeCycleManager Life cycle startup complete. System ready.2016-08-21T21:56:25.895+0800 INFO main com.facebook.presto.metadata.CatalogManager -- Added catalog jmx using connector jmx --2016-08-21T21:56:25.897+0800 INFO main com.facebook.presto.security.AccessControlManager -- Loading system access control --2016-08-21T21:56:25.897+0800 INFO main com.facebook.presto.security.AccessControlManager -- Loaded system access control allow-all --2016-08-21T21:56:25.957+0800 INFO main com.facebook.presto.server.PrestoServer ======== SERVER STARTED ======== 运行bin/launcher–help，Presto将会列出支持的命令和命令行选项。 另外可以通过运行bin/launcher–verbose命令，来调试安装是否正确 启动完之后，日志将会写在data目录下，该目录下有如下文件： 12345678.├── etc -&gt; /root/sortware/presto-server-0.151/etc├── plugin -&gt; /root/sortware/presto-server-0.151/plugin└── var ├── log │ └── http-request.log └── run └── launcher.pid 打开WEB界面：http://example.net:8080 常见问题汇总 1.JDK版本过低 出现以下错误通常是因为JDK版本低于1.8造成的，Presto要求JDK版本必须为1.8+ 1234567891011121314Exception in thread "main" java.lang.UnsupportedClassVersionError: com/facebook/presto/server/PrestoServer : Unsupported major.minor version 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:800) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) at java.net.URLClassLoader.access$100(URLClassLoader.java:71) at java.net.URLClassLoader$1.run(URLClassLoader.java:361) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482) 2.node.properties配置问题 出现以下错误通常是因为node.properties文件没有配置，或者配置错误引起的，请参照前文重新配置 123456789101112131415161718192016-08-21T21:50:41.242+0800 ERROR main com.facebook.presto.server.PrestoServer Unable to create injector, see the following errors:1) Error: Invalid configuration property node.environment: may not be null (for class io.airlift.node.NodeConfig.environment) at io.airlift.node.NodeModule.configure(NodeModule.java:34)1 errorcom.google.inject.CreationException: Unable to create injector, see the following errors:1) Error: Invalid configuration property node.environment: may not be null (for class io.airlift.node.NodeConfig.environment) at io.airlift.node.NodeModule.configure(NodeModule.java:34)1 error at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:466) at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155) at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107) at com.google.inject.Guice.createInjector(Guice.java:96) at io.airlift.bootstrap.Bootstrap.initialize(Bootstrap.java:242) at com.facebook.presto.server.PrestoServer.run(PrestoServer.java:111) at com.facebook.presto.server.PrestoServer.main(PrestoServer.java:63) 参考：Presto 0.100 Documentation 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 编程指南 (二) [Spark Programming Guide]]]></title>
    <url>%2F2016%2F05%2F26%2FSpark-Programming-Guide-2%2F</url>
    <content type="text"><![CDATA[Python Programming Guide - Spark 弹性分布式数据集 (RDDs)Spark的核心概念是弹性分布式数据集—Resilient Distributed Datasets，这是一个具有容错能力并且可以进行并行计算的元素集合 对于RDD的基本概念，在 Spark 编程指南 (一) [Spark Programming Guide] 中有详细介绍 RDD的创建用户可以通过两种方式创建RDD： 并行化（Parallelizing）一个已经存在与驱动程序（Driver Program）中的集合（Collection），如set、list 引用外部存储系统上的一个数据集，比如HDFS、HBase，或者任何提供了Hadoop InputFormat的数据源 并行集合（Parallelized Collections）并行集合是在驱动程序中，由SparkContext’s parallelize方法从一个已经存在的迭代器或者集合中创建，集合中的元素会被复制到一个可以进行并行操作的分布式数据集中 例如：如下代码演示如何创建一个元素为1到5的并行数据集 12data = [1, 2, 3, 4, 5]distData = sc.parallelize(data) 这个数据集一旦创建，就可以被并行的操作，例如用下代码就可以对上面列表中元素进行叠加 1distData.reduce(lambda a, b: a + b) 在并行集合中有一个重要的参数—分片数，表示数据集的切分片数；Spark会在集群中为每个分片启动一个任务（task），通常情况下你希望集群中的每个CPU都有2—4个分片，但Spark会根据集群情况自动分配分片数；然而，你也可以通过第二个参数手动设置分片数 1sc.parallelize(data, 10) 外部数据集（External Datasets）PySpark可以从Hadoop所支持的任何存储数据源中构建出分布式数据集，包括本地文件系统、HDFS、Cassandra、HBase、Amazon S3，Spark支持text files、SequenceFiles和任何Hadoop InputFormat Text file RDDs可以通过SparkContext’s textFile方法创建，这个方法接收一个URI文件地址作为参数（或者是一个本地路径、hdfs://,s3n://等），并读取文件作为行的集合，下面是一个调用实例： 1distFile = sc.textFile("data.txt") 一旦创建完成，distFile就可以执行数据集的相关操作。例如：要对文件中的所有行进行求和，就可以用map和reduce操作 1distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b) Spark读取文件时的一些注意事项： 如果用本地文件系统，该文件必须在其工作节点上的相同目录下也可以访问。也可以将文件拷贝到所有的workers节点上，或者使用network-mounted共享文件系统 Spark中所有基于文件的输入方法，包括textFile，都支持在目录上运行，压缩文件和通配符。如：可以使用 textFile(“/my/directory”), textFile(“/my/directory/.txt”), 和 textFile(“/my/directory/.gz”) textFile方法也带有第二个可选参数，其作用是控制文件的分片数。默认情况下，Spark会为文件的每一个block（在HDFS中，block的默认大小为64MB）创建一个分片，或者你也可以通过传入更大的值，来设置更高的分片数，但要注意，你设置的分片数不能比文件的块数小 除了text files，Spark的Python API还支持其他的数据格式： SparkContext.wholeTextFiles 可以让你读取包含多个小text files的目录，并且对每一个文件返回这样的元祖对(filename, content)，而对于对应的textFile，文件的每一行对应着一条上述所说的返回元祖对 RDD.saveAsPickleFile 和 SparkContext.pickleFile支持将RDD保存成由pickled Python对象组成的简单格式，使用批处理的方式对pickle的对象进行序列化，默认的处理批次是10 SequenceFile 和 Hadoop Input/Output 的格式 注意：这个功能目前属于实验性质的，为高级用户而提供。在将来的版本中，可能会因为支持Spark SQL的读写而被取代，且Spark SQL的读写是首选方法 Writable支持 PySpark的SequenceFile支持加载Java中的键值对RDD（key-value），将Writable转换为基本的Java类型，并且通过Pyrolite在结果Java对象上执行pickles序列化操作。当将一个键值对的RDD保存为SequenceFIle时，PySpark会对其进行反操作。它会unpickles Python的对象为Java对象，然后再将它们转换为Writables。 下表中的Writables会被自动地转换： Writable Type Python Type Text unicode str IntWritable int FloatWritable float DoubleWritable float BooleanWritable bool BytesWritable bytearray NullWritable None MapWritable dict 数组不支持开箱（out-of-the-box）操作。在读或写数组时，用户需要指定自定义的ArrayWritable子类。当写数组时，用户也需要指定自定义的转换器（converters），将数组转换为自定义的ArrayWritable子类。当读数组时，默认的转换器会将自定义的ArrayWritable子类转换为Java的Object[]，然后被pickled成Python的元组。如果要获取Python中包含基本数据类型的数组—array.array的话，用户需要为该数组指定自定义的转换器。 保存和加载SequenFiles 同text files类似，SequenceFiles可以被保存和加载到指定路径。可以指定key-value的类型，但对标准的Writables类型则不需要指定 12345rdd = sc.parallelize(range(1,4)).map(lambda x: (x, "a" * x )) rdd.saveAsSequenceFile("path/to/file") sorted(sc.sequenceFile("path/to/file").collect()) # [(1, u'a'), (2, u'aa'), (3, u'aaa')] 保存和加载其他的Hadoop输入/输出格式 PySpark也可以读、写任何Hadoop InputFormat，包括”新”、”旧”两种Hadoop MapReduce APIs。如果需要的话，可以将传递进来的一个Hadoop配置当成一个Python字典 以下是一个Elasticsearch ESInputFormat的样例： 12345678SPARK_CLASSPATH=/path/to/elasticsearch-hadoop.jar./bin/pysparkconf = &#123;"es.resource" :"index/type"&#125; # assumeElasticsearch is running on localhost defaults rdd = sc.newAPIHadoopRDD("org.elasticsearch.hadoop.mr.EsInputFormat",\ "org.apache.hadoop.io.NullWritable","org.elasticsearch.hadoop.mr.LinkedMapWritable", conf=conf)rdd.first() # the result is a MapWritable that isconverted to a Python dict # (u'Elasticsearch ID', &#123;u'field1': True, u'field2': u'Some Text', 'field3': 12345&#125;) 注意:如果这个InputFormat只是简单地依赖于Hadoop配置和输入路径，以及key-value的类型，它就可以很容易地根据上面的表格进行转换，那么这种方法应该可以很好地处理这些情况 如果你有一个定制序列化的二进制数据（例如从Cassandra/HBase加载的数据），那么你首先要做的是用Scala/Java将数据转换为可供Pyrolite的pickler处理的数据，Converter特质提供了这一转换功能。简单地继承该特质，然后在convert方法中实现你自己的转换代码。记住要确保该类和访问InputFormat所需的依赖，都需要被打包到你的Spark作业的jar包，并且包含在PySpark的类路径中。 在Python样例和Converter样例上给出了带自定义转换器的Cassandra/HBase的InputFormat和OutputFormat使用样例。 RDD操作RDDs支持两种操作： 转换（transformations），可以从已有的数据集创建一个新的数据集 动作（actions），在数据集上运行计算后，会向驱动程序返回一个值 例如，map就是一种转换，它将数据集每一个元素都传递给函数，并返回一个新RDD来表示结果。另一方面，reduce是一种动作，通过一些函数将所有的元素聚合起来，并将最终结果返回给驱动程序（不过还有一个并行的reduceByKey，能返回一个分布式数据集）。 Spark中的所有转换都是惰性的，也就是说它们并不会马上执行得到结果。相反的，它们只是记住应用到基础数据集（例如一个文件）上的这些转换动作。只有当触发一个需要返回结果的动作给驱动程序时，这些转换才会真正执行，这种设计让Spark更加有效率地运行。例如，我们对map操作创建的数据集进行reduce操作时，只会向驱动返回reduce操作的结果，而不是返回更大的map操作创建的数据集。 默认情况下，每一个转换过的RDD都会在你对它执行一个动作时被重新计算。而然，你也可以使用持久化或者缓存方法，把一个RDD持久化到内存中。在这种情况下，Spark会在集群中保存相关元素，以便你下次查询这个RDD时能更快速地访问。对于把RDDs持久化到磁盘上，或在集群中复制到多个节点同样是支持的。 基础操作 为了描述RDD的基础操作，可以考虑下面的简单程序： 123lines = sc.textFile("data.txt") lineLengths = lines.map(lambda s: len(s)) totalLength = lineLengths.reduce(lambda a, b: a+ b) 第一行通过一个外部文件定义了一个基本的RDD。这个数据集未被加载到内存，也未执行操作：lines仅仅指向这个文件。 第二行定义了lineLengths作为map转换结果。此外，由于惰性，不会立即计算lineLengths。 最后，我们运行reduce，这是一个动作。这时候，Spark才会将这个计算拆分成不同的task，并运行在独立的机器上，并且每台机器运行它自己的map部分和本地的reducatin，仅仅返回它的结果给驱动程序 如果我们希望以后可以复用lineLengths，可以添加： 1lineLengths.persist() 在reduce执行之前，这将导致lineLengths在第一次被计算之后，被保存在内存中 将函数传入Spark Spark的API，在很大程度上依赖于把驱动程序中的函数传递到集群上运行 有三种推荐方法可以使用： 使用Lambda表达式来编写可以写成一个表达式的简单函数（Lambdas不支持没有返回值的多语句函数或表达式） Spark调用的函数中的Local defs，可以用来代替更长的代码 模块中的顶级函数 例如，如果想传递一个支持使用lambda表达式的更长的函数，可以考虑以下代码： 12345678"""MyScript.py""" if __name__ == "__main__": def myFunc(s): words = s.split(" ") return len(words) sc = SparkContext(...) sc.textFile("file.txt").map(myFunc) 注意：由于可能传递的是一个类实例方法的引用（而不是一个单例对象（singleton object）），在传递方法的时候，应该同时传递包含该方法的对象。 比如： 12345class MyClass(object): def func(self, s): return s def doStuff(self, rdd): return rdd.map(self.func) 这里，如果我们创建了一个类实例new MyClass，并且调用了实例的doStuff方法，该方法中的map处调用了这个MyClass实例的func方法，所以需要将整个对象传递到集群中 类似地，访问外部对象的字段时将引用整个对象： 12345class MyClass(object): def __init__(self): self.field = "Hello" def doStuff(self, rdd): return rdd.map(lambda s: self.field + x) 为了避免这种问题，最简单的方式是把field拷贝到本地变量，而不是去外部访问它： 123def doStuff(self, rdd): field= self.field return rdd.map(lambda s: field + x) 理解闭包 关于Spark的一个更困难的问题是理解当在一个集群上执行代码的时候，变量和方法的范围以及生命周期。修改范围之外变量的RDD操作经常是造成混乱的源头。在下面的实例中我们看一下使用foreach()来增加一个计数器的代码，不过同样的问题也可能有其他的操作引起。 实例 考虑下面的单纯的RDD元素求和，根据是否运行在同一个虚拟机上，它们表现的行为完全不同。一个简单的例子是在local模式（–master=local[n]）下运行Spark对比将Spark程序部署到一个集群上（例如通过spark-submit提交到YARN） 12345678910counter = 0rdd = sc.parallelize(data)# Wrong: Don't do this!!def increment_counter(x): global counter counter += xrdd.foreach(increment_counter)print("Counter value: ", counter) 本地模式 VS 集群模式 上述代码的行为是未定义的,不能按照预期执行。为了执行作业，Spark将RDD操作拆分成多个task，每个任务由一个执行器操作。在执行前，Spark计算闭包。闭包是指执行器要在RDD上进行计算时必须对执行节点可见的那些变量和方法（在这里是foreach()）。这个闭包被序列化并发送到每一个执行器。 闭包中的变量被发送到每个执行器都是被拷贝的，因此，当计数器在foreach函数中引用时，它不再是驱动节点上的那个计数器了。在驱动节点的内存中仍然有一个计数器，但它对执行器来说不再是可见的了。执行器只能看到序列化闭包中的拷贝。因此，计数器最终的值仍然是0，因为所有在计数器上的操作都是引用的序列化闭包中的值。 在这种情况下要确保一个良好定义的行为，应该使用Accumulator。Spark中的累加器是一个专门用来在执行被分散到一个集群中的各个工作节点上的情况下安全更新变量的机制。本指南中的累加器部分会做详细讨论。 一般来说，闭包-构造像循环或者本地定义的方法，不应该用来改变一些全局状态。Spark没有定义或者是保证改变在闭包之外引用的对象的行为。一些这样做的代码可能会在local模式下起作用，但那仅仅是个偶然，这样的代码在分布式模式下是不会按照期望工作的。如果需要一些全局的参数，可以使用累加器。 打印RDD中的元素 另一个常见的用法是使用rdd.foreach(println)方法或者rdd.map(println)方法试图打印出RDD中的元素。 在单台机器上，这样会产生期望的输出并打印出RDD中的元素。然而，在集群模式中，被执行器调用输出到stdout的输出现在被写到了执行器的stdout，并不是在驱动上的这一个，因此驱动上的stdout不会显示这些信息。 在驱动上打印所有的元素，可以使用collect()方法首先把RDD取回到驱动节点如： 1rdd.collect().foreach(println) 然而，这可能导致驱动内存溢出，因为collect()将整个RDD拿到了单台机器上；如果你只需要打印很少几个RDD的元素，一个更安全的方法是使用take()方法： 1rdd.take(100).foreach(println) 使用键值对（key-value）虽然在包含任意类型的对象的RDDs中，可以使用大部分的Spark操作，但也有一些特殊的操作只能在键值(key-value)对的 RDDs上使用 最常见的一个就是分布式的”shuffle”操作，诸如基于key值对元素进行分组或聚合的操作 在Python中，RDDs支持的操作包含Python内置的元组(tuples)操作，比如 (1, 2)。你可以简单地创建这样的元组，然后调用期望的操作 例如，下面的代码在键值(key-value)对上使用 reduceByKey操作来计算在一个文件中每行文本出现的总次数： 123lines = sc.textFile("data.txt") pairs = lines.map(lambda s: (s, 1)) counts = pairs.reduceByKey(lambda a, b: a + b) 我们也可以使用 counts.sortByKey()，例如，按照字典序(alphabetically)排序键值对。最后调用counts.collect()转换成对象的数组形式，返回给驱动程序(driver program) 转换操作下表中列出了 Spark支持的一些常见的转换。详情请参考RDD API文档 (Scala, Java, Python)和 pair RDD函数文档 (Scala, Java) Transformation Meaning map(func) 返回一个新分布式数据集，由每一个输入元素经过func函数计算后组成 filter(func) 返回一个新数据集，由经过func函数计算后返回值为true的元素组成 flatMap(func) 类似于map，但是每一个输入元素可以被映射为0或多个输出元素(因此func应该返回一个序列(Seq)，而不是单一元素) mapPartitions(func) 类似于map，但独立地在RDD的每一个分区（对应块block)上运行，当在类型为T的RDD上运行时，func的函数类型必须是Iterator =&gt; Iterator mapPartitionsWithIndex(func) 类似于mapPartitions，但func带有一个整数参数表示分区(partition)的索引值。当在类型为T的RDD上运行时， func的函数类型必须是(Int, Iterator) =&gt; Iterator sample(withReplacement, fraction, seed) 根据fraction指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed用于指定随机数生成器种子 union(otherDataset) 返回一个新的数据集，新数据集由源数据集和参数数据集的元素联合(union)而成 intersection(otherDataset) 返回一个新的数据集，新数据集由源数据集和参数数据集的元素的交集(intersection)组成} distinct([numTasks])) 返回一个新的数据集，新数据集由源数据集过滤掉多余的重复元素而成 groupByKey([numTasks]) 在一个 (K, V)对的数据集上调用，返回一个 (K, Iterable)对的数据集注意:如果你想在每个key上分组执行聚合（如总和或平均值）操作，使用reduceByKey或combineByKey会产生更好的性能注意:默认情况下，输出的并行数依赖于父RDD(parent RDD)的分区数(number of partitions)。你可以通过传递可选的第二个参数numTasks来设置不同的任务数 reduceByKey(func, [numTasks]) 在一个 (K, V)对的数据集上调用时，返回一个 (K, V)对的数据集，使用指定的reduce函数func将相同 key的值聚合到一起，该函数的类型必须是(V,V) =&gt; V。类似groupByKey，reduce的任务个数是可以通过第二个可选参数来配置的 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 在一个 (K, V)对的数据集上调用时，返回一个(K, U)对的数据集，对每个键的值使用给定的组合函数(combine functions)和一个中性的”零”值进行聚合。允许聚合后的值类型不同于输入的值类型，从而避免了不必要的内存分配。如同groupByKey，可以通过设置第二个可选参数来配置 reduce任务的个数 sortByKey([ascending], [numTasks]) 在一个 (K, V)对的数据集上调用，其中，K必须实现Ordered，返回一个按照Key进行排序的(K, V)对数据集，升序或降序由布尔参数ascending决定 join(otherDataset, [numTasks]) 在类型为(K, V)和(K, W)类型的数据集上调用时，返回一个相同key对应的所有元素对在一起的(K, (V, W))对的数据集。也支持外联(Outer joins)，通过使用leftOuterJoin和rightOuterJoin cogroup(otherDataset, [numTasks]) 在类型为(K, V)和(K, W)的数据集上调用，返回一个(K, Iterable, Iterable)元组(tuples的数据集。这个操作也可以称之为groupWith cartesian(otherDataset) 笛卡尔积，在类型为T和U类型的数据集上调用时，返回一个(T, U)对的数据集(所有元素交互进行笛卡尔积) pipe(command, [envVars]) 以管道(Pipe)方式将RDD的各个分区(partition)传递到shell命令，比如一个Perl或bash脚本中。RDD的元素会被写入进程的标准输入(stdin)，并且将作为字符串的RDD(RDD of strings)，在进程的标准输出(stdout)上输出一行行数据 coalesce(numPartitions) 把RDD的分区数降低到指定的numPartitions。过滤掉一个大数据集之后再执行操作会更加有效 repartition(numPartitions) 随机地对RDD的数据重新洗牌(Reshuffle)，以便创建更多或更少的分区，对它们进行平衡。总是对网络上的所有数据进行洗牌(shuffles) repartitionAndSortWithinPartitions(partitioner) 根据给定的分区器对RDD进行重新分区，在每个结果分区中，将记录按照key值进行排序。这在每个分区中比先调用repartition再排序效率更高，因为它可以推动排序到分牌机器上 动作下表中列出了 Spark支持的一些常见的动作(actions)。详情请参考 RDD API文档(Scala,Java, Python) 和pair RDD函数文档(Scala, Java) Action Meaning reduce(func) 通过函数func(接受两个参数，返回一个参数)，聚集数据集中的所有元素。该函数应该是可交换和可结合的，以便它可以正确地并行计算 collect() 在驱动程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作，并返回一个足够小的数据子集后再使用会比较有用 count() 返回数据集的元素的个数 first() 返回数据集的第一个元素。 (类似于take(1)) take(n) 返回一个由数据集的前n个元素组成的数组。注意，这个操作目前不能并行执行，而是由驱动程序(driver program)计算所有的元素 takeSample(withReplacement,num, [seed]) 返回一个数组，由数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，可以指定可选参数seed，预先指定一个随机数生成器的种子 takeOrdered(n, [ordering]) 返回一个由数据集的前 n个元素，并使用自然顺序或定制顺序对这些元素进行排序 saveAsTextFile(path) 将数据集的元素，以text file(或text file的集合)的形式，保存到本地文件系统的指定目录，Spark会对每个元素调用 toString方法，然后转换为文件中的文本行 saveAsSequenceFile(path)(Java and Scala) 将数据集的元素，以Hadoop sequencefile的格式，保存到各种文件系统的指定路径下，包括本地系统， HDFS或者任何其它hadoop支持的文件系统。该方法只能用于键值(key-value)对的RDDs，或者实现了Hadoop的Writable接口的情况下。在 Scala中，也可以用于支持隐式转换为Writable的类型。(Spark包括了基本类型的转换，例如 Int，Double String，等等) saveAsObjectFile(path)(Java and Scala) 以简单地Java序列化方式将数据集的元素写入指定的路径，对应的可以用SparkContext.objectFile()加载该文件 countByKey() 只对(K,V)类型的RDD有效。返回一个 (K，Int)对的hashmap，其中(K,Int)对表示每一个 key对应的元素个数 foreach(func) 在数据集的每一个元素上，运行 func函数。这通常用于副作用(sideeffects)，例如更新一个累加器变量(accumulator variable)(参见下文)，或者和外部存储系统进行交互 Shuffle操作Spark触发一个事件后进行的一些操作成为Shuffle。Shuffle是Spark重新分配数据的机制，这样它就可以跨分区分组。这通常涉及在执行器和机器之间复制数据，这就使得Shuffle是一个复杂和高代价的操作。 背景 为了理解在洗牌的时候发生了什么，我们可以考虑reduceByKey操作的例子。reduceByKey操作产生了一个新的RDD，在这个RDD中，所有的单个的值被组合成了一个元组，key和执行一个reduce函数后的结果中与这个key有关的所有值。面临的挑战是一个key的所有的值并不都是在同一个分区上的，甚至不是一台机器上的，但是他们必须是可连接的以计算结果 在Spark中，数据一般是不会跨分区分布的，除非是在一个特殊的地方为了某种特定的目的。在计算过程中，单个任务将在单个分区上操作—因此，为了组织所有数据执行单个reduceByKey中的reduce任务，Spark需要执行一个all-to-all操作 它必须读取所有分区，找到所有key的值，并跨分区把这些值放到一起来计算每个key的最终结果—这就叫做Shuffle 尽管在每个分区中新的Shuffle的元素集合是确定性的，分区本身的顺序也同样如此，这些元素的顺序就不一定是了。如果期望在Shuffle后获得可预测的有序的数据，可以使用： mapPartitions来排序每个分区，例如使用，.sorted repartitionAndSortWithinPartitions在重新分区的同时有效地将分区排序 sortBy来创建一个全局排序的RDD 可以引起Shuffle的操作有重分区例如repartition和coalesce，‘ByKey操作（除了计数）像groupByKey和reduceByKey，还有join操作例如cogroup和join 性能影响 Shuffle是一个代价高昂的操作，因为它调用磁盘I/O，数据序列化和网络I/O。要组织shuffle的数据，Spark生成一个任务集合—map任务来组织数据，并使用一组reduce任务集合来聚合它。它的命名来自与MapReduce，但并不直接和Spark的map和reduce操作相关 在内部，单个的map任务的结果被保存在内存中，直到他们在内存中存不下为止。然后，他们基于目标分区进行排序，并写入到一个单个的文件中。在reduce这边，任务读取相关的已经排序的块（blocks） 某些shuffle操作会消耗大量的堆内存，因为他们用在内存中的数据结构在转换操作之前和之后都要对数据进行组织。特别的，reduceByKey和aggregateByKey在map侧创建这些结构，‘ByKey操作在reduce侧生成这些结构。当数据在内存中存不下时，Spark会将他们存储到磁盘，造成额外的磁盘开销和增加垃圾收集（GC） Shuffle也会在磁盘上产生大量的中间文件。在Spark1.3中，这些文件直到Spark停止运行时才会从Spark的临时存储中清理掉，这意味着长时间运行Spark作业会消耗可观的磁盘空间。这些做了之后如果lineage重新计算了，那shuffle不需要重新计算了。在配置Spark上下文时，临时存储目录由spark.local.dir配置参数指定 Shuffle的行为可以通过调整各种配置参数来调整。请看Spark配置指南中的Shuffle Behavior部分 RDD的持久化Spark最重要的一个功能，就是在不同操作间，将一个数据集持久化(persisting) (或缓存caching)到内存中。当你持久化(persist)一个 RDD，每一个节点都会把它计算的所有分区(partitions)存储在内存中，并在对数据集 (或者衍生出的数据集)执行其他动作(actioins)时重用。这将使得后续动作(actions)的执行变得更加迅速(通常快10倍)。缓存(Caching)是用 Spark 构建迭代算法和快速地交互使用的关键 你可以使用persist()或cache()方法来持久化一个RDD。在首次被一个动作(action)触发计算后，它将会被保存到节点的内存中。Spark的缓存是带有容错机制的，如果 RDD丢失任何一个分区的话，会自动地用原先构建它的转换(transformations)操作来重新进行计算 此外，每一个被持久化的RDD都可以用不同的存储级别(storage level)进行存储，比如，允许你持久化数据集到硬盘，以序列化的Java对象(节省空间)存储到内存，跨节点复制，或者以off-heap的方式存储在Tachyon 这些级别的选择，是通过将一个StorageLevel对象 (Scala, Java, Python)传递到persist()方法中进行设置的。cache()方法是使用默认存储级别的快捷方法，也就是 StorageLevel.MEMORY_ONLY (将反序列化 (deserialized)的对象存入内存） 完整的可选存储级别如下： Storage Level Meaning MEMORY_ONLY 将RDD以反序列化(deserialized)的Java对象存储到JVM。如果RDD不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算。这是默认的级别 MEMORY_AND_DISK 将RDD以反序列化(deserialized)的Java对象存储到JVM。如果RDD不能被内存装下，超出的分区将被保存在硬盘上，并且在需要时被读取 MEMORY_ONLY_SER 将RDD以序列化(serialized)的Java对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化(deserialized)的空间利用率更高，尤其当使用快速序列化器(fast serializer)，但在读取时会比较耗CPU MEMORY_AND_DISK_SER 类似于MEMORY_ONLY_SER，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算 DISK_ONLY 只将RDD分区存储在硬盘上 MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. 与上述的存储级别一样，但是将每一个分区都复制到两个集群节点上 OFF_HEAP (experimental) 以序列化的格式 (serialized format) 将RDD存储到Tachyon。相比于MEMORY_ONLY_SER，OFF_HEAP降低了垃圾收集(GC)的开销，并使 executors变得更小而且共享内存池，这在大堆(heaps)和多应用并行的环境下是非常吸引人的。而且，由于RDDs驻留于Tachyon中，executor的崩溃不会导致内存中的缓存丢失。在这种模式下， Tachyon中的内存是可丢弃的。因此，Tachyon不会尝试重建一个在内存中被清除的分块 注意:在Python中，存储对象时总是使用Pickle库来序列化(serialized),而不管你是否选择了一个序列化的级别 Spark也会自动地持久化一些shuffle操作(比如，reduceByKey)的中间数据，即使用户没有调用persist。这么做是为了避免在一个节点上的shuffle过程失败时，重新计算整个输入。如果希望重用它的话,我们仍然建议用户在结果RDD上调用 persist 如何选择存储级别？Spark的存储级别是在满足内存使用和CPU效率权衡上的不同需求 我们建议通过以下方法进行选择： 如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快 如果不行，试着使用MEMORY_ONLY_SER，并且选择一个快速序列化库使对象在有比较高的空间使用率(space-efficient)的情况下，依然可以较快被访问 尽可能不要存储到硬盘上，除非计算数据集的函数的计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，可能和从硬盘中读取差不多快 如果你想有快速的故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求）。所有的存储级别都有通过重新计算丢失的数据来恢复错误的容错机制，但是复制的存储级别可以让你在RDD 上持续地运行任务，而不需要等待丢失的分区被重新计算 在大量的内存或多个应用程序的环境下，试验性的OFF_HEAP模式具有以下几个优点： 允许多个 executors共享 Tachyon中相同的内存池 极大地降低了垃圾收集器(garbage collection)的开销 即使个别的 executors崩溃了，缓存的数据也不会丢失 移除数据Spark会自动监控各个节点上的缓存使用情况，并使用最近最少使用算法(least-recently-used (LRU))删除老的数据分区 如果你想手动移除一个RDD，而不是等它自动从缓存中清除，可以使用RDD.unpersist()方法 参考：Spark Programming Guide 官方文档 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx整合Apache2和Tomcat]]></title>
    <url>%2F2016%2F05%2F24%2FNginx-Apache2-Tomcat%2F</url>
    <content type="text"><![CDATA[通常情况下，如果PHP业务和Java Web业务占用资源都不是很多的情况下，为了节省服务器开销，可以放到一台服务器上。此时可以利用Nginx依旧80端口，做一个请求转发来分别访问PHP应用和Java Web应用 环境及版本12345Ubuntu 14.04 Server x64MySQL 5.5Apache 2.4.7Nginx 1.4.6Tomcat 7.0.69 MySQL+PHP5+Apache2安装请参考：Apache2部署WordPress Nginx安装1sudo apt-get install nginx Tomcat安装Tomcat官方下载 解压文件（opt下） 1tar -zxvf apache-tomcat-7.0.69.tar.gz 开启Tomcat 123cd apache-tomcat-7.0.69/bin/./startup.sh 浏览器输入： http://IP:8080 Nginx配置1sudo vim /etc/nginx/nginx.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#user www-data;worker_processes 4;pid /run/nginx.pid;#error_log /home/jabo/nginx_logs/error.log;#error_log /home/jabo/nginx_logs/error.log notice;#error_log /home/jabo/nginx_logs/error.log info;events &#123; worker_connections 768; # multi_accept on;&#125;http &#123; ## # Basic Settings ## sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; # server_tokens off; # server_names_hash_bucket_size 64; # server_name_in_redirect off; include /etc/nginx/mime.types; default_type application/octet-stream; ## # Logging Settings ## access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## # Gzip Settings ## gzip on; gzip_disable "msie6"; # gzip_vary on; # gzip_proxied any; # gzip_comp_level 6; # gzip_buffers 16 8k; # gzip_http_version 1.1; # gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; ## # nginx-naxsi config ## # Uncomment it if you installed nginx-naxsi ## #include /etc/nginx/naxsi_core.rules; ## # nginx-passenger config ## # Uncomment it if you installed nginx-passenger ## #passenger_root /usr; #passenger_ruby /usr/bin/ruby; ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; #反向代理 upstream tomcat &#123; server 127.0.0.1:8080; &#125; server &#123; listen 80; server_name www.javaapp.com; location / &#123; proxy_pass http://tomcat; #转发到上边定义的java代理中去 proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; upstream php &#123; server 127.0.0.1:9090; &#125; server &#123; listen 80; server_name www.phpapp.com; location / &#123; proxy_pass http://php; #转发到php代理 proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;&#125;#mail &#123;# # See sample authentication script at:# # http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript# # # auth_http localhost/auth.php;# # pop3_capabilities "TOP" "USER";# # imap_capabilities "IMAP4rev1" "UIDPLUS";# # server &#123;# listen localhost:110;# protocol pop3;# proxy on;# &#125;# # server &#123;# listen localhost:143;# protocol imap;# proxy on;# &#125;#&#125; 修改Apache2配置 修改Apache2监听端口 1234sudo vim /etc/apache2/ports.conf# 修改Listen 80Listen 9090 添加虚拟主机 123456789101112131415161718192021222324252627282930313233sudo vim /etc/apache2/sites-available/test.local.conf# 加入以下内容&lt;VirtualHost *:9090&gt; # The ServerName directive sets the request scheme, hostname and port that # the server uses to identify itself. This is used when creating # redirection URLs. In the context of virtual hosts, the ServerName # specifies what hostname must appear in the request's Host: header to # match this virtual host. For the default virtual host (this file) this # value is not decisive as it is used as a last resort host regardless. # However, you must set it for any further virtual host explicitly. #ServerName www.example.com ServerAdmin webmaster@localhost DocumentRoot /var/www/html/wordpress ServerName www.phpapp.com # Available loglevels: trace8, ..., trace1, debug, info, notice, warn, # error, crit, alert, emerg. # It is also possible to configure the loglevel for particular # modules, e.g. #LogLevel info ssl:warn ErrorLog $&#123;APACHE_LOG_DIR&#125;/error.log CustomLog $&#123;APACHE_LOG_DIR&#125;/access.log combined # For most configuration files from conf-available/, which are # enabled or disabled at a global level, it is possible to # include a line for only one particular virtual host. For example the # following line enables the CGI configuration for this host only # after it has been globally disabled with "a2disconf". #Include conf-available/serve-cgi-bin.conf&lt;/VirtualHost&gt; 让test.local.config生效 1sudo a2ensite test.local.conf 重启Apache2 1sudo service apache2 restart 关于WordPress的部署请看： Apache2部署WordPress Java Web测试程序GitHub地址 将程序打成war包，放入Tomcat的 webapps 目录下 其他工作 重启Nginx 1sudo service nginx restart 修改hosts 1234sudo vim /etc/hosts127.0.0.1 www.javaapp.com127.0.0.1 www.phpapp.com 测试 浏览器输入： www.javaapp.com/myTest/index.jsp 转到Java Web测试程序主页 浏览器输入： www.phpapp.com 转到WordPress安装主页 转载请注明出处]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>Nginx</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache2部署WordPress]]></title>
    <url>%2F2016%2F05%2F22%2FApache2-Wordpress%2F</url>
    <content type="text"><![CDATA[WordPress是一种使用PHP语言开发的博客平台，用户可以在支持PHP和MySQL数据库的服务器上架设属于自己的网站。功能强大、扩展性强，这主要得益于其插件众多，易于扩充功能，基本上一个完整网站该有的功能，通过其第三方插件都能实现所有功能。 环境及版本1234Ubuntu 14.04 LTS x64MySQL 5.5Apache 2.4.7WordPress 4.5.2-zh_CN 安装MySQL1sudo apt-get install mysql-server 安装过程中会有几个设置选项： New password for the MySQL “root” user: Repeat password for the MySQL “root” user: 测试MySQL安装成功： 1mysql -u root -p 12345678910111213Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 42Server version: 5.5.49-0ubuntu0.14.04.1 (Ubuntu)Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; 安装Apache21sudo apt-get install apache2 浏览器地址中输入： http://localhost/ 或者 http://127.0.0.1 看到Apache2主页 常见问题： Apache2重启时遇到 1AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message 解决： 1234567sudo vim /etc/apache2/apache2.conf# 输入一下内容ServerName 127.0.0.1# 重启Apache2sudo service apache2 restart 安装php51sudo apt-get install php5 安装PHP其他模块 12# MySQL连接sudo apt-get install php5-mysql 1234567891011121314# 安装phpMyAdminsudo apt-get install phpMyAdmin# Web server to reconfigure automatically# 选择Apache2# Configure database for phpmyadmin with dbconfig-common? # Yes# Password of the database's administrative user: # MySQL application password for phpmyadmin:# Password confirmation:# 创建连接sudo ln -s /usr/share/phpmyadmin /var/www/html 测试PHP和phpMyAdmin安装成功 PHP测试 12345cd /var/www/htmlsudo vim test.php# 加入Hello PHP! 保存退出 浏览器输入： 1http://localhost/test.php phpMyAdmin测试 浏览器输入： 123http://localhost/phpmyadmin# 进入登录页面 安装WordPress官网下载WordPress 解压WordPress 1sudo tar -zxvf wordpress-4.5.2-zh_CN.tar.gz -C /var/www/html/ 创建数据库 123456789# 登录MySQLmysql -u root -p# 创建数据库create database db_wp;# 查看数据库show databases; 配置WordPress 浏览器输入: http://localhost/wordpress 现在就开始 12345数据库名数据库用户名数据库密码数据库主机数据库表前缀 抱歉，我不能写入wp-config.php文件 您可以手工创建wp-config.php文件并将以下信息贴入其中 123456789sudo vim /var/www/html/wordpress/wp-config.php# 将浏览器上的内容复制进去# 将编码格式改成utf8/** 创建数据表时默认的文字编码 */define('DB_CHARSET', 'utf8');# 保存退出，浏览器进行安装 设置WordPress基本信息 1234站点标题用户名密码电子邮箱 登录后进入后台仪表盘，安装结束！ 转载请注明出处]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Apache</tag>
        <tag>WordPress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thriftpy—[RPC文件传输]]]></title>
    <url>%2F2016%2F05%2F14%2Fthriftpy-rpc-file%2F</url>
    <content type="text"><![CDATA[ThriftPy is a pure python implementation of Apache Thrift in a pythonic way. Github地址Documentation 环境及版本12345【Server、Client】Ubuntu 14.04 LTS x64Thriftpy 0.3.8Python 2.7Pycharm 4.5.1 简单Server &amp; ClientThriftpy的使用和Thrift类似，用两台Ubuntu分别做Server和Client，实现跨机器通信 Server123456789101112131415161718192021#!/usr/bin/env python# -*- coding: utf-8 -*-import thriftpyfrom thriftpy.rpc import make_serverimport osclass MyRPC(object): # 提供调用的方法 def print_fun(self,name): str = "Hello " + name return strif __name__ == "__main__": file_path = os.path.abspath("../conf/simple.thrift") # 加载注册文件 simple_thrift = thriftpy.load(file_path, module_name="simple_thrift") server = make_server(simple_thrift.RPCTest, MyRPC(), '192.168.1.105', 6000) print "Thriftpy listening 6000......" server.serve() Client123456789101112131415#!/usr/bin/env python# -*- coding: utf-8 -*-import thriftpyfrom thriftpy.rpc import make_clientimport osif __name__ == "__main__": file_path = os.path.abspath("../conf/simple.thrift") # 加载注册文件 simple_thrift = thriftpy.load(file_path, module_name="simple_thrift") client = make_client(simple_thrift.RPCTest, '192.168.1.105', 6000) print client.print_fun("wxmimperio") Service Conf方法注册文件以.thrif为后缀，Server与Client都必须有，文件目录可以指定 123service RPCTest &#123; string print_fun(1:string name),&#125; Thrift支持的数据类型Thriftpy里都支持 基本数据类型（Java为准） 类型 说明 bool（boolean） 布尔类型(TRUE or FALSE) byte（byte） 8位带符号整数 i16（short） 16位带符号整数 i32（int） 32位带符号整数 i64（long） 64位带符号整数 double（double） 64位浮点数 string（String） 采用UTF-8编码的字符串 复合数据类型（Java为准） 类型 说明 list（java.util.ArrayList） 列表 set（java.util.HashSet） 集合 map（java.util.HashMap） 键值对 特殊数据类型（Java为准） 类型 说明 binary（ByteBuffer） 未经过编码的字节流 struct（结构体） 定义了一个很普通的OOP对象，但是没有继承特性 复合、特殊数据、基本数据类型可以嵌套使用 RPC传输文件 这里我采用Base64进行编码与解码，将文件从Client传向Server Base64是网络上最常见的用于传输8Bit字节代码的编码方式之一，大家可以查看RFC2045～RFC2049，上面有MIME的详细规范。Base64编码可用于在HTTP环境下传递较长的标识信息。例如，在Java Persistence系统Hibernate中，就采用了Base64来将一个较长的唯一标识符（一般为128-bit的UUID）编码为一个字符串，用作HTTP表单和HTTP GET URL中的参数。在其他应用程序中，也常常需要把二进制数据编码为适合放在URL（包括隐藏表单域）中的形式。此时，采用Base64编码具有不可读性，即所编码的数据不会被人用肉眼所直接看到。 Server 接收文件123456789101112131415161718192021222324252627282930#!/usr/bin/env python# -*- coding: utf-8 -*-__author__ = 'wxmimperio'import thriftpyfrom thriftpy.rpc import make_serverimport osclass MyFileRPC(object): def file_server(self, file_list): if file_list: files_path = os.path.abspath("../") for item in file_list: with open(files_path + "/" + item[0], "wb") as file: file.write(item[1].decode("base64")) return "file create!" else: return "file_list is empty!"def server_start(conf_path, server_ip, server_port): # 加载注册文件 file_thrift = thriftpy.load(conf_path, module_name="file_thrift") server = make_server(file_thrift.FileRPC, MyFileRPC(), server_ip, server_port) print "Thriftpy listening " + str(server_port) + "......" server.serve()if __name__ == "__main__": conf_path = os.path.abspath("../conf/file.thrift") server_start(conf_path, "192.168.1.105", 6000) Client 发送文件1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/env python# -*- coding: utf-8 -*-__author__ = 'wxmimperio'import thriftpyfrom thriftpy.rpc import make_clientimport osdef client_start(conf_path, client_ip, client_port): file_thrift = thriftpy.load(conf_path, module_name="file_thrift") return make_client(file_thrift.FileRPC, client_ip, client_port)def opt_files(files_path_list): files_context_list = [] files_name_list = [] for item in files_path_list: with open(item, "rb") as files: files_context_list.append(files.read().encode("base64")) # 获取文件名 files_name_list.append(item.split('/')[-1]) return zip(files_name_list, files_context_list)if __name__ == "__main__": conf_path = os.path.abspath("../conf/file.thrift") client = client_start(conf_path, "192.168.1.105", 6000) files_path_list = [] files_path_list.append(os.path.abspath("../files/markdown.md")) files_path_list.append(os.path.abspath("../files/markdown.pdf")) files_path_list.append(os.path.abspath("../files/photo.jpg")) files_info = opt_files(files_path_list) print client.file_server(files_info) Service Conf123service FileRPC &#123; string file_server(1:list&lt;list&lt;string&gt;&gt; file_list),&#125; 本文Github源码 转载请注明出处]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Thriftpy</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础—文件和流操作]]></title>
    <url>%2F2016%2F03%2F10%2Fpython-file-stream-ops%2F</url>
    <content type="text"><![CDATA[Python打开文件可以用open函数语法：open(filename, mode[, buffering]),返回一个文件对象mode为文件模式，buffering为缓冲，都是可选参数 Demo Github源码 环境及版本123Windows 10 x64Python 2.7Pycharm 4.5.1 文件模式（mode） 值 描述 ‘r’ 只读模式 ‘w’ 只写模式（会覆盖掉已经有的内容） ‘a’ 追加模式（向已有的内容后面进行追加） ‘b’ 二进制模式 ‘t’ 文本模式 ‘+’ 读写模式 ‘U’ 通用匹配换行符模式 文件模式组合 值 描述 文件不存在 ‘r’或’rt’ 只读模式（default） 报错 ‘rb’ 只读模式（针对二进制文件） 报错 ‘w’或’wt’ 只写模式（覆盖已有内容） 创建 ‘wb’ 只写模式（针对二进制文件，覆盖已有内容） 创建 ‘a’ 只写模式（向已有的内容后面进行追加） 创建 ‘r+’ 读写模式（覆盖） 报错 ‘w+’ 读写模式（覆盖） 创建 ‘a+’ 读写模式（向已有的内容后面进行追加） 创建 缓冲（buffering） 值 描述 0或False 无缓冲，读写操作直接针对磁盘 1或True 有缓冲，只有使用flush或者close时才会写入磁盘 大于1 数字表示缓冲区大小（字节） 任意负数 表示默认缓冲区大小 读和写 方法名 描述 read([size]) 以字符串形式返回数据，可选参数size可以指定读取的字节数，如果未指定表示返回全部数据 write(str) 将字符串写入文件 123456789# 读文件file_read = open(file_path, 'r+')print file_read.read()file_read.close()# 写文件file_wtite = open(file_path, 'a+')file_wtite.write("This is a test!".decode("utf-8")) # 中文UTF-8file_wtite.close() 读写行 方法名 描述 readline() 以当前位置，从文件中读取一行，以字符串返回这行数据，offset指向下一行起始位置 readlines([size]) 将文件返回行为列表（list类型），可选参数size用于指定返回的行数；如果size未指定，表示返回所有行数 writelines() 将一个字符串列表一次性写入到文件中，不会加换行符 123456789101112131415# 读一行数据file_readline = open(file_path, 'r+')print file_readline.readline().decode("utf-8")# 读指定行数lines = file_readline.readlines(2)for line in lines: print linefile_readline.close()# 写入一个字符串列表str_list = ["a\n", "b\n", "c\n"]file_writelines = open(file_path, 'a+')file_writelines.writelines(str_list)file_writelines.close() 随机访问 方法名 描述 seek(offset[, whence]) 把当前位置移动到offset和whence定义的位置。offset时一个字节（字符）偏移量，whence默认0表示偏移量从文件的开头计算；whence为1表示相对于当前位置偏移；whence为2表示从文件的尾部向前偏移 tell() 返回当前文件的位置 1234567891011121314# offset偏移offset_path = os.path.abspath('../doc/offset_test.txt')file_offset = open(offset_path, 'w')file_offset.write("abcdefg")file_offset.seek(2)file_offset.write('12346')file_offset.close()file_offset = open(offset_path)print file_offset.read()# tell返回当前文件位置file_tell = open(offset_path, 'w')file_tell.seek(3)print file_tell.tell() 关闭、清空缓冲 方法名 描述 close() 关闭文件流 flush() 强制写入磁盘，并清空缓冲区 通常文件、流的关闭应该在try/except/finally语句中 12345678# 此处打开文件try: # 文件操作except: # 捕获可能出现的异常finally: # 关闭文件 file.close() 还可以使用with语句（上下文管理器） 文件在语句结束后会自动关闭，即使引发了异常也是这样 12with open("file_path") as f: do_something(f) 文件迭代 按字节迭代1234567file_name = open(file_path)while True: char = file_name.read(1) if not char: break process(char)file_name.close() 按行迭代1234567file_name = open(file_path)while True: line = file_name.readline() if not line: break process(line)file_name.close() 迭代全文1234file_name = open(file_path)for line in file_name: process(line)file_name.close() 参考：Python基础教程(第2版·修订版) 转载请注明出处]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>file</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础—日期与时间操作]]></title>
    <url>%2F2016%2F03%2F05%2FPython-date-time-ops%2F</url>
    <content type="text"><![CDATA[Python标准库中提供了datatime模块来操作日期和时间 Demo Github源码 环境及版本123Windows 10 x64Python 2.7Pycharm 4.5.1 datetime.date 表示日期的类，常用的属性有year, month, day year的范围是[1, 9999]，month的范围是[1, 12]，day的最大值根据给定的year, month参数来决定（区分闰年） 常用的属性123# 利用构造函数创建日期now_time = date(2016, 1, 1)print now_time date.max、date.min: 最大、最小日期 12# 最大最小日期print now_time.min, now_time.max date.resolution: 表示日期的最小单位 12# 表示日期的最小单位print now_time.resolution date.year,date.month,date.day: 取得年，月，日 12# 取得年、月、日print "year=&#123;0&#125;,month=&#123;1&#125;,day=&#123;2&#125;".format(now_time.year, now_time.month, now_time.day) 常用的类方法 date.today() 12# 返回一个表示当前本地日期的date对象print date.today() date.fromtimestamp(timestamp) 123# 根据给定的时间戮,返回一个date对象timesstamp = 1451577600print date.fromtimestamp(timesstamp) date.replace(year, month, day) 123# 生成一个新的日期对象replace_date = now_time.replace(year=1990)print replace_date date.timetuple() 12# 返回日期对应的time.struct_time对象print now_time.timetuple() date.weekday() 12# 返回weekday,如果是星期一,返回0,如果是星期2,返回1,以此类推.print now_time.weekday() date.isocalendar() 12# 返回格式如(year,month,day)的元组;print now_time.isocalendar() date.isoformat() 12# 返回格式如'%Y-%m-%d'的字符串print now_time.isoformat() date.strftime(fmt) 123# 自定义格式化字符串fmt = "%Y-%m-%d %H:%M:%S"print now_time.strftime(fmt) 两个日期运算 123date_1 = date(1992, 1, 20)date_2 = date(2000, 5, 3)print "date_2 - date_1 = &#123;0&#125;".format(date_2 - date_1) 比较日期时返回True or False 1234if date_1 &gt; date_2: print "date_1 &gt; date_2"else: print "date_1 &lt; date_2" datetime.time time类表示时间，由hour、minute、second、microsecond（时、分、秒以及微秒）组成 hour的范围为[0, 24)、minute的范围为[0, 60)、second的范围为[0, 60)、microsecond的范围为[0, 1000000) 常用的属性123# 构造函数创建时间now_time = time(10, 12, 13)print now_time time.min、time.max 12# 最小、最大时间print now_time.min, now_time.max time.resolution 12# 时间的最小单位，微妙print now_time.resolution time.hour、time.minute、time.second、time.microsecond 12# 时、分、秒、微秒print now_time.hour, now_time.minute, now_time.second, now_time.microsecond 常用方法 time.replace() 123# 创建一个新的时间对象,用参数指定的时、分、秒、微秒代替原有对象中的属性new_time = now_time.replace(hour=19,second=50)print new_time time.isoformat() 12# 返回型如"%H:%M:%S"格式的字符串表示print now_time.isoformat() time.strftime(fmt) 123# 返回自定义格式化字符串。在下面详细介绍fmt = "%H:%M:%S:%f"print now_time.strftime(fmt) datetime.datetime datetime是date与time的结合体,包括date与time的所有信息 常用属性123# 获取当前时间now_datetime = datetime.now()print now_datetime datetime.min、datetime.max 12# 最小值与最大值print now_datetime.min, now_datetime.max datetime.resolution 12# datetime最小单位print now_datetime.resolution datetime.year、month、day、hour、minute、second、microsecond、tzinfo 1234# 获取datetime的年、月、日、时、分、秒、时区print now_datetime.year, now_datetime.month, now_datetime.dayprint now_datetime.hour, now_datetime.minute, now_datetime.second, now_datetime.microsecondprint now_datetime.tzinfo 常用方法 datetime.today() 12# 返回一个表示当前本地时间的datetime对象print now_datetime.today() datetime.utcnow() 12# 返回一个当前utc时间的datetime对象print now_datetime.utcnow() datetime.fromtimestamp(timestamp[, tz]) 123# 根据时间戮创建一个datetime对象,参数tz指定时区信息timesstamp = 1451577600print datetime.fromtimestamp(timesstamp) datetime.utcfromtimestamp(timestamp) 12# 根据时间戮创建一个datetime对象print datetime.utcfromtimestamp(timesstamp) datetime.combine(date, time) 1234# 根据date和time,创建一个datetime对象new_date = date.today()new_time = time(12, 12, 12)print datetime.combine(new_date, new_time) datetime.strptime(date_string, format) 1234# 将格式字符串转换为datetime对象fmt = "%Y-%m-%d %H:%M:%S:%f"fmt_datetime = "2016-1-2 12:8:52:2569"print datetime.strptime(fmt_datetime, fmt) datetime.strftime(format) 123# 将datetime对象格式化成字符串datetime_str = datetime.today().strftime("%Y-%m-%d %H:%M:%S:%f")print datetime_str datetime.weekday() 12# 获取星期print datetime.today().weekday() datetime.utctimetuple() 12# 转换成UTC的元祖print datetime.today().utctimetuple() 格式化日期符号表 符号 意义 %y 两位数的年份表示（00-99） %Y 四位数的年份表示（000-9999） %m 月份（01-12） %d 月内中的一天（0-31） %H 24小时制小时数（0-23） %I 12小时制小时数（01-12） %M 分钟数（00=59） %S 秒（00-59） %a 本地简化星期名称 %A 本地完整星期名称 %b 本地简化的月份名称 %B 本地完整的月份名称 %c 本地相应的日期表示和时间表示 %j 年内的一天（001-366） %p 本地A.M.或P.M.的等价符 %U 一年中的星期数（00-53）星期天为星期的开始 %w 星期（0-6），星期天为星期的开始 %W 一年中的星期数（00-53）星期一为星期的开始 %x 本地相应的日期表示 %X 本地相应的时间表示 %Z 当前时区的名称 %% %号本身 参考：Python基础教程(第2版·修订版) 转载请注明出处]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>datatime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis监控工具—Redis-stat、RedisLive]]></title>
    <url>%2F2016%2F02%2F25%2FRedis-Monitor-Tools%2F</url>
    <content type="text"><![CDATA[Redis-stat（Ruby）和Redis Live（python）是两款Redis监控工具，下面将介绍如何安装部署这两个工具，监控Redis运行情况 12345测试环境： Ubuntu 14.04 LTS x64 Redis redis-3.0.7.tar.gz Ruby ruby 1.9.3 Python 2.7.6 Redis 安装Redis安装请参照：Redis安装与配置 Redis-stat 安装部署redis-stat is a simple Redis monitoring tool written in Ruby. It is based on INFO command of Redis, and thus generally won’t affect the performance of the Redis instance unlike the other monitoring tools based on MONITOR command. redis-stat allows you to monitor Redis instances either with vmstat-like output from the terminal or with the dashboard page served by its embedded web server. 通常来说，不会像基于MONITOR命令的监控工具一样，对Redis本身有性能上的影响 Github地址 卸载原有Ruby1sudo apt-get autoremove --purge ruby* 安装Ruby1sudo apt-get install ruby-full 安装Redis-stat1gem install redis-stat 基本使用 redis-stat命令参数 1234567891011121314usage: redis-stat [HOST[:PORT] ...] [INTERVAL [COUNT]] -a, --auth=PASSWORD 设置密码 -v, --verbose 显示更多信息 --style=STYLE 输出编码类型: unicode|ascii --no-color 取消ANSI颜色编码 --csv=OUTPUT_CSV_FILE_PATH 以CSV格式存储结果 --es=ELASTICSEARCH_URL 把结果发送到 ElasticSearch: [http://]HOST[:PORT][/INDEX] --server[=PORT] 运行redis-stat的web server (默认端口号: 63790) --daemon 使得redis-stat成为进程。必须使用 --server 选项 --version 显示版本号 --help 显示帮助信息 redis-stat运行命令行监控 1234567redis-statredis-stat 1redis-stat 1 10redis-stat --verboseredis-stat localhost:6380 1 10redis-stat localhost localhost:6380 localhost:6381 5redis-stat localhost localhost:6380 1 10 --csv=/tmp/outpu.csv --verbose Server端运行界面 Web界面中的redis-stat 当设置–server选项之后，redis-stat会在后台启动一个嵌入式的web server(默认端口号：63790)，可以让你在浏览器中监控Redis 12345678redis-stat --serverredis-stat --verbose --server=8080 5# redis-stat server can be daemonizedredis-stat --server --daemon# Kill the daemonkillall -9 redis-stat-daemon Web端运行界面 然后在你的浏览器中输入： 1http://你的Redis IP:63790 RedisLive 安装部署Redis Live is a dashboard application with a number of useful widgets. At it’s heart is a monitoring script that periodically issues INFO and MONITOR command to the redis instances and stores the data for analytics. 长时间运行对Redis性能有所影响 Github地址Real time dashboard for redis 安装运行依赖 tornado 1pip install tornado redis.py 1pip install redis python-dateutil 1pip install python-dateutil 下载RedisLive1git clone https://github.com/kumarnitin/RedisLive.git conf配置进入src目录 1cp redis-live.conf.example ./redis-live.conf 1vim redis-live.conf 12345678910111213141516171819202122232425&#123; "RedisServers": [ &#123; "server" : "你的Redis IP地址", "port" : 6379 &#125; ........ 可以多个 ], "DataStoreType" : "redis", "RedisStatsServer": &#123; "server" : "你的Redis 监控IP地址", "port" : 6379 &#125;, "SqliteStatsStore" : &#123; "path": "to your sql lite file" &#125;&#125; 其中RedisServers为你要监控的redis实例，可以添加多个，RedisStatsServer是存储RedisLive监控数据的实例，如果redis有密码，可以在实例配置中加入password选项；如果没有存储RedisLive数据的实例，需要将DataStoreType改成”DataStoreType” : “sqlite”这种设置 启动RedisLive 启动监控脚本，监控120秒，duration参数是以秒为单位 1sudo ./redis-monitor.py --duration=120 启动webserver。RedisLive使用tornado作为web服务器，所以不需要单独安装服务器Tornado web server 是使用Python编写出來的一个极轻量级、高可伸缩性和非阻塞IO的Web服务器软件 1sudo ./redis-live.py Web运行界面然后在你的浏览器中输入： 1http://你的Redis IP:8888/index.html 转载请注明出处]]></content>
      <categories>
        <category>NoSQL</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>数据库</tag>
        <tag>NoSQL</tag>
        <tag>图形监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis安装与配置]]></title>
    <url>%2F2016%2F02%2F20%2FRedis-Install-Config%2F</url>
    <content type="text"><![CDATA[Redis is an open source (BSD licensed), in-memory data structure store, used as database, cache and message broker.It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs and geospatial indexes with radius queries. Redis作为如今比较火热的NoSQL数据库，在数据的热数据存储和查询方面有着不错的应用，这篇文章将介绍Redis的安装和配置信息。 下载、安装Redis Redis 官方下载 1wget http://download.redis.io/releases/redis-3.0.7.tar.gz 解压 1tar xzf redis-3.0.7.tar.gz 赋予权限 1sudo chmod -R 775 redis-3.0.7/ make 12cd redis-3.0.7/make install tcl 1sudo apt-get install tcl make test 1make test make install 1make PREFIX=/home/server/software/redisInstall install 配置与启动 进入目录 1cd /home/server/software/redisInstall/bin 文件介绍 文件名 介绍 redis-benchmark redis性能测试工具 redis-check-aof aof日志检查工具 redis-check-dump rdb日志检查工具 redis-cli redis客户端连接 redis-server redis服务进程 复制配置文件 1cp ../../redis-3.0.7/redis.conf ../ 启动redis服务 12cd .../bin/redis-server ./redis.conf 123456789101112131415161718192026049:M 24 Mar 16:59:56.768 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.26049:M 24 Mar 16:59:56.768 # Redis can't set maximum open files to 10032 because of OS error: Operation not permitted.26049:M 24 Mar 16:59:56.768 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'. _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 3.0.7 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 26049 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 开启后台进程配置 1234vim redis.conf# 修改如下字段，重启serverdaemonize yes 客户端测试连接 1234./bin/redis-cliset test "This is a redis test"get test redis.conf 配置详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359######################### 通用 ########################## 启动后台进程daemonize yes# 后台进程的pid文件存储位置pidfile /var/run/redis.pid# 默认监听端口port 6379# 在高并发的环境中，为避免慢客户端的连接问题，需要设置一个高速后台日志tcp-backlog 511# 只接受以下绑定的IP请求# Examples:# bind 192.168.1.100 10.0.0.1bind 127.0.0.1# 设置unix监听，默认为空# unixsocket /tmp/redis.sock# unixsocketperm 700#客户端空闲多长时间，关闭链接，0表示不关闭timeout 0# TCP keepalive.# 如果是非零值，当失去链接时，会使用SO_KEEPALIVE发送TCP ACKs 到客户端。# 这个参数有两个作用:# 1.检测断点。# 2.从网络中间设备来看，就是保持链接# 在Linux上，设定的时间就是发送ACKs的周期。# 注意：达到双倍的设定时间才会关闭链接。在其他内核上，周期依赖于内核设置。# 一个比较合理的值为60stcp-keepalive 0# 指定日志级别，以下记录信息依次递减# debug用于开发/测试# verbose没debug那么详细# notice适用于生产线# warning只记录非常重要的信息loglevel notice#日志文件名称，如果为stdout则输出到标准输出端，如果是以后台进程运行则不产生日志logfile ""# 要想启用系统日志记录器，设置一下选项为yes# syslog-enabled no# 指明syslog身份# syslog-ident redis# 指明syslog设备。必须是一个用户或者是local0 ~ local7之一# syslog-facility local0#设置数据库数目，第一个数据库编号为：0databases 16######################### 快照 ########################## 在什么条件下保存数据库到磁盘，条件可以有很多个，满足任何一个条件都会进行快照存储# 在900秒之内有一次key的变化save 900 1# 在300秒之内，有10个key的变化save 300 10# 在60秒之内有10000个key变化save 60 10000# 当持久化失败的时候，是否继续提供服务stop-writes-on-bgsave-error yes# 当写入磁盘时，是否使用LZF算法压缩数据，默认为yesrdbcompression yes# 是否添加CRC64校验到每个文件末尾--花费时间保证安全rdbchecksum yes# 磁盘上数据库的保存名称dbfilename dump.rdb# Redis工作目录，以上数据库保存文件和AOF日志都会写入此目录dir ./######################### 主从同步 ########################## 主从复制，当本机是slave时配置# slaveof &lt;masterip&gt; &lt;masterport&gt;# 当主机需要密码验证时候配置# masterauth &lt;master-password&gt;# 当slave和master丢失链接，或正处于同步过程中。是否响应客户端请求# 设置为yes表示响应# 设置为no，直接返回"SYNC with master in progress"（正在和主服务器同步中）slave-serve-stale-data yes# 设置slave是否为只读。# 注意：即使slave设置为只读，也不能令其暴露在不受信任的网络环境中slave-read-only yes# 无硬盘复制功能repl-diskless-sync no# 等待多个slave一起来请求之间的间隔时间repl-diskless-sync-delay 5# 设置slave给master发送ping的时间间隔# repl-ping-slave-period 10# 设置数据传输I/O，主机数据、ping响应超时时间，默认60s# 这个时间一定要比repl-ping-slave-period大，否则会不断检测到超时# repl-timeout 60# 是否在SYNC后slave socket上禁用TCP_NODELAY？# 如果你设置为yes，Redis会使用少量TCP报文和少量带宽发送数据给slave。# 但是这样会在slave端出现延迟。如果使用Linux内核的默认设置，大概40毫秒。# 如果你设置为no，那么在slave端研究就会减少但是同步带宽要增加。# 默认我们是为低延迟优化的。# 但是如果流量特别大或者主从服务器相距比较远，设置为yes比较合理。repl-disable-tcp-nodelay no# 设置复制的后台日志大小。# 复制的后台日志越大， slave 断开连接及后来可能执行部分复制花的时间就越长。# 后台日志在至少有一个 slave 连接时，仅仅分配一次。# repl-backlog-size 1mb# 在 master 不再连接 slave 后，后台日志将被释放。下面的配置定义从最后一个 slave 断开连接后需要释放的时间（秒）。# 0 意味着从不释放后台日志# repl-backlog-ttl 3600# 设置slave优先级，默认为100# 当主服务器不能正确工作的时候，数字低的首先被提升为主服务器，但是0是禁用选择slave-priority 100# 如果少于 N 个 slave 连接，且延迟时间 &lt;=M 秒，则 master 可配置停止接受写操作。# 例如需要至少 3 个 slave 连接，且延迟 &lt;=10 秒的配置：# min-slaves-to-write 3# min-slaves-max-lag 10# 设置 0 为禁用# 默认 min-slaves-to-write 为 0 （禁用）， min-slaves-max-lag 为 10######################### 安全 ########################## 设置客户端连接密码，因为Redis响应速度可以达到每秒100w次，所以密码要特别复杂# requirepass 1413# 命令重新命名，或者禁用。# 重命名命令为空字符串可以禁用一些危险命令比如：FLUSHALL删除所有数据# 需要注意的是，写入AOF文件或传送给slave的命令别名也许会引起一些问题# rename-command CONFIG ""# 设置客户端连接密码，因为Redis响应速度可以达到每秒100w次，所以密码要特别复杂requirepass 1413# 命令重新命名，或者禁用。# 重命名命令为空字符串可以禁用一些危险命令比如：FLUSHALL删除所有数据# 需要注意的是，写入AOF文件或传送给slave的命令别名也许会引起一些问题# rename-command CONFIG ""######################### 限制 ########################## 设置最多链接客户端数量，默认为10000。# 实际可以接受的请求数目为设置值减去32，这32是Redis为内部文件描述符保留的# maxclients 10000# 设置最多链接客户端数量，默认为10000。# 实际可以接受的请求数目为设置值减去32，这32是Redis为内部文件描述符保留的# maxclients 10000# 设置最大使用内存数量，在把Redis当作LRU缓存时特别有用。# 设置的值要比系统能使用的值要小# 因为当启用删除算法时，slave输出缓存也要占用内存# maxmemory &lt;bytes&gt;#达到最大内存限制时，使用何种删除算法# volatile-lru 使用LRU算法移除带有过期标致的key# allkeys-lru -&gt; 使用LRU算法移除任何key# volatile-random -&gt; 随机移除一个带有过期标致的key# allkeys-random -&gt; 随机移除一个key# volatile-ttl -&gt; 移除最近要过期的key# noeviction -&gt; 不删除key，当有写请求时，返回错误#默认设置为volatile-lru# maxmemory-policy noeviction# LRU和最小TTL算法没有精确的实现# 为了节省内存只在一个样本范围内选择一个最近最少使用的key，可以设置这个样本大小# maxmemory-samples 5######################### AO模式 ########################## AOF和RDB持久化可以同时启用# Redis启动时候会读取AOF文件，AOF文件有更好的持久化保证appendonly no# AOF的保存名称，默认为appendonly.aofappendfilename "appendonly.aof"# 设置何时写入追加日志，又三种模式# no：表示由操作系统决定何时写入。性能最好，但可靠性最低# everysec：表示每秒执行一次写入。折中方案，推荐# always：表示每次都写入磁盘。性能最差，比上面的安全一些# appendfsync alwaysappendfsync everysec# appendfsync no# 当AOF同步策略设定为alway或everysec# 当后台存储进程（后台存储或者AOF日志后台写入）会产生很多磁盘开销# 某些Linux配置会使Redis因为fsync()调用产生阻塞很久# 现在还没有修复补丁，甚至使用不同线程进行fsync都会阻塞我们的同步write(2)调用。# 为了缓解这个问题，使用以下选项在一个BGSAVE或BGREWRITEAOF运行的时候# 可以阻止fsync()在主程序中被调用，no-appendfsync-on-rewrite no# AOF自动重写（合并命令，减少日志大小）# 当AOF日志大小增加到一个特定比率，Redis调用BGREWRITEAOF自动重写日志文件# 原理：Redis 会记录上次重写后AOF文件的文件大小。# 如果刚启动，则记录启动时AOF大小# 这个基本大小会用来和当前大小比较。如果当前大小比特定比率大，就会触发重写。# 你也需要指定一个AOF需要被重写的最小值，这样会避免达到了比率。# 但是AOF文件还很小的情况下重写AOF文件。# 设置为0禁用自动重写auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb#redis在启动时可以加载被截断的AOF文件，而不需要先执行 redis-check-aof 工具aof-load-truncated yes######################### LUA脚本 ########################## Lua脚本的最大执行时间，单位毫秒# 超时后会报错，并且计入日志# 当一个脚本运行时间超过了最大执行时间# 只有SCRIPT KILL和 SHUTDOWN NOSAVE两个命令可以使用。# SCRIPT KILL用于停止没有调用写命令的脚本。# SHUTDOWN NOSAVE是唯一的一个，在脚本的写命令正在执行# 用户又不想等待脚本的正常结束的情况下，关闭服务器的方法。# 以下选项设置为0或负数就会取消脚本执行时间限制lua-time-limit 5000####################### redis集群 ######################### 是否启用集群# cluster-enabled yes# 集群配置文件# 集群配置变更后会自动写入改文件# cluster-config-file nodes-6379.conf# 节点互连超时的阀值 # 节点超时时间，超过该时间无法连接主要Master节点后，会停止接受查询服务 # cluster-node-timeout 15000# 控制从节点FailOver相关的设置,设为0，从节点会一直尝试启动FailOver.# 设为正数，失联大于一定时间（factor*节点TimeOut），不再进行FailOver# cluster-slave-validity-factor 10# 最小从节点连接数# cluster-migration-barrier 1# 默认为Yes,丢失一定比例Key后（可能Node无法连接或者挂掉），集群停止接受写操作# 设置为No，集群丢失Key的情况下仍提供查询服务# cluster-require-full-coverage yes######################### 慢查询 ########################## Redis慢查询日志记录超过设定时间的查询，且只记录执行命令的时间# 不记录I/O操作，比如：和客户端交互，发送回复等。# 时间单位为微妙，1000000微妙 = 1 秒# 设置为负数会禁用慢查询日志，设置为0会记录所有查询命令slowlog-log-slower-than 10000# 日志长度没有限制，但是会消耗内存。超过日志长度后，最旧的记录会被移除# 使用SLOWLOG RESET命令可以回收内存slowlog-max-len 128######################### 延迟监测 ########################## 系统只记录超过设定值的操作，单位是毫秒，0表示禁用该功能 # 可以通过命令“CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;” 直接设置而不需要重启redis latency-monitor-threshold 0######################### 事件通知 ########################## 当事件发生时， Redis 可以通知 Pub/Sub 客户端。# 可以在下表中选择 Redis 要通知的事件类型。事件类型由单个字符来标识：# K Keyspace 事件，以 _keyspace@&lt;db&gt;_ 的前缀方式发布# E Keyevent 事件，以 _keysevent@&lt;db&gt;_ 的前缀方式发布# g 通用事件（不指定类型），像 DEL, EXPIRE, RENAME, …# $ String 命令# s Set 命令# h Hash 命令# z 有序集合命令# x 过期事件（每次 key 过期时生成）# e 清除事件（当 key 在内存被清除时生成）# A g$lshzxe 的别称，因此 ”AKE” 意味着所有的事件# notify-keyspace-events 带一个由 0 到多个字符组成的字符串参数。空字符串意思是通知被禁用。# 例子：启用 list 和通用事件：# notify-keyspace-events Elg# 默认所用的通知被禁用，因为用户通常不需要改特性，并且该特性会有性能损耗。# 注意如果你不指定至少 K 或 E 之一，不会发送任何事件。notify-keyspace-events ""#notify-keyspace-events AKE######################### 高级设置 ########################## 当有少量条目的时候，哈希使用高效内存数据结构。最大的条目也不能超过设定的阈值。# “少量”定义如下：hash-max-ziplist-entries 512hash-max-ziplist-value 64# 和哈希编码一样，少量列表也以特殊方式编码节省内存。“少量”设定如下：list-max-ziplist-entries 512list-max-ziplist-value 64# 集合只在以下情况下使用特殊编码来节省内存# --&gt;集合全部由64位带符号10进制整数构成的字符串组成# 下面的选项设置这个特殊集合的大小。set-max-intset-entries 512# 当有序集合的长度和元素设定为以下数字时，又特殊编码节省内存zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog 稀疏表示字节限制# 这个限制包含了16个字节的头部，当一个HyperLogLog使用sparse representation# 超过了这个显示，它就会转换到dense representation上hll-sparse-max-bytes 3000# 哈希刷新使用每100个CPU毫秒中的1毫秒来帮助刷新主哈希表（顶级键值映射表）。# Redis哈希表使用延迟刷新机制，越多操作，越多刷新。# 如果服务器空闲，刷新操作就不会进行，更多内存会被哈希表占用# 默认每秒进行10次主字典刷新，释放内存。# 如果你有硬性延迟需求，偶尔2毫秒的延迟无法忍受的话。设置为no# 否则设置为yesactiverehashing yes# 客户端输出缓存限制强迫断开读取速度比较慢的客户端# 有三种类型的限制# normal -&gt; 正常# slave -&gt; slave和 MONITOR# pubsub -&gt; 客户端至少订阅了一个频道或者模式# 客户端输出缓存限制语法如下（时间单位：秒）# client-output-buffer-limit &lt;类别&gt; &lt;强制限制&gt; &lt;软性限制&gt; &lt;软性时间&gt;# 达到强制限制缓存大小，立刻断开链接。# 达到软性限制，仍然会有软性时间大小的链接时间# 默认正常客户端无限制，只有请求后，异步客户端数据请求速度快于它能读取数据的速度# 订阅模式和主从客户端又默认限制，因为它们都接受推送。# 强制限制和软性限制都可以设置为0来禁用这个特性client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# 设置Redis后台任务执行频率，比如清除过期键任务。# 设置范围为1到500，默认为10.越大CPU消耗越大，延迟越小。# 建议不要超过100hz 10# 当子进程重写AOF文件，以下选项开启时，AOF文件会每产生32M数据同步一次。# 这有助于更快写入文件到磁盘避免延迟aof-rewrite-incremental-fsync yes 参考：邢栋博客——redis3.0.6配置文件redis.conf参数详解 转载请注明出处]]></content>
      <categories>
        <category>NoSQL</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>数据库</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础-异常处理(except)]]></title>
    <url>%2F2016%2F02%2F06%2Fpython-base-except%2F</url>
    <content type="text"><![CDATA[什么是异常？当程序运行时发生了某些错误，这时就会引发异常。Python中用异常对象(exception object)来表示异常情况，如果异常对象并没有被处理，程序就会终止，并发出回溯(traceback)显示错误信息 环境及版本123Windows 10 x64Python 2.7Pycharm 4.5.1 重要内建异常 异常类名称 描述 Exception 常规异常的基类 KeyboardInterrupt 用户中断执行 OverflowError 数值运算超出最大限制 ZeroDivisionError 在除法或者取模操作时第二个参数为0时引发 AttributeError 特性引用或复制失败时引发 IOError 输入、输出错误时引发 ImportError 导入模块/对象失败 IndexError 在使用不错的索引时引发 NameError 找不到变量（名字）时引发 SyntaxError python代码语法错误 TypeError 内建函数应用于错误对象类型时引发 ValueError 内建函数应用于正确对象，但是该对象使用不合适的值时引发 UnicodeError unicode编码相关错误 Warning 警告类型的基类 异常处理 完整形式12345678910111213try: 有异常语句...except &lt;name1&gt;: 处理name1异常的语句 except &lt;name2&gt;: 处理name2异常的语句 . . .else: 没有触发异常执行的语句finally: 无论有没有异常都会执行的语句 实例12345678910111213141516171819202122232425try: x = input("Enter the first number: ") y = input("Enter the second number: ") print x / yexcept TypeError, e: print "Type Error " + e.messageexcept ZeroDivisionError, e: print "ZeroDivisionError " + e.messageelse: print "No Error"finally: print "Finish"# 结果#（输入5、0）Enter the first number: 1Enter the second number: 0ZeroDivisionError integer division or modulo by zeroFinish#（输入d）Enter the first number: dName Error name 'd' is not definedFinish 当发生错误没有用try/except时，错误就会被传播到调用的函数中；如果调用函数中也没有捕获异常，这些异常就会传递到程序的最顶层 Python会从头到尾以及从左到右查看except子句，然后执行第一个相符的except下的语句 except和else语句只会执行其中的一个，try捕获到异常时会进入except处理异常，try没有捕获到异常会进入else执行相应代码 finally无论是否捕获到异常，都会去执行里面的语句；通常是将释放资源、关闭IO流等操作放在里面 except多种形式 上述例子中except后跟的是异常的名字，except语句还有很多种写法 形式 描述 except: 捕捉全部异常 except name: 捕捉特定name的异常 except name as value: 返回一个异常对象赋值给value except (name1,name2): 一个代码块中捕获多个异常 except (name1,name2) as value: 一个代码块中捕获多个异常，返回一个异常对象赋值给value 1234567891011try: x = input("Enter the first number: ") y = input("Enter the second number: ") print x / yexcept ZeroDivisionError as msg: print "ZeroDivisionError " + msg.message# 结果（输入8、0）Enter the first number: 8Enter the second number: 0ZeroDivisionError integer division or modulo by zero 12345678910try: x = input("Enter the first number: ") y = input("Enter the second number: ") print x / yexcept: print "All Except"# 结果（输入d）Enter the first number: dAll Except 1234567891011try: x = input("Enter the first number: ") y = input("Enter the second number: ") print x / yexcept (ZeroDivisionError,NameError) as msg: print msg.message# 结果（输入8、0）Enter the first number: 8Enter the second number: 0integer division or modulo by zero 只用except:时请注意，这样捕获异常是危险的，因为它会隐藏所有程序员没有想到并且没有做好准备处理的错误；应该使用明确的异常类进行处理 手动引发异常 我们可以使用raise语句来手动引发异常 为了引发异常，可以使用一个类（应该是Exception的子类）或者实例参数调用raise语句 使用类时，程序会自动创建类的一个实例 1234567# 实例raise IOErrorTraceback (most recent call last): File "D:/My office/learning-python/learning_except/except_test.py", line 42, in &lt;module&gt; raise IOErrorIOError 1234567# 实例raise IOError("This is a IOError")Traceback (most recent call last): File "D:/My office/learning-python/learning_except/except_test.py", line 42, in &lt;module&gt; raise IOError("This is a IOError")IOError: This is a IOError 自定义异常 有些时候我们需要创建自己的异常类，这个类就像其他类一样，只要是确保从Exception类继承不管是简介继承还是直接继承，也就是是继承其他内建异常类也是可以的class SomeOneException(Exception): pass 1234567891011class MyException(Exception): def __init__(self,message): self.message = messagetry: raise MyException("This is my exception class")except MyException, e: print e.message# 结果This is my exception class with/as语句 在处理IO流的时候可以用这种方式简化异常处理 123# 基本语法with expression [as variable]: code 在这里的expression要返回一个对象，从而支持环境管理协议。如果选用的as字句存在时，此对象也可返回一个值，赋值给变量名variable. 12345# 实例with open("test.txt") as message: for msg in message: print msg 无论是否引发异常，文件都会被正确关闭 异常处理注意事项 一个try就有一个except或多个except，不要只t捕获不处理 慎用异常：1.找到python的内置异常 2.理解python的内置异常分别对应什么情况 3.阅读你的代码，找到你的代码里可能会抛出内置异常的地方 4.仅对这几行代码做异常处理 假设你无法知道你的代码会抛出什么异常，那么你的异常处理便是无效的 参考：Python基础教程(第2版·修订版) 转载请注明出处]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>except</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text3 问题汇总]]></title>
    <url>%2F2016%2F01%2F29%2FSublime-PackageControl-Failure%2F</url>
    <content type="text"><![CDATA[最近在使用Sublime的时候发现Package Control无法安装插件，所以总结了一下自己的一些问题和解决方法 测试环境123测试环境： Win 10 Sublime Text3 Build 3103 问题一： Package Control：There are no packages available for installation 这种情况通常是被墙了 解决方法： IP反查域名 IP反查域名网址 输入sublime.wbond.net，得到 150.116.34.243 sublime.wbond.net IP和域名一起添加到hosts 1host地址：C:\Windows\System32\drivers\etc\hosts ps: 修改hosts通常需要管理员权限，建议复制一份，添加好后替换 刷新DNS 123456# 打开终端win + Rcmd# 输入以下内容ipconfig /flushdns ps: 到这里问题可能就解决了，如果还没有继续往下看 查看错误原因 12# sublime中快捷键ctrl + ~ 错误如下：1234Package Control: Channel https://packagecontrol.io/channel_v3.json does not appear to be a valid channel file because the &quot;schema_version&quot; is not a valid number.error: Package ControlThere are no packages available for installation 利用同样方法，将packagecontrol.io域名和对应的IP加入hosts，并刷新DNS在浏览器中打开https://packagecontrol.io/channel_v3.json，是否能看到如下内容： 123&#123;&quot;dependencies_cache&quot;: &#123;&quot;https://packagecontrol.io/repository.json&quot;: .................................................................................................................................... ps: 内容非常长，如果不能看到继续往下看 下载channel_v3.json channel_v3.json地址 （提取码：5d43） 搭建服务器，放置文件 可以用apache、nginx等Web服务器在虚拟机上搭建一个文件“仓库”将channel_v3.json文件放入，并获取地址效果是在自己搭建的平台下看见channel_v3.json内容，且虚拟机能Ping通主机 Sublime菜单中选择：首选项—插件设置—Package Control—Settings-user 添加如下代码并保存重启： 123456&#123; &quot;channels&quot;: [ &quot;http://&lt;IP:Port&gt;/目录名/channel_v3.json&quot; ],&#125; 转载请注明出处]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Debian安装VMwareTools]]></title>
    <url>%2F2016%2F01%2F25%2FInstall-VmwareTooles-In-Debian%2F</url>
    <content type="text"><![CDATA[刚开始接触Debian，在VM里安装后发现不能从Win下复制粘贴内容原来是没有安装VMware Tools，下面是如何在Debian下安装VMware Tools的教程 测试环境1234测试环境： debian-8.3.0-amd64 VMware 11.0.0 build-2305329 VMwareTools-9.9.0-2304977.tar.gz 安装sudo、vim12345# 切换rootsu rootapt-get install sudoapt-get install vim 123456789101112# 设置用户权限vim /etc/sudoers# 添加用户名 ALL=(ALL:ALL) ALL# 强制保存退出wq!# 退出rootexit 载入光驱，复制压缩包并解压在VM虚拟机菜单上选择：虚拟机—安装VMware Tools此时会在桌面上显示VMware Tools的加载镜像 打开虚拟光驱，右键打开终端 1234567mkdir /home/用户名/softwarecp VMwareTools-9.9.0-2304977.tar.gz /home/用户名/softwarecd /home/用户名/softwaretar -zxvf VMwareTools-9.9.0-2304977.tar.gz -C /home/用户名/software 安装gcc、linux-headers123456sudo apt-get install gcc# 查看系统linux-headers版本uname -rsudo apt-get install linux-headers-你刚才查询的版本 安装VMwareTools12cd vmware-tools-distribsudo ./vmware-install.pl 之后一直回车，直到安装完成 1234# 安装成功Enjoy,--the VMware team 重启系统1sudo init 6 转载请注明出处]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Debian</tag>
        <tag>VMware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 历史记录 Web 监控 [Spark JobHistory Web UI]]]></title>
    <url>%2F2016%2F01%2F22%2FSpark-JobHistory-Monitoring%2F</url>
    <content type="text"><![CDATA[4040端口只能看到正在运行App的情况，并不能看到App运行结束后的各项数据 Mesos或YARN模式下可以通过各自的Web UI看到JobHistory情况的，但是在Standalone模式下是有自己的JobHistory Web UI 以下内容主要是怎样开启Standalone模式下的JobHistory Web UI 测试环境12345678910测试环境： Ubuntu 14.04 LTS x64 Hadoop：hadoop-2.7.1.tar.gz Spark：spark-1.5.2-bin-hadoop2.6.tgz Scala：scala-2.11.7.tgz 伪分布式 + Yarn hostname IP role spark-master： 192.168.108.20 master &amp; worker 概述 翻译：运行结束后监控 [Viewing After the Fact]在Spark的 Standalone 模式下，有它自己的web UI监控界面，如果一个应用程序已经记录了它的生命周期，在程序运行结束后，Standalone模式下master主机的web UI将会自动重现App的信息UI。 如果Spark运行在Mesos或YARN上，并且App的日志记录存在，依然有可能通过Spark历史服务来重新修改已经完成应用的UI。你可以通过以下内容启动历史服务： 1./sbin/start-history-server.sh 当使用文件系统作为日志文件的存储时，必须提供 spark.history.fs.logDirectory 参数的配置（请看下面的 spark.history.provider 参数详情），并且应该包含子目录，每个子目录表示一个应用程序日志记录的位置。这将会创建一个默认web界面：1http://&lt;server-url&gt;:18080 历史服务的配置参数如下： 环境变量 意义 SPARK_DAEMON_MEMORY 分配给历史服务器的内存（默认：1G） SPARK_DAEMON_JAVA_OPTS 历史服务器JVM选项（默认：none） SPARK_PUBLIC_DNS 设置历史服务器的公共地址，如果不设定，连接应用程序的历史记录会使用服务器内部地址，可能会导致连接失效（默认：none） SPARK_HISTORY_OPTS 历史服务器的spark.history.*配置选项（默认：none） spark.history.* 如下： 参数名称 默认值 意义 spark.history.provider org.apache.spark.deploy.history.FsHistoryProvider 历史应用接入的类名，目前只有一个Spark提供的实现类，它是通过存储在文件系统中的应用程序日志中寻找出来的 spark.history.fs.logDirectory file:/tmp/spark-events 用于存放提供给历史服务器加载的应用程序日志目录 spark.history.fs.update.interval 10s 呈现在历史服务器上的应用数据更新周期，每一个更新操作都会记录日志并做持久化操作 spark.history.retainedApplications 50 保留在UI上应用数目，如果超过这个限制，旧的应用记录将被移除 spark.history.ui.port 18080 历史服务器所绑定的UI端口 spark.history.kerberos.enabled false 用于表明历史服务器是否允许使用kerberos登录，当历史服务器需要访问一个安全Hadoop上的HDFS时，这是很有用的；如果设置为true，则用配置spark.history.kerberos.principal 和 spark.history.kerberos.keytab spark.history.kerberos.principal (none) Kerberos安全配置的名字 spark.history.kerberos.keytab (none) kerberos keytab的存放位置 spark.history.ui.acls.enable false 指定是否应该检查访问控制列表，以授权用户查看应用详情；如果启用，当应用运行后，检查将忽略应用本身设置的spark.ui.acls.enable参数；应用所有者有权查看他们自己的应用情况，当设置了spark.ui.view.acls参数的其他用户也有权查看这类应用；如果禁用，则不会进行检查 spark.history.fs.cleaner.enabled false 指定历史服务器是否应该定期从存储设备上清理日志信息 spark.history.fs.cleaner.interval 1d 设定job历史文件清理周期，只有超过spark.history.fs.cleaner.maxAge参数设定的文件才会被删除 spark.history.fs.cleaner.maxAge 7d Job历史文件超过这个设定时间，将会被删除 UI上的所有信息，都可以通过点击他们的标题头部进行分类，这样就可以方便的看出慢的tasks，数据不均衡等问题。 历史服务器上展现的只是已经完成的Spark Jobs，一个标志Spark Job完成的信号就是Spark Context的停止 （sc.stop()） 在Python中可以使用:1with SparkContext() as sc: 去操纵Spark Context的启动和销毁，并且能让运行完成的应用展示在历史工作的UI上 历史监控的配置将历史Log保存到HDFS上 HDFS上创建 Spark History Log 目录1hadoop fs -mkdir -p /test_log/spark_hislog 修改spark-env.sh1234vim conf/spark-env.sh# 添加PARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://localhost:9000/test_log/spark_hislog" 修改spark-defaults.conf123456789cp spark-defaults.conf.template ./spark-defaults.confvim spark-defaults.conf# 开启日志记录spark.eventLog.enabled true# 日志存储地址spark.eventLog.dir hdfs://localhost:9000/test_log/spark_hislog# 是否进行压缩spark.eventLog.compress true 启动start-history-server.sh1./sbin/start-history-server.sh 访问Web UI1http://&lt;server-url&gt;:18080 参考：Monitoring and Instrumentation 官方文档 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 编程指南 (一) [Spark Programming Guide]]]></title>
    <url>%2F2016%2F01%2F20%2FSpark-Programming-Guide-1%2F</url>
    <content type="text"><![CDATA[Python Programming Guide - Spark Spark应用基本概念每一个运行在cluster上的spark应用程序，是由一个运行main函数的driver program和运行多种并行操作的executes组成 其中spark的核心是弹性分布式数据集（Resilient Distributed Dataset—RDD） Resilient（弹性）：易变化、易计算 Distributed（分布式）：可横跨多台机器，集群分布 Dataset（数据集）：大批量数据的集合 RDD基本概念RDD是逻辑集中的实体，代表一个分区的只读数据集，不可发生改变 【RDD的重要内部属性】 分区列表(partitions)对于一个RDD而言，分区的多少涉及对这个RDD并行计算的粒度，每一个RDD分区的计算都会在一个单独的任务中执行，每一个分区对应一个Task，分区后的数据存放在内存当中 计算每个分区的函数(compute)对于Spark中每个RDD都是以分区进行计算的，并且每个分区的compute函数是在对迭代器进行复合操作，不需要每次计算，直到提交动作触发才会将之前所有的迭代操作进行计算，lineage在容错中有重要作用 对父级RDD的依赖(dependencies)由于RDD存在转换关系，所以新生成的RDD对上一个RDD有依赖关系，RDD之间通过lineage产生依赖关系 【窄依赖】每一个父RDD的分区最多只被子RDD的一个分区所使用，可以类似于流水线一样，计算所有父RDD的分区；在节点计算失败的恢复上也更有效，可以直接计算其父RDD的分区，还可以进行并行计算 子RDD的每个分区依赖于常数个父分区（即与数据规模无关）输入输出一对一的算子，且结果RDD的分区结构不变，主要是map、flatmap输入输出一对一，但结果RDD的分区结构发生了变化，如union、coalesce从输入中选择部分元素的算子，如filter、distinct、subtract、sample 【宽依赖】多个子RDD的分区会依赖于同一个父RDD的分区，需要取得其父RDD的所有分区数据进行计算，而一个节点的计算失败，将会导致其父RDD上多个分区重新计算 子RDD的每个分区依赖于所有父RDD分区对单个RDD基于key进行重组和reduce，如groupByKey、reduceByKey对两个RDD基于key进行jion和重组，如jion 对key-value数据类型RDD的分区器，控制分区策略和分区数(partitioner) partitioner就是RDD的分区函数，即HashPartitioner（哈希分区）和RangePartitioner（区域分区），分区函数决定了每个RDD的分区策略和分区数，并且这个函数只在(k-v)类型的RDD中存在，在非(k-v)结构的RDD中是None 每个数据分区的地址列表(preferredLocations)与Spark中的调度相关，返回的是此RDD的每个partition所出储存的位置，按照“移动数据不如移动计算”的理念，在spark进行任务调度的时候，尽可能将任务分配到数据块所存储的位置 控制操作（control operation）spark中对RDD的持久化操作是很重要的，可以将RDD存放在不同的存储介质中，方便后续的操作可以重复使用。主要有cache、persist、checkpoint，checkpoint接口是将RDD持久化到HDFS中，与persist的区别是checkpoint会切断此RDD之前的依赖关系，而persist会保留依赖关系。checkpoint的两大作用：一是spark程序长期驻留，过长的依赖会占用很多的系统资源，定期checkpoint可以有效的节省资源；二是维护过长的依赖关系可能会出现问题，一旦spark程序运行失败，RDD的容错成本会很高 Python连接SparkSpark 1.6.0 支持 Python 2.6+ 或者 Python 3.4+，它使用标准的CPython解释器, 所以像NumPy这样的C语言类库也可以使用，同样也支持PyPy 2.3+ 可以用spark目录里的bin/spark-submit脚本在python中运行spark应用程序，这个脚本可以加载Java/Scala类库，让你提交应用程序到集群当中。你也可以使用bin/pyspark脚本去启动python交互界面 如果你希望访问HDFS上的数据集，你需要建立对应HDFS版本的PySpark连接。 最后，你的程序需要import一些spark类库：1from pyspark import SparkContext, SparkConf PySpark 要求driver和workers需要相同的python版本，它通常引用环境变量PATH默认的python版本；你也可以自己指定PYSPARK_PYTHON所用的python版本，例如：12PYSPARK_PYTHON=python3.4 bin/pysparkPYSPARK_PYTHON=/opt/pypy-2.5/bin/pypy bin/spark-submit examples/src/main/python/pi.py 初始化Spark一个Spark应用程序的第一件事就是去创建SparkContext对象，它的作用是告诉Spark如何建立一个集群。创建SparkContext之前，先要创建SparkConf对象，SparkConf包含了应用程序的相关信息。 12conf = SparkConf().setAppName(appName).setMaster(master)sc = SparkContext(conf=conf) appName：应用的名称，用户显示在集群UI上 master：Spark、Mesos或者YARN集群的URL，如果是本地运行，则应该是特殊的’local’字符串 在实际运行时，你不会讲master参数写死在程序代码里，而是通过spark-submit来获取这个参数；在本地测试和单元测试中，你仍然需要’local’去运行Spark应用程序 使用Shell在PySpark Shell中，一个特殊SparkContext已经帮你创建好了，变量名是：sc，然而在Shell中创建你自己的SparkContext是不起作用的。 你可以通过–master参数设置master所连接的上下文主机；你也可以通过–py-files参数传递一个用逗号作为分割的列表，将Python中的.zip、.egg、.py等文件添加到运行路径当中；你同样可以通过–packages参数，传递一个用逗号分割的maven列表，来个这个Shell会话添加依赖（例如Spark的包） 任何额外的包含依赖的仓库（如SonaType），都可以通过–repositories参数添加进来。Spark中所有的Python依赖（requirements.txt的依赖包列表），在必要时都必须通过pip手动安装 例如用4个核来运行bin/pyspark：1./bin/pyspark --master local[4] 或者，将code.py添加到搜索路径中（为了后面可以import）：1./bin/pyspark --master local[4] --py-files code.py 通过运行pyspark –help来查看完整的操作帮助信息，在这种情况下，pyspark会调用一个通用的spark-submit脚本 在IPython这样增强Python解释器中，也可以运行PySpark Shell；支持IPython 1.0.0+；在利用IPython运行bin/pyspark时，必须将PYSPARK_DRIVER_PYTHON变量设置成ipython：1PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark 你可以通过PYSPARK_DRIVER_PYTHON_OPTS参数来自己定制ipython命令，比如在IPython Notebook中开启PyLab图形支持：1PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook" ./bin/pyspark 参考：Spark Programming Guide 官方文档 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础—断言(assert)]]></title>
    <url>%2F2016%2F01%2F13%2Fpython-base-assert%2F</url>
    <content type="text"><![CDATA[环境及版本123Windows 10 x64Python 2.7Pycharm 4.5.1 什么是断言 assert和if语句类似，是用来检查一个条件，但不同的是如果它为真，就不做任何事。如果它为假，则会抛出AssertError异常。 123456789a = 10assert a &gt; 0, 'a &gt; 0'assert a &lt; 20, 'a &lt; 20'assert a%2 != 0, 'a is not an even number'Traceback (most recent call last): File "D:/pythonlearning/assertlearning/assert_test.py", line 9, in &lt;module&gt; assert a%2 != 0, 'a is not an even number'AssertionError: a is not an even number 什么时候使用断言 在Stack Overflow上有个问题：Best practice for Python Assert【节选一些回答】 Asserts should be used to test conditions that should never happen. The purpose is to crash early in the case of a corrupt program state. Exceptions should be used for errors that can conceivably happen, and you should almost always create your own Exception classes.——Deestan 简单翻译一下就是（英语不好见谅）： 断言应该使用在某种情况几乎不会发生的条件下（它是为了确保代码的正确性），目的是为了保证这种小概率事件发生时能尽早终止程序 异常应该使用在某些问题预见性的会出现，并且你可以创建自己的异常类 总的来说断言的作用不是用来检查代码的逻辑性错误，所以应该区分断言和异常，不能将需要捕获异常的地方用断言代替 如果你能保证你的代码某部分是正确的，则这个部分就无需用断言，因为即使用了也不会触发 断言做单元测试 在代码中利用断言做小的单元测试是可以的 比如测试某个函数的执行是否正确 123456789def assert_test(a,b): return a + bassert 3 == assert_test(1,2), 'right'assert 3 == assert_test(1,4), 'error'Traceback (most recent call last): File "D://pythonlearning/assertlearning/assert_test.py", line 16, in &lt;module&gt; assert 3 == assert_test(1,4), 'error'AssertionError: error 断言做防御型编程 不是让你的代码防御现在的错误，而是防止在代码修改后引发的错误 比如现在的代码是良好的，但是随着开发的继续，可能某些函数、依赖关系发生变化，可能会导致错误的出现 在这种情况下，可以利用断言使代码发生不兼容的变化时立刻发生错误，防御型编程能帮助人们今早的发现bug 不要用断言的场景 不要用它测试用户提供的数据 不要用断言来检查你觉得在你的程序的常规使用时会出错的地方。断言是用来检查非常罕见的问题。 有的情况下，不用断言是因为它比精确的检查要短，它不应该是懒码农的偷懒方式 不要用它来检查对公共库的输入参数，因为它不能控制调用者，所以不能保证调用者会不会打破双方的约定 不要为你觉得可以恢复的错误用断言。换句话说，不用改在产品代码里捕捉到断言错误 不要用太多断言以至于让代码很晦涩 参考：[1]StackOverflow—Best practice for Python Assert、[2]伯乐在线—Python中何时使用断言 转载请注明出处]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>assert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础—字符串(string)]]></title>
    <url>%2F2016%2F01%2F11%2Fpython-base-str%2F</url>
    <content type="text"><![CDATA[环境及版本123Windows 10 x64Python 2.7Pycharm 4.5.1 字符串(string) 字符串即使一串字符组成的序列 基本概念 引号和转义 字符串中的单引号和双引号没有区别，可以混用 123456789print "hello"print 'hello'print "'hello'"print '"hello"'hellohello'hello'"hello" 转义 当字符串中想要出现单引号、百分号、回车等符号时，需要进行转义 12345678910print "let\'s go"print "100\\2=50"print '\"hello\"'print "this \nis"let's go100\2=50"hello"this is 拼接字符串 通常可以用加号来连接两个字符串 12345str1 = "100+"str2 = "5=105"print str1 + str2100+5=105 长字符串 如果需要跨多行的字符串，可以用’’’和”””来表示 12345678910111213print """123"""print '''abc'''123abc 用这种方式可以进行多行注释 原始字符串 原始字符串对于反斜杠不会转义 原始字符串以r开头 不能在原始字符串的末尾加反斜杠 123print r"let\'s go"let\'s go Unicode字符串 python中普通字符串在内部是以8为的ASCII码储存，而Unicode字符串储存为16位Unicode字符 123456str3 = u'hello world'print str3print isinstance(str3, unicode)hello worldTrue 字符串操作 字符串操作和序列的操作基本相同，但字符串不能进行修改 字符串格式化 基础格式化 基础格式化通常用百分号%来完成 在字符串的左边放一个字符串，右侧是希望被格式化的值 格式化字符串的%s部分称为转换说明符，它们标记了需要插入转换值的位置，s表示格式化为字符串，d表示格式化为整型，f表示格式化为浮点型 12345format_str = "My %s is %s"values = ('name','wxmimperio')print format_str % valuesMy name is wxmimperio 完整格式化 格式化操作符的右侧可以是任意类型，如果是元组或者映射类型，那么字符串格式化将会有所不同。 基本的转换说明符： 1.%字符：表示格式化的开始 2.转换标志：-表示左对齐，+表示在转换值之间加上正负号，””表示正数之前保留空格，0表示转换值若位数不够用0填充 3.点.表示精度 12345678910111213141516171819202122from math import piprint "%d" % 10print "%f" % 12.5print "%s" % 'abc'print "%10f" % piprint "%10.2f" % piprint "%.2f" % piprint "%010.2f" % piprint "%-10.2f" % piprint "% d,% d" % (10,-20)print "%+d,%+d" % (5,-30)1012.500000abc 3.141593 3.143.140000003.143.14 10,-20+5,-30 字符串方法 find方法 可以在一个较长的字符串中查找子串，返回子串所在位置的最左索引，如果没找到返回-1 123456str4 = "My name is wxmimperio"print str4.find("name")print str4.find("we")3-1 还可以由第二个、第三个参数指定索引范围，但不包括第二个索引位置 123print str4.find('is',0,10)8 join方法 优雅的字符串连接方法，它是split方法的逆方法 被连接的序列元素都必须是字符串 1234567list1 = ['1','2','3','4','5','6']print '+'.join(list1)dict1 = &#123;'a','b','c'&#125;print '&amp;'.join(dict1)1+2+3+4+5+6a&amp;c&amp;b lower、upper、title方法 lower将字符串全部变成小写字母后返回 upper将字符串全部变成大写字母后返回 title将字符串全部变成起始字母大写后返回 12345678str5 = "ABCD EFG"print str5.lower()print str5.lower().upper()print str5.title()abcd efgABCD EFGAbcd Efg replace方法 将某个字符串被匹配项替换后，返回替换后字符串 123print str5.replace('A','12345')12345BCD EFG split方法 用来将字符串分割成序列，是join的逆方法 1234str6 = "a+b+-c+d+e-f+g"print str6.split("+")['a', 'b', '-c', 'd', 'e-f', 'g'] strip方法 去除两侧的空格，但不去除内侧的空格 1234str7 = " 1 2 3 4 5 "print str7.strip()1 2 3 4 5 translate方法 用于同时进行多个字符串的替换 123456from string import maketranstable = maketrans('abc','123')str8 = 'abc'print str8.translate(table)123 参考：Python基础教程(第2版·修订版) 转载请注明出处]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>string</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark With Tachyon On Yarn]]></title>
    <url>%2F2016%2F01%2F10%2FSpark-With-Tachyon-on-Yarn%2F</url>
    <content type="text"><![CDATA[Spark在HA情况下，以Tachyon为内存文件系统，如何运行在Yarn上？ 测试环境1234567891011121314测试环境： Ubuntu 14.04 LTS x64 Tachyon：tachyon-0.7.1-bin.tar.gz Hadoop：hadoop-2.7.1.tar.gz Spark：spark-1.5.2-bin-hadoop2.6.tgz Maven：apache-maven-3.3.9-bin.tar.gz Scala：scala-2.11.7.tgz hostname IP role spark-master： 192.168.108.20 master &amp; worker spark-slave1： 192.168.108.21 worker spark-slave2： 192.168.108.22 worker !默认情况全部操作在root下进行 Scala安装 Scala环境变量123456789/** * 对每台主机做如下配置 */vim /etc/profileexport SCALA_HOME=/home/jabo/software/scala-2.11.7export PATH=$&#123;SCALA_HOME&#125;/bin:$PATHsource /etc/profile 测试Scala123scala -versionScala code runner version 2.11.7 -- Copyright 2002-2013, LAMP/EPFL Java环境安装请参考：Ubuntu下安装JDK环境 ZooKeeper集群安装请参考：Zookeeper集群环境搭建 Tachyon集群安装请参考：Tachyon集群部署 Hadoop2.X集群安装请参考：Hadoop集群环境搭建 Tachyon集群High Available请参考：Tachyon集群High Available Spark集群安装 Spark下载下载地址：Spark官方下载地址 下载前请先查看，Tachyon和Spark相关版本支持 spark环境变量123456vim /etc/profileexport SPARK_HOME=/home/jabo/software/spark-1.5.2-bin-hadoop2.6export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbinsource /etc/profile 目录权限1sudo chmod -R 775 spark-1.5.2-bin-hadoop2.6/ spark-env.sh配置文件12345678910111213141516cp ./conf/spark-env.sh.template ./conf/spark-env.shvim ./conf/spark-env.shexport SCALA_HOME=/home/jabo/software/scala-2.11.7export JAVA_HOME=/usr/lib/jvm/javaexport SPARK_MASTER_IP=spark-masterexport SPARK_WORKER_MEMORY=1Gexport SPARK_WORKER_PORT=7077export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop#开启HAexport SPARK_JAVA_OPTS=&quot; -Dtachyon.zookeeper.address=spark-master:2181,spark-slave1:2181,spark-slave2:2181 -Dtachyon.usezookeeper=true $SPARK_JAVA_OPTS&quot; 配置slaves123456cp ./conf/slaves.template ./conf/slavesvim ./conf/slavesspark-masterspark-slave1spark-slave2 新建core-site.xml123456789vim ./conf/core-site.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.tachyon-ft.impl&lt;/name&gt; &lt;value&gt;tachyon.hadoop.TFSFT&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 分发Spark目录分发Spark目录到所有主机 启动Zookeeper集群1234/** * 每台主机上运行 */zkServer.sh start 启动Hadoop集群1234/** * spark-master上运行 */./sbin/start-all.sh 启动Tachyon集群123456/** * spark-master上运行 */./bin/tachyon format./bin/tachyon-start.sh all NoMount 启动Spark集群1./sbin/start-all.sh 查看各自集群情况12345678910111213141516171819202122232425262728293031323334root@spark-master: jps115633 Jps93392 JournalNode95446 TachyonMaster92948 NameNode93756 ResourceManager115442 Worker115246 Master3072 QuorumPeerMain93107 DataNode93932 NodeManager93642 DFSZKFailoverController95643 TachyonWorkerroot@spark-slave1: jps85099 Worker66267 JournalNode3021 QuorumPeerMain65967 NameNode66621 NodeManager85448 Jps67496 TachyonWorker66448 DFSZKFailoverController66088 DataNoderoot@spark-slave2: jps11296 NodeManager13100 Worker11172 JournalNode11050 DataNode2976 QuorumPeerMain13166 Jps11794 TachyonWorker 1234567891011121314151617181920212223242526272829/** * spark-master主机上 *///hadoop addresshttp://spark-master:9000/It looks like you are making an HTTP request to a Hadoop IPC port. This is not the correct port for the web interface on this daemon.//Yarn addresshttp://spark-master:8188左侧栏Nodes查看节点情况左侧栏Applications查看应用执行情况//HDFS addresshttp://spark-master:50070Datanodes查看节点情况//Tachyon addresshttp://spark-master:19999Workers查看节点情况Browse File System查看文件//Spark addresshttp://spark-master:8080/查看节点情况 Spark With Tachyon测试12345678//上传文件至Tachyontachyon tfs copyFromLocal /home/test.txt /test//运行MASTER=spark://spark-master:7077 spark-shellval s = sc.textFile(&quot;tachyon-ft://spark-master:19998/test&quot;)s.saveAsTextFile(&quot;tachyon-ft://activeHost:19998/test_done&quot;) Spark On Yarn测试 cluster模式123456789101112131415161718192021222324/** * 运行过程中，可以通过Yarn WebUI查看Applications运行情况 */ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode cluster \ lib/spark-examples*.jar \ 2 //运行结果16/01/21 11:06:15 INFO yarn.Client: client token: N/A diagnostics: N/A ApplicationMaster host: 192.168.108.20 ApplicationMaster RPC port: 0 queue: default start time: 1453345452519 final status: SUCCEEDED tracking URL: http://spark-master:8088/proxy/application_1453340102205_0006/ user: root16/01/21 11:06:15 INFO util.ShutdownHookManager: Shutdown hook called16/01/21 11:06:15 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-edf14341-3117-47c7-a96d-9741db4824bf client模式1./bin/spark-shell --master yarn --deploy-mode client 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>Tachyon</tag>
        <tag>文件系统</tag>
        <tag>Yarn</tag>
        <tag>High Available</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tachyon集群High Available]]></title>
    <url>%2F2016%2F01%2F09%2Ftachyon-cluster-HA%2F</url>
    <content type="text"><![CDATA[Tachyon如何配置High Available？ 测试环境1234567891011测试环境： Ubuntu 14.04 LTS x64 Tachyon：tachyon-0.7.1-bin.tar.gz Maven：apache-maven-3.3.9-bin.tar.gz hostname IP role spark-master： 192.168.108.20 master &amp; worker spark-slave1： 192.168.108.21 worker spark-slave2： 192.168.108.22 worker !默认情况全部操作在root下进行 安装Zookeeper集群请参考：Zookeeper集群环境搭建 安装Tachyon集群请参考：Tachyon集群部署 安装hadoop2.X集群请参考：Hadoop集群环境搭建 修改Tachyon底层文件系统为HDFS安装Maven下载Maven：Maven官方下载 配置Maven环境变量123456sudo vim /etc/profileMAVEN_HOME=#Maven目录地址PATH=$MAVEN_HOME/bin:$PATHsource /etc/profile 重新编译Tachyon1234/** * 进入Tachyon目录 */mvn -Dhadoop.version=2.7.1 clean package 修改Tachyon配置123456789101112131415vim ./conf/tachyon-env.shexport JAVA_HOME=/usr/lib/jvm/javaexport JAVA=&quot;$JAVA_HOME/bin/java&quot;export TACHYON_MASTER_ADDRESS=spark-masterexport TACHYON_UNDERFS_ADDRESS=hdfs://spark-master:9000export TACHYON_WORKER_MEMORY_SIZE=512MBexport TACHYON_UNDERFS_HDFS_IMPL=org.apache.hadoop.hdfs.DistributedFileSystemexport TACHYON_WORKER_MAX_WORKER_THREADS=2048export TACHYON_MASTER_MAX_WORKER_THREADS=2048export TACHYON_SSH_FOREGROUND=&quot;yes&quot;export TACHYON_WORKER_SLEEP=&quot;0.02&quot;-Dtachyon.usezookeeper=true-Dtachyon.zookeeper.address=spark-master:2181,spark-slave1:2181,spark-slave2:2181 配置slaves和workers12345//打开slaves配置文件，注释掉localhost，添加spark-mastervim ./conf/slaves//打开workers配置文件，注释掉localhost，添加spark-mastervim ./conf/workers 分发Tachyon目录将新编译和配置的Tachyon目录分发给其他主机 初始化Tachyon1./bin/tachyon format 启动Tachyon集群1./bin/tachyon-start.sh all NoMount 文件上传测试1234tachyon tfs copyFromLocal /home/jabo/software/tachyon-0.7.1/docs/index.md /testtachyon tfs ls /6.49 KB 01-19-2016 16:49:03:675 In Memory /test 查看Web UI在master上用浏览器打开：hostname:19999 (spark-master:19999)点击Works页面，会看到三台主机基本信息点击Browse File System，会看到刚才上传的文件 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Tachyon</tag>
        <tag>文件系统</tag>
        <tag>High Available</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础—字典(dict)和集合(set)]]></title>
    <url>%2F2016%2F01%2F08%2Fpython-base-dict-set%2F</url>
    <content type="text"><![CDATA[环境及版本123Windows 10 x64Python 2.7Pycharm 4.5.1 字典(dict) 是python中唯一内建的映射类型，字典中的值是无序的 在Java中也就是map，是key-value键值对 键可以是数字、字符串、元组等不可变元素 字典的创建 字典由多个键值对组成，之间用逗号隔开 键和值之间用冒号分割，而整个dict则用花括号括起 空字典用{}表示 1234age_dict = &#123;'Alex': 22, 'Bob': 21, 'Tom': 25, 'Beth': 18&#125;print age_dict&#123;'Bob': 21, 'Beth': 18, 'Alex': 22, 'Tom': 25&#125; dict类型 可以其他映射的序列来建立字典 1234567student_list = [('name','Alex'),('age',20),('gender','male')]student_dict = dict(student_list)print student_dictprint student_dict['name']&#123;'gender': 'male', 'age': 20, 'name': 'Alex'&#125;Alex 也可以通过关键字参数来创建dict 1234student_list_1 = dict(name='Tom', age=20)print student_list_1&#123;'age': 20, 'name': 'Tom'&#125; 字典操作字典的基本操作与序列类似 len()返回参数中键值对的数量 d[k]返回键k上对应的值 d[k]=x类似于赋值语句，设定键k上的值为x dek d[k]删除键为k的那一项 k in d检查d中是否含有键为k的项 1234567891011121314print len(student_dict)print student_dict['gender']student_dict['age'] = 23print student_dictdel student_dict['age']print student_dictresult = 'name' in student_dictprint result3male&#123;'gender': 'male', 'age': 23, 'name': 'Alex'&#125;&#123;'gender': 'male', 'name': 'Alex'&#125;True 键类型：可以是任意不可变类型，如浮点数、字符串、元组等 自动添加：即使键一开始并不存在，也可以为它赋值，这样dict就会建立新的键值对 成员资格：表达式k in d查找的是键，而不是值 在dict中查找成员资格比在list中效率高 字典的格式化字符串 这是一种模版时的格式化，可以使需要格式化的字符串结构更加清晰 1234hobby_dict = &#123;'Bob':'game','Alex':'tennis','Tom':'basketball'&#125;print "My hobby is %(Tom)s" % hobby_dictMy hobby is basketball 字典方法clear方法 用于清除字典中的所有项，这是一个原地操作，没有返回值 12345clear_dict = &#123;'name':'Bob','age':23&#125;clear_dict.clear()print clear_dict&#123;&#125; copy方法 浅拷贝：返回一个具有相同键值对的新dict，这种方式是引用，指向同一块内存地址 123456789101112light_copy = &#123;'name':'Tom','hobby':['basketball','fishing']&#125;light_copy_dict = light_copy.copy()light_copy_dict['name'] = 'Alex'print light_copyprint light_copy_dict['hobby'].remove('fishing')print light_copyprint light_copy_dict&#123;'hobby': ['basketball', 'fishing'], 'name': 'Tom'&#125;None&#123;'hobby': ['basketball'], 'name': 'Tom'&#125;&#123;'hobby': ['basketball'], 'name': 'Alex'&#125; 深拷贝：复制其包含的所有值，这是重新创建一个dict，内存地址不同 1234567891011from copy import deepcopydeepcopy_dict = &#123;&#125;deepcopy_dict['names'] = ['Tome','Tob','Alex']deepcopy_dict_1 = deepcopy_dict.copy()deepcopy_dict_2 = deepcopy(deepcopy_dict)deepcopy_dict_2['names'].append('Jack')print deepcopy_dict_1print deepcopy_dict_2&#123;'names': ['Tome', 'Tob', 'Alex']&#125;&#123;'names': ['Tome', 'Tob', 'Alex', 'Jack']&#125; fromkeys方法 用于使用给定的键，建立新的字典，每个键都对应一个默认值None 如果不想用None作为默认值，可以自己指定默认值 12345print dict.fromkeys(['name','age'])print dict.fromkeys(['name','age'],'NULL')&#123;'age': None, 'name': None&#125;&#123;'age': 'NULL', 'name': 'NULL'&#125; get方法 通过get可以获取对应键的值，并且在键值不存在时返回默认None 默认值None可以自己指定 1234567print deepcopy_dict.get('xxx')print deepcopy_dict.get('names')print deepcopy_dict.get('yyy','NULL')None['Tome', 'Tob', 'Alex']NULL has_key方法 该方法可以检查字典中是否含有特定的键 python3.0不包括这个函数 12345print deepcopy_dict.has_key('names')print deepcopy_dict.has_key('age')TrueFalse items和iteritems方法 items方法返回所有键值对组成的列表，但并没有顺序，因为dict本身是无序的 iteritems会返回一个迭代器，而不是列表 在一些情况下iteritems会高效很多 123456print hobby_dict.items()for x in hobby_dict.iteritems(): print x, [('Bob', 'game'), ('Alex', 'tennis'), ('Tom', 'basketball')]('Bob', 'game') ('Alex', 'tennis') ('Tom', 'basketball') keys和iterkeys方法 keys方法将字典中的键以列表的形式返回 iterkeys则返回一个迭代器，而不是列表 123456print hobby_dict.keys()for x in hobby_dict.iterkeys(): print x, ['Bob', 'Alex', 'Tom']Bob Alex Tom pop方法 获取对应给定键的值，然后将这个键值对从dict中删除 123456pop_dict = &#123;'name':'Tom','age':20&#125;print pop_dict.pop('name')print pop_dictTom&#123;'age': 20&#125; popitem方法 因为dict是无序的，所以popitem方法是随机弹出一项 同样因为无序，dict中没有append方法 123456popitem_dict = &#123;'name':'Alex','gender':'male','age':20,'hobby':'tennis'&#125;print popitem_dict.popitem()print popitem_dict('gender', 'male')&#123;'age': 20, 'name': 'Alex', 'hobby': 'tennis'&#125; setdefault方法 类似与get方法，能够获得给定键所对应的值 同样还能在不含有给定键的情况下设定相应的键值 12345678910setdefault_dict = &#123;&#125;setdefault_dict.setdefault('name','NULL')print setdefault_dictsetdefault_dict['name'] = 'Tom'print setdefault_dict.setdefault('name','xxx')print setdefault_dict&#123;'name': 'NULL'&#125;Tom&#123;'name': 'Tom'&#125; update方法 可以利用一个dict项更新另一个dict 若有相同的项，则会被替换掉 123456update_dict = &#123;'name':'Alex','gender':'male','age':20,'hobby':'tennis'&#125;change_dict = &#123;'language':'chinese'&#125;update_dict.update(change_dict)print update_dict&#123;'gender': 'male', 'age': 20, 'name': 'Alex', 'language': 'chinese', 'hobby': 'tennis'&#125; values和itervalues方法 values返回dict中值的列表，返回列表中可以包含重复元素 itervalues返回的是一个迭代器，而不是列表 123456print update_dict.values()for x in update_dict.itervalues(): print x, ['male', 20, 'Alex', 'chinese', 'tennis']male 20 Alex chinese tennis 集合(set) set是一个无序不重复的元素集合，基本功能是关系测试和消除重复 集合对象支持union(并集)、intersection(交集)、difference(差集)、sysmmetric difference(对称差集) set是一个只有key的集合 set类型 set可以将传入的序列变成一个set类型 当传入的序列中有重复元素时，会自动去重 1234set_list = [1,2,2,2,3,4,5,5,6]print set(set_list)set([1, 2, 3, 4, 5, 6]) 集合的运算 交集运算 123456//set_1.intersection(set_2)set_1 = set([1,2,3,4,5])set_2 = set([3,4,5,6,7,8])print set_1 &amp; set_2set([3, 4, 5]) 并集运算 1234//set_1.union(set_2)print set_1 | set_2set([1, 2, 3, 4, 5, 6, 7, 8]) 差集运算 1234//set_1.difference(set_2)print set_1 - set_2set([1, 2]) 对称差集 1234//set_1.symmetric_difference(set_2)print set_1 ^ set_2set([1, 2, 6, 7, 8]) 基本操作add和update方法 add用于在set中添加一项 update用于在set中添加多项 12345678set_3 = set([0,1,2,3])set_3.add(4)print set_3set_3.update([5,6])print set_3set([0, 1, 2, 3, 4])set([0, 1, 2, 3, 4, 5, 6]) len方法 返回set的长度 123print len(set_3)7 in方法 检测某个key是否在set中 12345print 0 in set_3print 10 in set_3TrueFalse discard方法 在set中存在这个元素，则删除；返回值为None 1234set_3.discard(1)print set_3set([0, 2, 3, 4, 5, 6]) pop方法 随机返回一个元素，然后从set中删除 12345print set_3.pop()print set_30set([2, 3, 4, 5, 6]) clear方法 清空整个set，返回值为None 1234set_3.clear()print set_3set([]) 参考：Python基础教程(第2版·修订版) 转载请注明出处]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>dict</tag>
        <tag>set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础—列表(list)和元组(tuple)]]></title>
    <url>%2F2016%2F01%2F08%2Fpython-base-list-tuple%2F</url>
    <content type="text"><![CDATA[环境及版本123Windows 10 x64Python 2.7Pycharm 4.5.1 列表(list) 列表是一种可变的有序集合，可以随时添加、修改、删除列表中的元素 list类型 list()实际上是一种类型而不是一个函数，并且使适用于所有类型的序列，可以将传入的内容变为一个列表 1234list_1 = list("I am wxmimperio")print list_1['I', ' ', 'a', 'm', ' ', 'w', 'x', 'm', 'i', 'm', 'p', 'e', 'r', 'i', 'o'] 列表的操作 所有序列的操作方法都适用于列表 元素赋值(修改列表) 类似于C、Java里给数组赋值a[i] = 10，通过索引给元素赋值 但不能给一个不存在的位置赋值，否则会产生越界 12345list_2 = [1,2,3,4,5,6,7,8,9]list_2[1]='a'print list_2[1, 'a', 3, 4, 5, 6, 7, 8, 9] 删除元素 通过del语句来实现元素的删除，当然del还能用于其他序列类型的删除操作 1234del list_2[2]print list_2[1, 'a', 4, 5, 6, 7, 8, 9] 分片赋值操作 结合分片操作，可以一次性给多个元素赋值 12345list_2[6:] = list("hello")print list_2//从index=6开始，替换了8、9，并添加了llo[1, 'a', 4, 5, 6, 7, 'h', 'e', 'l', 'l', 'o'] 也可以在不替换元素的情况下直接插入元素 12345list_3 = [1,6]list_3[1:1] = [2,3,4,5]print list_3[1, 2, 3, 4, 5, 6] 也可以用来删除元素，利用[]空列表完成 1234list_3[1:4] = []print list_3[1, 5, 6] 列表的方法 方法是一个与某些对象相关联的函数 对象名.方法名(参数)，进行调用 append方法 用于在列表的末尾添加新元素 用此方法进行操作时，是直接修改原来的列表，也就是新列表和旧列表指向同一块地址 12345list_4 = ['a','b','c','d','e','f','g']list_4.append(1)print list_4['a', 'b', 'c', 'd', 'e', 'f', 'g', 1] count方法 用来统计某个元素在列表中出现的次数 返回int类型的数值 1234conut = list_4.count('a')print conut1 extend方法 可以在列表的末尾一次性添加多个元素 此方法修改了原来的列表 123456list_4.extend([2,3,4])print list_4print (len(list_4))['a', 'b', 'c', 'd', 'e', 'f', 'g', 1, 2, 3, 4]11 index方法 可以从列表中找出索引位置上元素的值 若索引的元素不再列表中，则会抛出异常 1234print list_4.index('b')#print list_4.index('xx')1 insert方法 用于将元素插入到索引对应的位置 1234list_4.insert(2,'wxm')print list_4['a', 'b', 'wxm', 'c', 'd', 'e', 'f', 'g', 1, 2, 3, 4] pop方法 用于移除列表中的一个元素，默认是最后一个，并且返回该元素的值 pop方法是唯一一个既能改变列表又能返回元素之的方法 通过pop和append方法的组合，可以实现后进先出(LIFO)的数据结构——栈 12345print list_4.pop()print list_4.pop(2)4//弹出最后一个元素wxm//弹出index为2的元素 remove方法 用于移除列表中某个元组值的第一个匹配项 此方法没有返回值，但是修改了原来的列表 123456list_4.insert(3,1)list_4.remove(1)print list_4//只删除了新加入的，位于前面的1['a', 'b', 'c', 'd', 'e', 'f', 'g', 1, 2, 3] reverse方法 将列表中的元素逆序 该方法也没有返回值，但是改变了原列表 利用reversed可以产生一个迭代器 1234list_4.reverse()print list_4[3, 2, 1, 'g', 'f', 'e', 'd', 'c', 'b', 'a'] 1234for x in reversed(list_4): print x, a b c d e f g 1 2 3 sort方法 用于给列表中的元素排序，默认升序 该方法没有返回值，但是改变了原来列表 利用sorted可以产生一个迭代器 123456list_5 = [88,1,9,5,20]list_5_1 = list_5[:]list_5_1.sort()print list_5_1[1, 5, 9, 20, 88] 多因素排序，可选参数key和reverse key表示排序标准，按照什么逻辑进行排序，如长度等 reverse是布尔值True or False，用来指明排序是否需要逆序 12345list_6 = ['xx','ab','g','y23','i85dfa']list_6.sort(key=len,reverse=True)print list_6['i85dfa', 'y23', 'xx', 'ab', 'g'] 元组 元组和列表一样，也是一种序列 不同的是元组的元组值一旦确定就 不能 被修改 要创建元组，必须使用逗号分隔 1234567tuple_1 = 1,2,3print tuple_1tuple_2 = 5,print tuple_2(1, 2, 3)(5,) 空元组则用()表示 1234tuple_empty = ()print tuple_empty() tuple类型 同list类型一样，以一个序列为参数，将传入的参数转化为元组类型 123print tuple([1,2,3,4,5,6])(1, 2, 3, 4, 5, 6) 元组的操作 元组的操作只有创建和访问，因为不能修改，所以它没有append()，insert()这样的方法 123456tuple_3 = tuple('tuple')print tuple_3[1]print tuple_3[2:5]u('p', 'l', 'e') 多元化元组 可以通过元组和其他的序列组合，完成元组的“修改” 123456tuple_4 = tuple([1,2,3,['a','b','c']])tuple_4[3][0] = 'xx'tuple_4[3][1] = 'yy'print tuple_4(1, 2, 3, ['xx', 'yy', 'c']) 元组可以在映射(集合的成员)中当键的作用，因为它不可改变，列表则不行 元组作为很多内建函数和方法的返回值存在，能保证代码的安全性 参考：Python基础教程(第2版·修订版) 转载请注明出处]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>list</tag>
        <tag>tuple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础—序列(sequence)]]></title>
    <url>%2F2016%2F01%2F07%2Fpython-base-sequence%2F</url>
    <content type="text"><![CDATA[环境及版本123Windows 10 x64Python 2.7Pycharm 4.5.1 Python包含6中内建的序列——列表(list)、元组(tuple)、字符串、Unicode字符串、buffer对象和xrange对象 序列是Python中最基本的数据结构，其中可以包含一个或多个元素，也可以没有任何元素，当然也可以包含其他的序列或对象 序列的操作 索引序列中的索引如同C、Java等语言中的数组下标，从0开始递增，元素的值可以通过下标访问，如同数组一样 所有序列都可以通过这种方式进行索引 1234test_list = [1,2,3,4,5,6,7,8]print test_list[0]1 索引为负数时表示从起始端为右边，最后一个元素的位置是-1 123print test_list[-1]8 分片 使用分片，可以访问某个范围内的元素 分片通过冒号:实现 第一个索引表示起始位置(包括)，第二个索引表示终止位置(不包括) 12345print test_list[0:2]print test_list[1:-1][1, 2][2, 3, 4, 5, 6, 7] 起始位置为空，表示从结束位置到起始位置的所有元素 终止位置为空，表示从起始位置到终止位置的所有元素 如果要复制整个序列，则起始位置和终止位置都为空 1234567print test_list[0:]print test_list[:-1]print test_list[:][1, 2, 3, 4, 5, 6, 7, 8][1, 2, 3, 4, 5, 6, 7]//由于终止位置不包含，所以没有最后一个元素[1, 2, 3, 4, 5, 6, 7, 8] 步长(step lengh) 步长的设定通常是隐式的，默认为1 即按照步长的间隔分割序列 步长不能为0，但可以为负数，边说分片从右到左，间隔步长提取元素 12345print test_list[0::2]print test_list[7:2:-1][1, 3, 5, 7][8, 7, 6, 5, 4] 序列相加(+) 两种类型相同的序列才能进行连接操作(相加) 1234test_list_1 = ['a','b','c','d','e','f']print test_list + test_list_1[1, 2, 3, 4, 5, 6, 7, 8, 'a', 'b', 'c', 'd', 'e', 'f'] 序列乘法(*) 用数字x乘以序列，则会生成一个原序列重复x次的新序列 1234none_list = [None]print none_list * 5[None, None, None, None, None] 成员资格(in) in运算符会检查一个对象是否为某个序列的成员 返回值：True or False 12345print 1 in test_listprint 'A' in test_list_1TrueFalse 长度(len)、最大值(max)、最小值(max) len函数返回序列中包含元素的个数 max函数返回序列中最大的元素 min函数返回序列中最小的元素 1234567print len(test_list)print max(test_list_1)print min(test_list_1)8fa 参考：Python基础教程(第2版·修订版) 转载请注明出处]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>sequence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群环境搭建]]></title>
    <url>%2F2016%2F01%2F07%2Fhadoop-deploy-cluster%2F</url>
    <content type="text"><![CDATA[Hadoop如何搭建集群环境？ 测试环境12345678910测试环境： Ubuntu 14.04 LTS x64 Hadoop：hadoop-2.7.1.tar.gz hostname IP role spark-master： 192.168.108.20 master &amp; worker spark-slave1： 192.168.108.21 worker spark-slave2： 192.168.108.22 worker !默认情况全部操作在root下进行 下载解压Hadoop地址：Hadoop官方下载 #配置ssh免密码登录 请参考：Tachyon集群部署 修改目录权限1sudo chmod -R 775 hadoop-2.7.1/ 修改yarn-env.sh配置文件1234vim etc/hadoop/yarn-env.shexport HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_HOME&#125;/lib/native export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot; 修改hadoop-env.sh配置文件12345vim etc/hadoop/hadoop-env.shexport JAVA_HOME=/usr/lib/jvm/javaexport HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_HOME&#125;/lib/native export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot; 修改core-site.xml1234567891011121314151617181920212223vim etc/hadoop/core-site.xml&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为masters --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://masters&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/jabo/software/hadoop-2.7.1/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;spark-master:2181,spark-slave1:2181,spark-slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.native.lib&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364vim etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;!--指定hdfs的nameservice为masters，需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;masters&lt;/value&gt; &lt;/property&gt; &lt;!-- masters下面有两个NameNode，分别是Master，Slave1 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.masters&lt;/name&gt; &lt;value&gt;Master,Slave1&lt;/value&gt; &lt;/property&gt; &lt;!-- Master的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.masters.Master&lt;/name&gt; &lt;value&gt;spark-master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- Master的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.masters.Master&lt;/name&gt; &lt;value&gt;spark-master:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- Slave1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.masters.Slave1&lt;/name&gt; &lt;value&gt;spark-slave1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- Slave1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.masters.Slave1&lt;/name&gt; &lt;value&gt;spark-slave1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://spark-master:8485;spark-slave1:8485;spark-slave2:8485/masters&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/jabo/software/hadoop-2.7.1/hadoop/journal&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode失败自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.masters&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;~/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改slaves12345vim etc/hadoop/slavesspark-masterspark-slave1spark-slave2 修改yarn-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182vim etc/hadoop/yarn-site.xml&lt;configuration&gt; &lt;!-- 指定resourcemanager地址 --&gt; &lt;!-- 开启RM高可靠 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的cluster id --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;RM_HA_ID&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 分别指定RM的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;spark-master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;spark-slave1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;spark-master:2181,spark-slave1:2181,spark-slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;spark-master:8132&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;spark-slave1:8132&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;spark-master:8130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;spark-slave1:8130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;spark-master:8131&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;spark-slave1:8131&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;spark-master:8188&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;spark-slave1:8188&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml1234567891011cp etc/hadoop/mapred-site.xml.template ./etc/hadoop/mapred-site.xmlvim etc/hadoop/mapred-site.xml&lt;configuration&gt; &lt;!-- 指定mr框架为yarn方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hadoop目录分发将Hadoop目录分别复制到其他主机 配置环境变量12345678910111213141516/** * 在每台主机上进行环境变量配置 */sudo vim /etc/profileexport HADOOP_HOME=/home/jabo/software/hadoop-2.7.1export HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOOME/sbin:$HADOOP_HOME/libexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;source /etc/profile 启动zookeeper集群1234/** * 启动每个节点 */zkServer.sh start 启动journalnode12345./sbin/hadoop-daemons.sh start journalnodespark-slave1: starting journalnode, logging to /home/jabo/software/hadoop-2.7.1/logs/hadoop-root-journalnode-spark-slave1.outspark-slave2: starting journalnode, logging to /home/jabo/software/hadoop-2.7.1/logs/hadoop-root-journalnode-spark-slave2.outspark-master: starting journalnode, logging to /home/jabo/software/hadoop-2.7.1/logs/hadoop-root-journalnode-spark-master.out 格式化HDFS12hadoop namenode -formatscp -r /home/jabo/software/hadoop-2.7.1/hadoop/tmp/ spark-slave1:/home/jabo/software/hadoop-2.7.1/hadoop/ 格式化zk1hdfs zkfc -formatZK 启动HDFS1./sbin/start-dfs.sh 启动YARN1./sbin/start-yarn.sh 测试HDFS1234hadoop fs -put /etc/profile /profilehadoop fs -ls /-rw-r--r-- 3 root supergroup 1928 2016-01-07 16:36 /profile 测试Yarn12hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /profile /outhadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar pi 20 10 查看进程123456789101112131415161718192021222324252627root@spark-master:/home/jabo/software/hadoop-2.7.1# jps4196 NodeManager4236 Jps3934 DFSZKFailoverController4055 ResourceManager3534 DataNode3396 NameNode2879 QuorumPeerMain3077 JournalNoderoot@spark-slave1:/home/jabo# jps3811 Jps3357 DataNode3106 JournalNode3697 NodeManager3256 NameNode3583 DFSZKFailoverController2945 QuorumPeerMainroot@spark-slave2:/home/jabo# jps3197 DataNode3087 JournalNode3390 NodeManager3498 Jps2924 QuorumPeerMain 统计访问1234http://spark-master:50070NameNode &apos;Master:9000&apos; (active)http://spark-slave1:50070NameNode &apos;Slave1:9000&apos; (standby) Hadoop集群情况1http://spark-master:8188 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>开源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper集群环境搭建]]></title>
    <url>%2F2016%2F01%2F06%2Fzookeeper-deploy-cluster%2F</url>
    <content type="text"><![CDATA[测试环境12345678910测试环境： Ubuntu 14.04 LTS x64 ZooKeeper：zookeeper-3.4.6.tar.gz hostname IP role spark-master： 192.168.108.20 master &amp; worker spark-slave1： 192.168.108.21 worker spark-slave2： 192.168.108.22 worker !默认情况全部操作在root下进行 下载解压ZooKeeper地址：ZooKeeper官方下载 #配置ssh免密码登录 请参考：Tachyon集群部署 修改目录权限1sudo chmod -R 775 zookeeper-3.4.6/ 创建目录结构进入ZooKeeper目录 创建数据目录 1mkdir data 创建日志目录 1mkdir logs 修改zoo.cfg配置文件1234567891011cp ./conf/zoo_sample.cfg ./conf/zoo.cfgvim ./conf/zoo.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/home/jabo/software/zookeeper-3.4.6/data #刚才创建的data目录clientPort=2181server.1=spark-master:2888:3888server.2=spark-slave1:2888:3888server.3=spark-slave2:2888:3888 ZooKeeper目录分发将ZooKeeper目录分别复制到其他主机 ZooKeeper环境变量123456789101112/** * 分别配置每台主机的环境变量 */sudo vim /etc/profile //添加如下信息export ZOO_HOME=/home/jabo/software/zookeeper-3.4.6export ZOO_LOG_DIR=$ZOO_HOME/logsexport PATH=$ZOO_HOME/bin:$PATH//使配置生效source /etc/profile 配置节点ID123456/** * master切换至root下进行 */echo 1 &gt;/home/jabo/software/zookeeper-3.4.6/data/myidssh spark-slave1 &quot;echo 2 &gt;/home/jabo/software/zookeeper-3.4.6/data/myid&quot;ssh spark-slave2 &quot;echo 3 &gt;/home/jabo/software/zookeeper-3.4.6/data/myid&quot; 启动和关闭1234567/** * 启动(分别对每台主机进行启动) */zkServer.sh start//关闭zkServer.sh stop 查看节点1234567891011root@spark-master:jps20093 Jps17164 QuorumPeerMainroot@spark-slave1: jps18666 QuorumPeerMain20352 Jpsroot@spark-slave2:jps4819 QuorumPeerMain94793 Jps 查看状态12345678910111213141516zkServer.sh statusroot@spark-master:/home/jabo/software/zookeeper-3.4.6# ./bin/zkServer.sh statusJMX enabled by defaultUsing config: /home/jabo/software/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: followerroot@spark-slave1:/home/jabo/software/zookeeper-3.4.6# bin/zkServer.sh statusJMX enabled by defaultUsing config: /home/jabo/software/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: leaderroot@spark-slave2:/home/jabo/software/zookeeper-3.4.6# bin/zkServer.sh statusJMX enabled by defaultUsing config: /home/jabo/software/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: follower 客户端连接123456789101112131415161718192021222324252627282930//可以连接任一节点至集群./bin/zkCli.sh -server spark-slave1:2181Connecting to spark-slave1:21812016-01-15 14:20:55,622 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT2016-01-15 14:20:55,640 [myid:] - INFO [main:Environment@100] - Client environment:host.name=spark-slave22016-01-15 14:20:55,640 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.7.0_792016-01-15 14:20:55,644 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2016-01-15 14:20:55,644 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java/jre2016-01-15 14:20:55,645 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/home/jabo/software/zookeeper-3.4.6/bin/../build/classes:/home/jabo/software/zookeeper-3.4.6/bin/../build/lib/*.jar:/home/jabo/software/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/home/jabo/software/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/home/jabo/software/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/home/jabo/software/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/home/jabo/software/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/home/jabo/software/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/home/jabo/software/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/home/jabo/software/zookeeper-3.4.6/bin/../conf:.:/usr/lib/jvm/java/lib:/usr/lib/jvm/java/jre/lib2016-01-15 14:20:55,645 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2016-01-15 14:20:55,651 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2016-01-15 14:20:55,651 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2016-01-15 14:20:55,651 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux2016-01-15 14:20:55,651 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd642016-01-15 14:20:55,651 [myid:] - INFO [main:Environment@100] - Client environment:os.version=3.13.0-32-generic2016-01-15 14:20:55,651 [myid:] - INFO [main:Environment@100] - Client environment:user.name=root2016-01-15 14:20:55,652 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/root2016-01-15 14:20:55,652 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/home/jabo/software/zookeeper-3.4.62016-01-15 14:20:55,658 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=spark-slave1:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@4b4bc1eWelcome to ZooKeeper!2016-01-15 14:20:55,722 [myid:] - INFO [main-SendThread(spark-slave1:2181):ClientCnxn$SendThread@975] - Opening socket connection to server spark-slave1/192.168.108.21:2181. Will not attempt to authenticate using SASL (unknown error)2016-01-15 14:20:55,736 [myid:] - INFO [main-SendThread(spark-slave1:2181):ClientCnxn$SendThread@852] - Socket connection established to spark-slave1/192.168.108.21:2181, initiating sessionJLine support is enabled2016-01-15 14:20:55,767 [myid:] - INFO [main-SendThread(spark-slave1:2181):ClientCnxn$SendThread@1235] - Session establishment complete on server spark-slave1/192.168.108.21:2181, sessionid = 0x25243ec79bf0001, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: spark-slave1:2181(CONNECTED) 0] 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>ZooKeeper</tag>
        <tag>开源</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tachyon集群部署]]></title>
    <url>%2F2016%2F01%2F05%2FTachyon-deploy-cluster%2F</url>
    <content type="text"><![CDATA[Tachyon如何进行集群部署？ 测试环境12345678910测试环境： Ubuntu 14.04 LTS x64 Tachyon：tachyon-0.7.1-bin.tar.gz hostname IP role spark-master： 192.168.108.20 master &amp; worker spark-slave1： 192.168.108.21 worker spark-slave2： 192.168.108.22 worker !默认情况全部操作在root下进行 Tachyon集群配置准备工作修改主机名12345678/** * 分别配置三台主机的主机名 * hostname： * spark-master * spark-slave1 * spark-slave2 */sudo vim /etc/hostname 修改hosts12345678910/** * 分别配置修改三台主机的hosts * 添加如下内容： * 192.168.108.20 spark-master * 192.168.108.21 spark-slave1 * 192.168.108.22 spark-slave2 * 如果发现hosts文件中有127.0.1.1或者除了127.0.0.1之外的其他内容，将他们删除 * */sudo vim /etc/hosts 重启生效1sudo init 6 spark-master 配置请参考：Tachyon本地部署 分配ssh123456789101112131415/** * master主机中运行 * 切换至root */su rootssh-keygen -t rsa /** * 分发ssh至每台机子 */su rootscp ~/.ssh/id_rsa.pub root@spark-master:/homescp ~/.ssh/id_rsa.pub root@spark-slave1:/homescp ~/.ssh/id_rsa.pub root@spark-slave2:/home 授权ssh12345678/** * 分别在所有主机运行 */su rootcat /home/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keyscd ~/.ssh/chmod 600 authorized_keys 修改ssh配置123456789/** * 分别在所有主机运行 */sudo vim /etc/ssh/sshd_config注释：PermitRootLogin without-password添加：PermitRootLogin yessudo service ssh restart Tachyon环境变量123456789/** * 分别在所有主机运行，并添加如下内容 */sudo vim /etc/profileexport TACHYON_HOME=#Tachyon路径export PATH=$TACHYON_HOME/bin:$PATHsource /etc/profile Tachyon目录分发123456789101112/** * 复制master下的Tachyon目录到slave1、slave2下，并修改master配置 */sudo vim ./conf/workersspark-masterspark-slave1spark-slave2sudo vim ./conf/slavesspark-masterspark-slave1spark-slave2 初始化Tachyon1234567891011121314151617/** * master主机下运行 * 要用root权限运行 */sudo ./bin/tachyon formatConnecting to spark-master as root...root@spark-master&apos;s password: Formatting Tachyon Worker @ spark-masterConnection to spark-master closed.Connecting to spark-slave1 as root...Formatting Tachyon Worker @ spark-slave1Connection to spark-slave1 closed.Connecting to spark-slave2 as root...Formatting Tachyon Worker @ spark-slave2Connection to spark-slave2 closed.Formatting Tachyon Master @ spark-master 启动Tachyon集群1234567891011121314151617181920212223242526272829303132/** * master主机下运行 * 要用root权限运行 */sudo ./bin/tachyon-start.sh all Mount Killed 0 processes on spark-masterKilled 0 processes on spark-masterConnecting to spark-master as root...root@spark-master&apos;s password: Killed 0 processes on spark-masterConnection to spark-master closed.Connecting to spark-slave1 as root...Killed 0 processes on spark-slave1Connection to spark-slave1 closed.Connecting to spark-slave2 as root...Killed 0 processes on spark-slave2Connection to spark-slave2 closed.Starting master @ spark-masterConnecting to spark-master as root...root@spark-master&apos;s password: Formatting RamFS: /mnt/ramdisk (512mb)Starting worker @ spark-masterConnection to spark-master closed.Connecting to spark-slave1 as root...Formatting RamFS: /mnt/ramdisk (512mb)Starting worker @ spark-slave1Connection to spark-slave1 closed.Connecting to spark-slave2 as root...Formatting RamFS: /mnt/ramdisk (512mb)Starting worker @ spark-slave2Connection to spark-slave2 closed. 运行测试1./bin/tachyon runTests 查看每个主机Tachyon进程12345678910111213141516/** * 每个主机切换到root下 * su root */root@spark-master:25598 TachyonWorker25377 TachyonMaster28559 Jpsroot@spark-slave1:4555 Jps4518 TachyonWorkerroot@spark-slave2:4467 TachyonWorker4502 Jps 查看Web UI在master上用浏览器打开：hostname:19999 (spark-master:19999)点击Works页面，会看到三台主机基本信息 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Tachyon</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tachyon本地部署]]></title>
    <url>%2F2016%2F01%2F05%2FTachyon-deploy-local%2F</url>
    <content type="text"><![CDATA[Tachyon如何进行本地部署？ 测试环境123测试环境： Ubuntu 14.04 LTS x64 Tachyon：tachyon-0.7.1-bin.tar.gz 配置系统关闭防火墙1234/** * 查看防火墙状态：sudo ufw status */sudo ufw disable 配置主机名(spark-master)1sudo vim /etc/hostname 修改hosts123456/** * 如果发现hosts文件中有127.0.1.1或者除了127.0.0.1之外的其他内容，将他们删除 */sudo vim /etc/hostsIP地址 spark-master 配置Java环境请参考：Ubuntu下安装JDK环境 配置SSH免密码登录请参考：Ubuntu配置SSH免密码登录 安装Tachyon下载Tachyon官方网站：Tachyon官网 配置Tachyon环境变量12345678910sudo vim /etc/profile/** * 添加Tachyon路径 */export TACHYON_HOME=/home/jabo/software/tachyon-0.7.1export PATH=$TACHYON_HOME/bin:$PATH//使配置生效source /etc/profile 目录权限1sudo chmod -R 775 tachyon-0.7.1/ 复制Tachyon配置文件12345//进入Tachyon目录cd /software/tachyon-0.7.1//复制配置模版cp ./conf/tachyon-env.sh.template ./conf/tachyon-env.sh 配置Tachyon123456789//打开配置文件vim ./conf/tachyon-env.sh//添加如下内容export JAVA_HOME=/usr/lib/jvm/javaexport JAVA=&quot;$JAVA_HOME/bin/java&quot;export TACHYON_MASTER_ADDRESS=spark-masterexport TACHYON_UNDERFS_ADDRESS=$TACHYON_HOME/underfsexport TACHYON_WORKER_MEMORY_SIZE=512MB 配置slaves和workers12345//打开slaves配置文件，注释掉localhost，添加spark-mastervim ./conf/slaves//打开workers配置文件，注释掉localhost，添加spark-mastervim ./conf/workers 初始化文件系统1./bin/tachyon format 1234Connecting to spark-master as jabo...Formatting Tachyon Worker @ spark-masterConnection to spark-master closed.Formatting Tachyon Master @ spark-master 启动Tachyon的local模式1./bin/tachyon-start.sh local 12345678Killed 0 processes on spark-masterKilled 0 processes on spark-masterConnecting to spark-master as jabo...Killed 0 processes on spark-masterConnection to spark-master closed.Formatting RamFS: /mnt/ramdisk (512mb)Starting master @ spark-masterStarting worker @ spark-master 查看Tachyon进程12345jps7648 TachyonWorker7835 Jps7614 TachyonMaster 运行测试1./bin/tachyon runTests 查看Web UI用浏览器打开：hostname:19999 (spark-master:19999) 转载请注明出处]]></content>
      <categories>
        <category>BigData</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>大数据</tag>
        <tag>Tachyon</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下安装JDK环境]]></title>
    <url>%2F2016%2F01%2F04%2Fubuntu-jdk%2F</url>
    <content type="text"><![CDATA[Ubuntu如何安装JDK？ 测试环境与用例准备123测试环境： Ubuntu 14.04 LTS x64 JDK版本：jdk-7u79-linux-x64.gz 创建JDK存放目录1sudo mkdir /usr/lib/jvm 解压JDK压缩包1sudo tar zxvf jdk-7u79-linux-x64.tar.gz -C /usr/lib/jvm 重命名目录12cd /usr/lib/jvmsudo mv jdk1.7.0_79 java 配置环境变量123456789101112131415161718192021/** * 打开配置文件 */sudo vim ~/.bashrc/** * 最后面加入代码 */export JAVA_HOME=/usr/lib/jvm/javaexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH/** * 保存退出，运行下面代码使配置生效 */source ~/.bashrc/** * 切换到root下，做同样的配置，确保JDK配置成功 */ 配置默认JDK版本12345678910sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/java/bin/java 300sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java/bin/javac 300sudo update-alternatives --install /usr/bin/jar jar /usr/lib/jvm/java/bin/jar 300sudo update-alternatives --install /usr/bin/javah javah /usr/lib/jvm/java/bin/javah 300sudo update-alternatives --install /usr/bin/javap javap /usr/lib/jvm/java/bin/javap 300/** * 查看当前各种JDK版本和配置 */sudo update-alternatives --config java 检查安装成功12345678java -version/** * 显示如下内容安装成功 */java version &quot;1.7.0_79&quot;Java(TM) SE Runtime Environment (build 1.7.0_79-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode) 转载请注明出处]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
        <tag>Java</tag>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache虚拟主机]]></title>
    <url>%2F2016%2F01%2F03%2FApache-virtual-host%2F</url>
    <content type="text"><![CDATA[一个IP地址如何绑定多个域名？ 测试环境与用例准备12测试环境： Ubuntu 14.04 LTS x64 1234测试用例： 主机IP：192.168.20.130 域名1配置：domain1.local 域名2配置：domain2.local Apache2安装 更新软件源 1sudo apt-get update 安装Apache2服务器 1sudo apt-get install apache2 测试是否安装成功：http://服务器IP地址/ (跳转到Apache主页) 配置虚拟主机站点 分别创建域名站点目录 123456/** * domain1_site存放站点一 * domain2_site存放站点二 */sudo mkdir -p /var/www/domain1_sitesudo mkdir -p /var/www/domain2_site 分别创建站点 1234/** * 创建domain1站点 */sudo vi /var/www/domain1_site/index.html 12345678910111213/** * domain1站点内容 */&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt; &lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;www.domain1.local&lt;/title&gt; &lt;/head&gt; &lt;body&gt; This is www.domain1.local! &lt;/body&gt;&lt;/html&gt; 12345/** * 创建domain2站点 */sudo cp /var/www/domain1_site/index.html /var/www/domain2_site/index.htmlsudo vim /var/www/domain2_site/index.html 12345678910111213/** * domain2站点内容 */&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt; &lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;www.domain2.local&lt;/title&gt; &lt;/head&gt; &lt;body&gt; This is www.domain2.local! &lt;/body&gt;&lt;/html&gt; 创建虚拟主机配置文件 apache有一个默认的虚拟主机文件000-default.conf，以此文件为模版进行修改 domain1配置文件 12sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/domain1.local.confsudo vim /etc/apache2/sites-available/domain1.local.conf 12345678910111213141516171819202122232425262728293031323334353637/** * #为注释部分 * 需要修改地方： * ServerAdmin：管理员邮箱 * DocumentRoot：站点地址 * ServerName：域名(如果有多个用空格隔开) */&lt;VirtualHost *:80&gt; # The ServerName directive sets the request scheme, hostname and port that # the server uses to identify itself. This is used when creating # redirection URLs. In the context of virtual hosts, the ServerName # specifies what hostname must appear in the request&apos;s Host: header to # match this virtual host. For the default virtual host (this file) this # value is not decisive as it is used as a last resort host regardless. # However, you must set it for any further virtual host explicitly. #ServerName www.example.com ServerAdmin webmaster@localhost DocumentRoot /var/www/domain1_site ServerName domain1.local # Available loglevels: trace8, ..., trace1, debug, info, notice, warn, # error, crit, alert, emerg. # It is also possible to configure the loglevel for particular # modules, e.g. #LogLevel info ssl:warn ErrorLog $&#123;APACHE_LOG_DIR&#125;/error.log CustomLog $&#123;APACHE_LOG_DIR&#125;/access.log combined # For most configuration files from conf-available/, which are # enabled or disabled at a global level, it is possible to # include a line for only one particular virtual host. For example the # following line enables the CGI configuration for this host only # after it has been globally disabled with &quot;a2disconf&quot;. #Include conf-available/serve-cgi-bin.conf&lt;/VirtualHost&gt; domain2配置文件 12sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/domain2.local.confsudo vim /etc/apache2/sites-available/domain2.local.conf 12345678910111213141516171819202122232425262728293031323334353637/** * #为注释部分 * 需要修改地方： * ServerAdmin：管理员邮箱 * DocumentRoot：站点地址 * ServerName：域名(如果有多个用空格隔开) */&lt;VirtualHost *:80&gt; # The ServerName directive sets the request scheme, hostname and port that # the server uses to identify itself. This is used when creating # redirection URLs. In the context of virtual hosts, the ServerName # specifies what hostname must appear in the request&apos;s Host: header to # match this virtual host. For the default virtual host (this file) this # value is not decisive as it is used as a last resort host regardless. # However, you must set it for any further virtual host explicitly. #ServerName www.example.com ServerAdmin webmaster@localhost DocumentRoot /var/www/domain2_site ServerName domain2.local # Available loglevels: trace8, ..., trace1, debug, info, notice, warn, # error, crit, alert, emerg. # It is also possible to configure the loglevel for particular # modules, e.g. #LogLevel info ssl:warn ErrorLog $&#123;APACHE_LOG_DIR&#125;/error.log CustomLog $&#123;APACHE_LOG_DIR&#125;/access.log combined # For most configuration files from conf-available/, which are # enabled or disabled at a global level, it is possible to # include a line for only one particular virtual host. For example the # following line enables the CGI configuration for this host only # after it has been globally disabled with &quot;a2disconf&quot;. #Include conf-available/serve-cgi-bin.conf&lt;/VirtualHost&gt; 启用配置信息 修改虚拟主机文件后，禁用默认的虚拟主机配置，然后启用新的虚拟主机配置 12345678//禁用默认的虚拟主机配置sudo a2dissite 000-default.conf//启用domain1的配置sudo a2ensite domain1.local.conf//启用domain2的配置sudo a2ensite domain2.local.conf 重启apache服务器 1sudo service apache2 restart 修改hosts文件 1sudo vim /etc/hosts 123//添加如下内容192.168.20.130 domain1.local192.168.20.130 domain2.local 测试配置的虚拟主机 打开你的浏览器并访问http://domain1.local 或 http://domain2.local (将看到之前编辑的两个html，绑定了不同的域名，完成) 转载请注明出处]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
        <tag>Apache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH免密码登录]]></title>
    <url>%2F2016%2F01%2F03%2FUbuntu-ssh-no-password%2F</url>
    <content type="text"><![CDATA[Ubuntu配置SSH免密码登录？ 测试环境12测试环境： Ubuntu 14.04 LTS x64 安装SSH1sudo apt-get install openssh-server 生成SSH密匙12345/** * 此时会在/home/用户名/.ssh/，文件夹下生成id_rsa(私匙)和id_rsa.pub(公匙)两个文件 * 在每台机子上都生成 */ssh-keygen -t rsa 1234/** * 自身追加公匙授权 */cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 1234567891011/** * 如果多台机器(192.168.108.20为Worker) * 1. Master分发公匙 * 2. Worker追加授权 */scp id_rsa.pub username@hostname:/home/username/** * 登录192.168.108.20Worker */cat /home/username/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 修改authorized_keys权限12345/** * Master、Worker相同配置 */cd ~/.ssh/sudo chmod 600 authorized_keys 验证免密码登录1ssh hostname 转载请注明出处]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
        <tag>SSH</tag>
        <tag>免密码</tag>
      </tags>
  </entry>
</search>
